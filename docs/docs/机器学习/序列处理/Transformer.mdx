---
sidebar_position: 1
title: Transformer
---



:::info
Attention Is All You Need20252
:::


tokenizertokentokentokentoken



## 

Embedding LayerEncoderDecoderToken

Token  

tokentoken



 token tokenizer.json

 token  `<PAD>` `<UNK>`  tokenizer_config.json vocab.json





 token

## 



| /Token | 1 | 2 | 3 | 4 |
| ---------- | ----- | ----- | ----- | ----- |
| \<PAD\>    | 0.00  | 0.00  | 0.00  | 0.00  |
| \<UNK\>    | 0.12  | -0.51 | 0.32  | 0.89  |
|          | 0.87  | 0.42  | -0.26 | 0.35  |
|        | 0.32  | 0.52  | 0.75  | 0.22  |
|        | 0.45  | 0.68  | 0.21  | 0.37  |
|          | 0.65  | 0.71  | 0.38  | -0.15 |


""["", "", "", "", ""]

- "" → [0.87, 0.42, -0.26, 0.35]
- "" → [0.65, 0.71, 0.38, -0.15]
- "" → [0.45, 0.68, 0.21, 0.37]
- "" → [0.32, 0.52, 0.75, 0.22]
- "" → [0.45, 0.68, 0.21, 0.37]

for

### 

10

```python
import numpy as np

# 
vocab = {"<PAD>": 0, 
         "<UNK>": 1, 
         "": 2, 
         "": 3, 
         "": 4, 
        #..........
         "": 5000,
        }
embedding_matrix = np.array([
    # NN4
    [0.00, 0.00, 0.00, 0.00],  # <PAD>
    [0.12, -0.51, 0.32, 0.89], # <UNK>
    [0.87, 0.42, -0.26, 0.35], # 
    [0.65, 0.71, 0.38, -0.15], # 
    [0.45, 0.68, 0.21, 0.37],  # 
    # ........
    [0.32, 0.52, 0.75, 0.22],  # 
])

# 
def word_to_onehot(word, vocab_size):
    onehot = np.zeros(vocab_size)
    if word in vocab:
        onehot[vocab[word]] = 1 
        #  “”vocab[“”] 210. 
        # onehot[2] = 1 [0,0,1,0,0,.....,0]
    else:
        onehot[vocab["<UNK>"]] = 1 # 1.0 [0,1,0,0,0,.....,0]
    return onehot #  1,5000

# 
def get_embedding_via_onehot(word, vocab, embedding_matrix):
    onehot = word_to_onehot(word, len(vocab)) 
    return np.dot(onehot, embedding_matrix) # 1,5000  @ 5000,N =  1,N

# 
tokens = ["", "", "", "", ""] # 5
embeddings = [get_embedding_via_onehot(token, vocab, embedding_matrix) for token in tokens]
#  5  N
```

### 



```python
import torch
import numpy as np
embedding_matrix = np.array([
    # NN4
    [0.00, 0.00, 0.00, 0.00],  # <PAD>
    [0.12, -0.51, 0.32, 0.89], # <UNK>
    [0.87, 0.42, -0.26, 0.35], # 
    [0.65, 0.71, 0.38, -0.15], # 
    [0.45, 0.68, 0.21, 0.37],  # 
    # ........
    [0.32, 0.52, 0.75, 0.22],  # 
])

# PyTorchEmbedding
vocab_size,embedding_dim = embedding_matrix.shape
# embedding_dim 4
# vocab_size 

# 
vocab = {"<PAD>": 0, 
         "<UNK>": 1, 
         "": 2, 
         "": 3, 
         "": 4, 
        #..........
         "": 5000,
        }

# 
embedding = torch.nn.Embedding(vocab_size, embedding_dim)
# 
embedding.weight.data = torch.tensor(embedding_matrix, dtype=torch.float)

# 
def tokens_to_indices(tokens, vocab):
    return [vocab.get(token, vocab["<UNK>"]) for token in tokens]

# 
tokens = ["", "", "", "", ""]
indices = tokens_to_indices(tokens, vocab)
token_indices = torch.tensor(indices)
token_embeddings = embedding(token_indices) # 

print(token_embeddings)
'''

Transformer

TransformerPositional Encoding
'''
def get_positional_encoding(seq_len, d_model):
    pe = np.zeros((seq_len, d_model))
    for pos in range(seq_len):
        for i in range(0, d_model, 2):
            pe[pos, i] = np.sin(pos / (10000 ** (i / d_model)))
            if i + 1 < d_model:
                pe[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))
    return pe

# ()4
seq_len = len(tokens)
positional_encoding = get_positional_encoding(seq_len,embedding_dim)
print(positional_encoding)

# ()
final_embeddings = token_embeddings + torch.tensor(positional_encoding, dtype=torch.float)
```

