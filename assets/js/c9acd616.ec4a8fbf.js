"use strict";(globalThis.webpackChunkweuqiangcreate_website=globalThis.webpackChunkweuqiangcreate_website||[]).push([[7872],{60805(n,e,a){a.r(e),a.d(e,{assets:()=>i,contentTitle:()=>t,default:()=>p,frontMatter:()=>l,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"\u5927\u6570\u636e\u6280\u672f/Spark\u5feb\u901f\u5927\u6570\u636e\u5206\u6790","title":"Spark - \u5feb\u5982\u95ea\u7535\u7684\u5927\u6570\u636e\u5904\u7406","description":"Spark\u662f\u76ee\u524d\u6700\u6d41\u884c\u7684\u5927\u6570\u636e\u5904\u7406\u6846\u67b6\u3002\u8fd9\u4e00\u7ae0\u6559\u4f60\u771f\u6b63\u7528Spark\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u3002","source":"@site/docs/docs/\u5927\u6570\u636e\u6280\u672f/Spark\u5feb\u901f\u5927\u6570\u636e\u5206\u6790.mdx","sourceDirName":"\u5927\u6570\u636e\u6280\u672f","slug":"/\u5927\u6570\u636e\u6280\u672f/Spark\u5feb\u901f\u5927\u6570\u636e\u5206\u6790","permalink":"/weuqiangcreate_website/docs/\u5927\u6570\u636e\u6280\u672f/Spark\u5feb\u901f\u5927\u6570\u636e\u5206\u6790","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Spark - \u5feb\u5982\u95ea\u7535\u7684\u5927\u6570\u636e\u5904\u7406"},"sidebar":"tutorialSidebar","previous":{"title":"Hadoop\u751f\u6001\u7cfb\u7edf - \u4ece\u96f6\u5f00\u59cb","permalink":"/weuqiangcreate_website/docs/\u5927\u6570\u636e\u6280\u672f/Hadoop\u751f\u6001\u7cfb\u7edf"},"next":{"title":"Kafka - \u5b9e\u65f6\u6570\u636e\u6d41\u7684\u9ad8\u901f\u516c\u8def","permalink":"/weuqiangcreate_website/docs/\u5927\u6570\u636e\u6280\u672f/Kafka\u6d88\u606f\u961f\u5217"}}');var s=a(74848),d=a(28453);const l={sidebar_position:2,title:"Spark - \u5feb\u5982\u95ea\u7535\u7684\u5927\u6570\u636e\u5904\u7406"},t="Spark - \u6bd4Hadoop\u5feb100\u500d",i={},o=[{value:"\u4e3a\u4ec0\u4e48\u9009\u62e9Spark\uff1f",id:"\u4e3a\u4ec0\u4e48\u9009\u62e9spark",level:2},{value:"Spark vs MapReduce",id:"spark-vs-mapreduce",level:3},{value:"Spark\u7684\u4f18\u52bf",id:"spark\u7684\u4f18\u52bf",level:3},{value:"\u7b2c\u4e00\u90e8\u5206\uff1aSpark\u5feb\u901f\u5165\u95e8",id:"\u7b2c\u4e00\u90e8\u5206spark\u5feb\u901f\u5165\u95e8",level:2},{value:"\u5b89\u88c5Spark",id:"\u5b89\u88c5spark",level:3},{value:"\u7b2c\u4e00\u4e2aSpark\u7a0b\u5e8f",id:"\u7b2c\u4e00\u4e2aspark\u7a0b\u5e8f",level:3},{value:"\u7b2c\u4e8c\u90e8\u5206\uff1aRDD - Spark\u7684\u57fa\u7840",id:"\u7b2c\u4e8c\u90e8\u5206rdd---spark\u7684\u57fa\u7840",level:2},{value:"\u4ec0\u4e48\u662fRDD\uff1f",id:"\u4ec0\u4e48\u662frdd",level:3},{value:"\u521b\u5efaRDD",id:"\u521b\u5efardd",level:3},{value:"RDD\u8f6c\u6362\u64cd\u4f5c\uff08Transformation\uff09",id:"rdd\u8f6c\u6362\u64cd\u4f5ctransformation",level:3},{value:"RDD\u884c\u52a8\u64cd\u4f5c\uff08Action\uff09",id:"rdd\u884c\u52a8\u64cd\u4f5caction",level:3},{value:"\u5b9e\u6218\uff1aWordCount",id:"\u5b9e\u6218wordcount",level:3},{value:"\u7b2c\u4e09\u90e8\u5206\uff1aDataFrame - \u7ed3\u6784\u5316\u6570\u636e\u5904\u7406",id:"\u7b2c\u4e09\u90e8\u5206dataframe---\u7ed3\u6784\u5316\u6570\u636e\u5904\u7406",level:2},{value:"\u4e3a\u4ec0\u4e48\u7528DataFrame\uff1f",id:"\u4e3a\u4ec0\u4e48\u7528dataframe",level:3},{value:"\u521b\u5efaDataFrame",id:"\u521b\u5efadataframe",level:3},{value:"DataFrame\u57fa\u672c\u64cd\u4f5c",id:"dataframe\u57fa\u672c\u64cd\u4f5c",level:3},{value:"DataFrame\u805a\u5408\u64cd\u4f5c",id:"dataframe\u805a\u5408\u64cd\u4f5c",level:3},{value:"\u5b9e\u6218\uff1a\u7528\u6237\u884c\u4e3a\u5206\u6790",id:"\u5b9e\u6218\u7528\u6237\u884c\u4e3a\u5206\u6790",level:3},{value:"\u7b2c\u56db\u90e8\u5206\uff1aSpark SQL",id:"\u7b2c\u56db\u90e8\u5206spark-sql",level:2},{value:"\u4f7f\u7528SQL\u67e5\u8be2",id:"\u4f7f\u7528sql\u67e5\u8be2",level:3},{value:"\u8fde\u63a5\u64cd\u4f5c",id:"\u8fde\u63a5\u64cd\u4f5c",level:3},{value:"\u7b2c\u4e94\u90e8\u5206\uff1a\u6027\u80fd\u4f18\u5316",id:"\u7b2c\u4e94\u90e8\u5206\u6027\u80fd\u4f18\u5316",level:2},{value:"1. \u7f13\u5b58",id:"1-\u7f13\u5b58",level:3},{value:"2. \u5206\u533a",id:"2-\u5206\u533a",level:3},{value:"3. \u5e7f\u64ad\u53d8\u91cf",id:"3-\u5e7f\u64ad\u53d8\u91cf",level:3},{value:"4. \u907f\u514dShuffle",id:"4-\u907f\u514dshuffle",level:3},{value:"\u603b\u7ed3",id:"\u603b\u7ed3",level:2},{value:"\u7ec3\u4e60\u9898",id:"\u7ec3\u4e60\u9898",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,d.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"spark---\u6bd4hadoop\u5feb100\u500d",children:"Spark - \u6bd4Hadoop\u5feb100\u500d"})}),"\n",(0,s.jsxs)(e.p,{children:["Spark\u662f\u76ee\u524d\u6700\u6d41\u884c\u7684\u5927\u6570\u636e\u5904\u7406\u6846\u67b6\u3002\u8fd9\u4e00\u7ae0\u6559\u4f60",(0,s.jsx)(e.strong,{children:"\u771f\u6b63\u7528Spark\u89e3\u51b3\u5b9e\u9645\u95ee\u9898"}),"\u3002"]}),"\n",(0,s.jsx)(e.h2,{id:"\u4e3a\u4ec0\u4e48\u9009\u62e9spark",children:"\u4e3a\u4ec0\u4e48\u9009\u62e9Spark\uff1f"}),"\n",(0,s.jsx)(e.h3,{id:"spark-vs-mapreduce",children:"Spark vs MapReduce"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# MapReduce\uff1a\u6162\n# \u6bcf\u6b21\u8ba1\u7b97\u90fd\u8981\u8bfb\u5199\u78c1\u76d8\nMap \u2192 \u5199\u78c1\u76d8 \u2192 Shuffle \u2192 \u8bfb\u78c1\u76d8 \u2192 Reduce \u2192 \u5199\u78c1\u76d8\n\n# Spark\uff1a\u5feb\n# \u6570\u636e\u7f13\u5b58\u5728\u5185\u5b58\uff0c\u94fe\u5f0f\u8ba1\u7b97\nMap \u2192 Shuffle \u2192 Reduce  (\u5168\u5728\u5185\u5b58)\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"\u6027\u80fd\u5bf9\u6bd4"}),"\uff1a"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"\u8fed\u4ee3\u8ba1\u7b97\uff1aSpark\u5feb100\u500d"}),"\n",(0,s.jsx)(e.li,{children:"\u4ea4\u4e92\u5f0f\u67e5\u8be2\uff1aSpark\u5feb10\u500d"}),"\n",(0,s.jsx)(e.li,{children:"\u6d41\u5f0f\u5904\u7406\uff1aSpark\u5ef6\u8fdf\u66f4\u4f4e"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"spark\u7684\u4f18\u52bf",children:"Spark\u7684\u4f18\u52bf"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"\u901f\u5ea6\u5feb"}),"\uff1a\u5185\u5b58\u8ba1\u7b97"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"\u6613\u7528"}),"\uff1aAPI\u7b80\u6d01"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"\u901a\u7528"}),"\uff1a\u6279\u5904\u7406\u3001\u6d41\u5904\u7406\u3001\u673a\u5668\u5b66\u4e60\u3001\u56fe\u8ba1\u7b97"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"\u517c\u5bb9"}),"\uff1a\u53ef\u4ee5\u8bfb\u53d6HDFS\u3001Hive\u3001HBase\u7b49"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"\u7b2c\u4e00\u90e8\u5206spark\u5feb\u901f\u5165\u95e8",children:"\u7b2c\u4e00\u90e8\u5206\uff1aSpark\u5feb\u901f\u5165\u95e8"}),"\n",(0,s.jsx)(e.h3,{id:"\u5b89\u88c5spark",children:"\u5b89\u88c5Spark"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"\u65b9\u6cd51\uff1a\u4f7f\u7528Docker\uff08\u63a8\u8350\uff09"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:'# docker-compose.yml\nversion: \'3\'\nservices:\n  spark-master:\n    image: bitnami/spark:3.4\n    environment:\n      - SPARK_MODE=master\n    ports:\n      - "8080:8080"\n      - "7077:7077"\n  \n  spark-worker:\n    image: bitnami/spark:3.4\n    environment:\n      - SPARK_MODE=worker\n      - SPARK_MASTER_URL=spark://spark-master:7077\n    depends_on:\n      - spark-master\n\n# \u542f\u52a8\ndocker-compose up -d\n\n# \u8bbf\u95eeWeb UI: http://localhost:8080\n'})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"\u65b9\u6cd52\uff1a\u672c\u5730\u5b89\u88c5"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# \u4e0b\u8f7dSpark\nwget https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\ntar -xzf spark-3.4.0-bin-hadoop3.tgz\ncd spark-3.4.0-bin-hadoop3\n\n# \u542f\u52a8Spark Shell\n./bin/pyspark\n"})}),"\n",(0,s.jsx)(e.h3,{id:"\u7b2c\u4e00\u4e2aspark\u7a0b\u5e8f",children:"\u7b2c\u4e00\u4e2aSpark\u7a0b\u5e8f"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from pyspark.sql import SparkSession\n\n# \u521b\u5efaSpark\u4f1a\u8bdd\nspark = SparkSession.builder \\\n    .appName("HelloSpark") \\\n    .master("local[*]") \\\n    .getOrCreate()\n\n# \u521b\u5efa\u6570\u636e\ndata = [1, 2, 3, 4, 5]\nrdd = spark.sparkContext.parallelize(data)\n\n# \u8ba1\u7b97\nresult = rdd.map(lambda x: x * 2).collect()\nprint(result)  # [2, 4, 6, 8, 10]\n\n# \u505c\u6b62\nspark.stop()\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"\u8fd0\u884c"}),"\uff1a"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"spark-submit hello_spark.py\n"})}),"\n",(0,s.jsx)(e.h2,{id:"\u7b2c\u4e8c\u90e8\u5206rdd---spark\u7684\u57fa\u7840",children:"\u7b2c\u4e8c\u90e8\u5206\uff1aRDD - Spark\u7684\u57fa\u7840"}),"\n",(0,s.jsx)(e.h3,{id:"\u4ec0\u4e48\u662frdd",children:"\u4ec0\u4e48\u662fRDD\uff1f"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"RDD\uff08Resilient Distributed Dataset\uff09"}),"\uff1a\u5f39\u6027\u5206\u5e03\u5f0f\u6570\u636e\u96c6"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# RDD\u5c31\u662f\u5206\u5e03\u5728\u591a\u53f0\u673a\u5668\u4e0a\u7684\u6570\u636e\u96c6\u5408\n\u6570\u636e: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n# \u5206\u62103\u4e2a\u5206\u533a\nPartition 1: [1, 2, 3, 4]\nPartition 2: [5, 6, 7]\nPartition 3: [8, 9, 10]\n\n# \u6bcf\u4e2a\u5206\u533a\u5728\u4e0d\u540c\u7684\u673a\u5668\u4e0a\u5e76\u884c\u5904\u7406\n"})}),"\n",(0,s.jsx)(e.h3,{id:"\u521b\u5efardd",children:"\u521b\u5efaRDD"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from pyspark import SparkContext\n\nsc = SparkContext("local", "RDD Demo")\n\n# \u65b9\u6cd51\uff1a\u4ece\u96c6\u5408\u521b\u5efa\ndata = [1, 2, 3, 4, 5]\nrdd1 = sc.parallelize(data)\n\n# \u65b9\u6cd52\uff1a\u4ece\u6587\u4ef6\u521b\u5efa\nrdd2 = sc.textFile("data.txt")\n\n# \u65b9\u6cd53\uff1a\u4eceHDFS\u521b\u5efa\nrdd3 = sc.textFile("hdfs://namenode:9000/user/data/file.txt")\n'})}),"\n",(0,s.jsx)(e.h3,{id:"rdd\u8f6c\u6362\u64cd\u4f5ctransformation",children:"RDD\u8f6c\u6362\u64cd\u4f5c\uff08Transformation\uff09"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"\u8f6c\u6362\u64cd\u4f5c\u662f\u60f0\u6027\u7684\uff0c\u4e0d\u4f1a\u7acb\u5373\u6267\u884c"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# 1. map\uff1a\u5bf9\u6bcf\u4e2a\u5143\u7d20\u5e94\u7528\u51fd\u6570\nrdd = sc.parallelize([1, 2, 3, 4, 5])\nrdd2 = rdd.map(lambda x: x * 2)\n# [2, 4, 6, 8, 10]\n\n# 2. filter\uff1a\u8fc7\u6ee4\u5143\u7d20\nrdd3 = rdd.filter(lambda x: x % 2 == 0)\n# [2, 4]\n\n# 3. flatMap\uff1a\u5c55\u5e73\u7ed3\u679c\nrdd4 = sc.parallelize(["hello world", "spark is fast"])\nwords = rdd4.flatMap(lambda line: line.split())\n# ["hello", "world", "spark", "is", "fast"]\n\n# 4. distinct\uff1a\u53bb\u91cd\nrdd5 = sc.parallelize([1, 2, 2, 3, 3, 3])\nrdd6 = rdd5.distinct()\n# [1, 2, 3]\n\n# 5. union\uff1a\u5408\u5e76\nrdd7 = sc.parallelize([1, 2, 3])\nrdd8 = sc.parallelize([4, 5, 6])\nrdd9 = rdd7.union(rdd8)\n# [1, 2, 3, 4, 5, 6]\n\n# 6. intersection\uff1a\u4ea4\u96c6\nrdd10 = sc.parallelize([1, 2, 3, 4])\nrdd11 = sc.parallelize([3, 4, 5, 6])\nrdd12 = rdd10.intersection(rdd11)\n# [3, 4]\n\n# 7. groupByKey\uff1a\u6309key\u5206\u7ec4\nrdd13 = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])\nrdd14 = rdd13.groupByKey()\n# [("a", [1, 3]), ("b", [2])]\n\n# 8. reduceByKey\uff1a\u6309key\u805a\u5408\nrdd15 = rdd13.reduceByKey(lambda x, y: x + y)\n# [("a", 4), ("b", 2)]\n\n# 9. sortByKey\uff1a\u6309key\u6392\u5e8f\nrdd16 = sc.parallelize([(3, "c"), (1, "a"), (2, "b")])\nrdd17 = rdd16.sortByKey()\n# [(1, "a"), (2, "b"), (3, "c")]\n\n# 10. join\uff1a\u8fde\u63a5\nrdd18 = sc.parallelize([("a", 1), ("b", 2)])\nrdd19 = sc.parallelize([("a", "x"), ("b", "y")])\nrdd20 = rdd18.join(rdd19)\n# [("a", (1, "x")), ("b", (2, "y"))]\n'})}),"\n",(0,s.jsx)(e.h3,{id:"rdd\u884c\u52a8\u64cd\u4f5caction",children:"RDD\u884c\u52a8\u64cd\u4f5c\uff08Action\uff09"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"\u884c\u52a8\u64cd\u4f5c\u4f1a\u89e6\u53d1\u5b9e\u9645\u8ba1\u7b97"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'rdd = sc.parallelize([1, 2, 3, 4, 5])\n\n# 1. collect\uff1a\u6536\u96c6\u6240\u6709\u5143\u7d20\nresult = rdd.collect()\nprint(result)  # [1, 2, 3, 4, 5]\n\n# 2. count\uff1a\u8ba1\u6570\ncount = rdd.count()\nprint(count)  # 5\n\n# 3. first\uff1a\u7b2c\u4e00\u4e2a\u5143\u7d20\nfirst = rdd.first()\nprint(first)  # 1\n\n# 4. take\uff1a\u53d6\u524dn\u4e2a\u5143\u7d20\ntop3 = rdd.take(3)\nprint(top3)  # [1, 2, 3]\n\n# 5. reduce\uff1a\u805a\u5408\nsum_val = rdd.reduce(lambda x, y: x + y)\nprint(sum_val)  # 15\n\n# 6. foreach\uff1a\u5bf9\u6bcf\u4e2a\u5143\u7d20\u6267\u884c\u64cd\u4f5c\nrdd.foreach(lambda x: print(x))\n\n# 7. saveAsTextFile\uff1a\u4fdd\u5b58\u5230\u6587\u4ef6\nrdd.saveAsTextFile("output/")\n\n# 8. countByKey\uff1a\u7edf\u8ba1\u6bcf\u4e2akey\u7684\u6570\u91cf\nrdd_kv = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])\ncounts = rdd_kv.countByKey()\nprint(counts)  # {\'a\': 2, \'b\': 1}\n'})}),"\n",(0,s.jsx)(e.h3,{id:"\u5b9e\u6218wordcount",children:"\u5b9e\u6218\uff1aWordCount"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from pyspark import SparkContext\n\nsc = SparkContext("local", "WordCount")\n\n# \u8bfb\u53d6\u6587\u4ef6\ntext = sc.textFile("data.txt")\n\n# \u5904\u7406\ncounts = text.flatMap(lambda line: line.split()) \\\n    .map(lambda word: (word.lower(), 1)) \\\n    .reduceByKey(lambda x, y: x + y) \\\n    .sortBy(lambda x: x[1], ascending=False)\n\n# \u8f93\u51faTOP10\ntop10 = counts.take(10)\nfor word, count in top10:\n    print(f"{word}: {count}")\n\n# \u4fdd\u5b58\u7ed3\u679c\ncounts.saveAsTextFile("output/wordcount")\n\nsc.stop()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"\u7b2c\u4e09\u90e8\u5206dataframe---\u7ed3\u6784\u5316\u6570\u636e\u5904\u7406",children:"\u7b2c\u4e09\u90e8\u5206\uff1aDataFrame - \u7ed3\u6784\u5316\u6570\u636e\u5904\u7406"}),"\n",(0,s.jsx)(e.h3,{id:"\u4e3a\u4ec0\u4e48\u7528dataframe",children:"\u4e3a\u4ec0\u4e48\u7528DataFrame\uff1f"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# RDD\uff1a\u7075\u6d3b\u4f46\u7e41\u7410\nrdd.map(lambda x: x[0]) \\\n   .filter(lambda x: x > 18) \\\n   .groupBy(lambda x: x) \\\n   .count()\n\n# DataFrame\uff1a\u7b80\u6d01\u4e14\u4f18\u5316\ndf.select("age") \\\n  .filter(df.age > 18) \\\n  .groupBy("age") \\\n  .count()\n'})}),"\n",(0,s.jsx)(e.h3,{id:"\u521b\u5efadataframe",children:"\u521b\u5efaDataFrame"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName("DataFrame Demo").getOrCreate()\n\n# \u65b9\u6cd51\uff1a\u4ece\u5217\u8868\u521b\u5efa\ndata = [\n    ("Alice", 25, "F"),\n    ("Bob", 30, "M"),\n    ("Charlie", 35, "M")\n]\ndf = spark.createDataFrame(data, ["name", "age", "gender"])\n\n# \u65b9\u6cd52\uff1a\u4ece\u5b57\u5178\u521b\u5efa\ndata = [\n    {"name": "Alice", "age": 25, "gender": "F"},\n    {"name": "Bob", "age": 30, "gender": "M"}\n]\ndf = spark.createDataFrame(data)\n\n# \u65b9\u6cd53\uff1a\u4eceCSV\u8bfb\u53d6\ndf = spark.read.csv("data.csv", header=True, inferSchema=True)\n\n# \u65b9\u6cd54\uff1a\u4eceJSON\u8bfb\u53d6\ndf = spark.read.json("data.json")\n\n# \u65b9\u6cd55\uff1a\u4eceParquet\u8bfb\u53d6\uff08\u63a8\u8350\uff09\ndf = spark.read.parquet("data.parquet")\n\n# \u65b9\u6cd56\uff1a\u4ece\u6570\u636e\u5e93\u8bfb\u53d6\ndf = spark.read.jdbc(\n    url="jdbc:mysql://localhost:3306/db",\n    table="users",\n    properties={"user": "root", "password": "password"}\n)\n'})}),"\n",(0,s.jsx)(e.h3,{id:"dataframe\u57fa\u672c\u64cd\u4f5c",children:"DataFrame\u57fa\u672c\u64cd\u4f5c"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# \u67e5\u770b\u6570\u636e\ndf.show()  # \u663e\u793a\u524d20\u884c\ndf.show(5)  # \u663e\u793a\u524d5\u884c\ndf.head(3)  # \u8fd4\u56de\u524d3\u884c\n\n# \u67e5\u770b\u7ed3\u6784\ndf.printSchema()  # \u6253\u5370schema\ndf.columns  # \u5217\u540d\u5217\u8868\ndf.dtypes  # \u5217\u7c7b\u578b\n\n# \u7edf\u8ba1\u4fe1\u606f\ndf.count()  # \u884c\u6570\ndf.describe().show()  # \u7edf\u8ba1\u6458\u8981\ndf.summary().show()  # \u66f4\u8be6\u7ec6\u7684\u7edf\u8ba1\n\n# \u9009\u62e9\u5217\ndf.select("name", "age").show()\ndf.select(df.name, df.age).show()\ndf.select(df["name"], df["age"]).show()\n\n# \u8fc7\u6ee4\ndf.filter(df.age > 25).show()\ndf.filter("age > 25").show()\ndf.where(df.age > 25).show()\n\n# \u6392\u5e8f\ndf.orderBy("age").show()\ndf.orderBy(df.age.desc()).show()\ndf.sort("age", ascending=False).show()\n\n# \u53bb\u91cd\ndf.distinct().show()\ndf.dropDuplicates(["name"]).show()\n\n# \u91cd\u547d\u540d\ndf.withColumnRenamed("name", "username").show()\n\n# \u6dfb\u52a0\u5217\nfrom pyspark.sql.functions import col, lit\ndf.withColumn("age_plus_10", col("age") + 10).show()\ndf.withColumn("country", lit("USA")).show()\n\n# \u5220\u9664\u5217\ndf.drop("gender").show()\n\n# \u7f3a\u5931\u503c\u5904\u7406\ndf.na.drop().show()  # \u5220\u9664\u542b\u7f3a\u5931\u503c\u7684\u884c\ndf.na.fill(0).show()  # \u586b\u5145\u7f3a\u5931\u503c\ndf.na.fill({"age": 0, "name": "Unknown"}).show()\n'})}),"\n",(0,s.jsx)(e.h3,{id:"dataframe\u805a\u5408\u64cd\u4f5c",children:"DataFrame\u805a\u5408\u64cd\u4f5c"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from pyspark.sql.functions import *\n\n# \u5206\u7ec4\u805a\u5408\ndf.groupBy("gender").count().show()\ndf.groupBy("gender").agg(avg("age"), max("age"), min("age")).show()\n\n# \u591a\u5217\u5206\u7ec4\ndf.groupBy("gender", "city").count().show()\n\n# \u805a\u5408\u51fd\u6570\ndf.agg(\n    count("*").alias("total"),\n    avg("age").alias("avg_age"),\n    max("age").alias("max_age"),\n    min("age").alias("min_age"),\n    sum("salary").alias("total_salary")\n).show()\n\n# \u7a97\u53e3\u51fd\u6570\nfrom pyspark.sql.window import Window\n\n# \u6309\u90e8\u95e8\u6392\u540d\nwindow = Window.partitionBy("department").orderBy(col("salary").desc())\ndf.withColumn("rank", row_number().over(window)).show()\n\n# \u7d2f\u8ba1\u548c\nwindow = Window.partitionBy("user_id").orderBy("date")\ndf.withColumn("cumsum", sum("amount").over(window)).show()\n'})}),"\n",(0,s.jsx)(e.h3,{id:"\u5b9e\u6218\u7528\u6237\u884c\u4e3a\u5206\u6790",children:"\u5b9e\u6218\uff1a\u7528\u6237\u884c\u4e3a\u5206\u6790"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = SparkSession.builder.appName("UserBehavior").getOrCreate()\n\n# \u8bfb\u53d6\u6570\u636e\ndf = spark.read.json("user_behavior.json")\n\n# \u6570\u636e\u9884\u89c8\nprint("\u6570\u636e\u603b\u91cf:", df.count())\ndf.show(5)\ndf.printSchema()\n\n# 1. \u6bcf\u65e5\u6d3b\u8dc3\u7528\u6237\u6570\uff08DAU\uff09\ndau = df.select(\n    to_date("timestamp").alias("date"),\n    "user_id"\n).distinct().groupBy("date").count().withColumnRenamed("count", "dau")\n\ndau.orderBy("date").show()\n\n# 2. \u7528\u6237\u884c\u4e3a\u5206\u5e03\naction_dist = df.groupBy("action").count().orderBy("count", ascending=False)\naction_dist.show()\n\n# 3. \u70ed\u95e8\u5546\u54c1TOP10\ntop_items = df.filter(df.action == "view") \\\n    .groupBy("item_id") \\\n    .count() \\\n    .orderBy("count", ascending=False) \\\n    .limit(10)\n\ntop_items.show()\n\n# 4. \u7528\u6237\u6d3b\u8dc3\u5ea6\u5206\u6790\nuser_activity = df.groupBy("user_id").agg(\n    count("*").alias("total_actions"),\n    countDistinct("item_id").alias("unique_items"),\n    collect_set("action").alias("action_types")\n)\n\n# \u7528\u6237\u5206\u5c42\nuser_activity = user_activity.withColumn(\n    "user_level",\n    when(col("total_actions") >= 100, "\u9ad8\u6d3b\u8dc3")\n    .when(col("total_actions") >= 10, "\u4e2d\u6d3b\u8dc3")\n    .otherwise("\u4f4e\u6d3b\u8dc3")\n)\n\nuser_activity.groupBy("user_level").count().show()\n\n# 5. \u8f6c\u5316\u6f0f\u6597\nfunnel = df.groupBy("user_id").agg(\n    sum(when(col("action") == "view", 1).otherwise(0)).alias("views"),\n    sum(when(col("action") == "cart", 1).otherwise(0)).alias("carts"),\n    sum(when(col("action") == "purchase", 1).otherwise(0)).alias("purchases")\n)\n\nfunnel_stats = funnel.agg(\n    count("*").alias("total_users"),\n    sum("views").alias("total_views"),\n    sum("carts").alias("total_carts"),\n    sum("purchases").alias("total_purchases")\n)\n\nfunnel_stats.show()\n\n# \u8ba1\u7b97\u8f6c\u5316\u7387\nfunnel_stats = funnel_stats.withColumn(\n    "view_to_cart_rate",\n    col("total_carts") / col("total_views") * 100\n).withColumn(\n    "cart_to_purchase_rate",\n    col("total_purchases") / col("total_carts") * 100\n)\n\nfunnel_stats.show()\n\n# \u4fdd\u5b58\u7ed3\u679c\ndau.write.mode("overwrite").parquet("output/dau")\ntop_items.write.mode("overwrite").parquet("output/top_items")\nuser_activity.write.mode("overwrite").parquet("output/user_activity")\n'})}),"\n",(0,s.jsx)(e.h2,{id:"\u7b2c\u56db\u90e8\u5206spark-sql",children:"\u7b2c\u56db\u90e8\u5206\uff1aSpark SQL"}),"\n",(0,s.jsx)(e.h3,{id:"\u4f7f\u7528sql\u67e5\u8be2",children:"\u4f7f\u7528SQL\u67e5\u8be2"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# \u6ce8\u518c\u4e34\u65f6\u8868\ndf.createOrReplaceTempView("users")\n\n# SQL\u67e5\u8be2\nresult = spark.sql("""\n    SELECT \n        gender,\n        COUNT(*) as count,\n        AVG(age) as avg_age,\n        MAX(salary) as max_salary\n    FROM users\n    WHERE age > 25\n    GROUP BY gender\n    ORDER BY count DESC\n""")\n\nresult.show()\n\n# \u590d\u6742\u67e5\u8be2\nresult = spark.sql("""\n    WITH user_stats AS (\n        SELECT \n            user_id,\n            COUNT(*) as action_count,\n            COUNT(DISTINCT item_id) as unique_items\n        FROM user_behavior\n        GROUP BY user_id\n    )\n    SELECT \n        CASE \n            WHEN action_count >= 100 THEN \'\u9ad8\u6d3b\u8dc3\'\n            WHEN action_count >= 10 THEN \'\u4e2d\u6d3b\u8dc3\'\n            ELSE \'\u4f4e\u6d3b\u8dc3\'\n        END as user_level,\n        COUNT(*) as user_count,\n        AVG(unique_items) as avg_unique_items\n    FROM user_stats\n    GROUP BY user_level\n""")\n\nresult.show()\n'})}),"\n",(0,s.jsx)(e.h3,{id:"\u8fde\u63a5\u64cd\u4f5c",children:"\u8fde\u63a5\u64cd\u4f5c"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# \u8bfb\u53d6\u6570\u636e\nusers = spark.read.csv("users.csv", header=True)\norders = spark.read.csv("orders.csv", header=True)\n\n# \u5185\u8fde\u63a5\nresult = users.join(orders, users.user_id == orders.user_id, "inner")\n\n# \u5de6\u8fde\u63a5\nresult = users.join(orders, "user_id", "left")\n\n# \u53f3\u8fde\u63a5\nresult = users.join(orders, "user_id", "right")\n\n# \u5168\u5916\u8fde\u63a5\nresult = users.join(orders, "user_id", "outer")\n\n# \u591a\u8868\u8fde\u63a5\nitems = spark.read.csv("items.csv", header=True)\nresult = users.join(orders, "user_id") \\\n    .join(items, "item_id")\n\n# SQL\u65b9\u5f0f\nusers.createOrReplaceTempView("users")\norders.createOrReplaceTempView("orders")\n\nresult = spark.sql("""\n    SELECT \n        u.name,\n        u.age,\n        o.order_id,\n        o.amount\n    FROM users u\n    LEFT JOIN orders o ON u.user_id = o.user_id\n""")\n'})}),"\n",(0,s.jsx)(e.h2,{id:"\u7b2c\u4e94\u90e8\u5206\u6027\u80fd\u4f18\u5316",children:"\u7b2c\u4e94\u90e8\u5206\uff1a\u6027\u80fd\u4f18\u5316"}),"\n",(0,s.jsx)(e.h3,{id:"1-\u7f13\u5b58",children:"1. \u7f13\u5b58"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# \u7f13\u5b58\u6570\u636e\u5230\u5185\u5b58\ndf.cache()\ndf.persist()\n\n# \u4f7f\u7528\u7f13\u5b58\ndf.filter(df.age > 25).count()  # \u7b2c\u4e00\u6b21\uff1a\u4ece\u78c1\u76d8\u8bfb\u53d6\ndf.filter(df.age > 30).count()  # \u7b2c\u4e8c\u6b21\uff1a\u4ece\u5185\u5b58\u8bfb\u53d6\uff0c\u5feb\uff01\n\n# \u91ca\u653e\u7f13\u5b58\ndf.unpersist()\n"})}),"\n",(0,s.jsx)(e.h3,{id:"2-\u5206\u533a",children:"2. \u5206\u533a"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# \u91cd\u65b0\u5206\u533a\ndf.repartition(10)  # \u589e\u52a0\u5206\u533a\ndf.coalesce(2)  # \u51cf\u5c11\u5206\u533a\n\n# \u6309\u5217\u5206\u533a\ndf.repartition("date")\n\n# \u4fdd\u5b58\u65f6\u5206\u533a\ndf.write.partitionBy("date", "hour").parquet("output/")\n'})}),"\n",(0,s.jsx)(e.h3,{id:"3-\u5e7f\u64ad\u53d8\u91cf",children:"3. \u5e7f\u64ad\u53d8\u91cf"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# \u5c0f\u8868\u5e7f\u64ad\u5230\u6240\u6709\u8282\u70b9\nsmall_df = spark.read.csv("small.csv")\nfrom pyspark.sql.functions import broadcast\n\nresult = large_df.join(broadcast(small_df), "key")\n'})}),"\n",(0,s.jsx)(e.h3,{id:"4-\u907f\u514dshuffle",children:"4. \u907f\u514dShuffle"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# \u574f\uff1a\u4f1a\u89e6\u53d1shuffle\ndf.groupBy("key").count()\n\n# \u597d\uff1a\u5148\u8fc7\u6ee4\u518d\u5206\u7ec4\ndf.filter(df.value > 100).groupBy("key").count()\n\n# \u4f7f\u7528reduceByKey\u4ee3\u66ffgroupByKey\nrdd.reduceByKey(lambda x, y: x + y)  # \u597d\nrdd.groupByKey().mapValues(sum)  # \u574f\n'})}),"\n",(0,s.jsx)(e.h2,{id:"\u603b\u7ed3",children:"\u603b\u7ed3"}),"\n",(0,s.jsx)(e.p,{children:"Spark\u662f\u5927\u6570\u636e\u5904\u7406\u7684\u5229\u5668\uff1a"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"RDD"}),"\uff1a\u5e95\u5c42API\uff0c\u7075\u6d3b\u5f3a\u5927"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"DataFrame"}),"\uff1a\u9ad8\u5c42API\uff0c\u7b80\u6d01\u9ad8\u6548"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Spark SQL"}),"\uff1a\u7528SQL\u5904\u7406\u5927\u6570\u636e"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"\u6027\u80fd\u4f18\u5316"}),"\uff1a\u7f13\u5b58\u3001\u5206\u533a\u3001\u5e7f\u64ad"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"\u4e0b\u4e00\u6b65"}),"\uff1a"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"\u5b66\u4e60Spark Streaming\uff08\u5b9e\u65f6\u5904\u7406\uff09"}),"\n",(0,s.jsx)(e.li,{children:"\u5b66\u4e60Spark MLlib\uff08\u673a\u5668\u5b66\u4e60\uff09"}),"\n",(0,s.jsx)(e.li,{children:"\u5b66\u4e60Spark GraphX\uff08\u56fe\u8ba1\u7b97\uff09"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:["\u8bb0\u4f4f\uff1a",(0,s.jsx)(e.strong,{children:"Spark\u4e0d\u662f\u9b54\u6cd5\uff0c\u662f\u5de5\u5177\u3002\u591a\u7ec3\u4e60\uff01"})]}),"\n",(0,s.jsx)(e.h2,{id:"\u7ec3\u4e60\u9898",children:"\u7ec3\u4e60\u9898"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"\u5206\u6790\u6dd8\u5b9d\u7528\u6237\u884c\u4e3a\u6570\u636e\u96c6"}),"\n",(0,s.jsx)(e.li,{children:"\u5b9e\u73b0\u534f\u540c\u8fc7\u6ee4\u63a8\u8350\u7b97\u6cd5"}),"\n",(0,s.jsx)(e.li,{children:"\u5904\u7406Twitter\u60c5\u611f\u5206\u6790"}),"\n",(0,s.jsx)(e.li,{children:"\u6784\u5efa\u5b9e\u65f6\u76d1\u63a7\u7cfb\u7edf"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"\u4e0b\u4e00\u7ae0\uff1aKafka\u6d88\u606f\u961f\u5217\uff08\u5373\u5c06\u63a8\u51fa\uff09"})]})}function p(n={}){const{wrapper:e}={...(0,d.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},28453(n,e,a){a.d(e,{R:()=>l,x:()=>t});var r=a(96540);const s={},d=r.createContext(s);function l(n){const e=r.useContext(d);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:l(n.components),r.createElement(d.Provider,{value:e},n.children)}}}]);