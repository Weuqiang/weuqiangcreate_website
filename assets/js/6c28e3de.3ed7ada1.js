"use strict";(globalThis.webpackChunkweuqiangcreate_website=globalThis.webpackChunkweuqiangcreate_website||[]).push([[72593],{390(n,e,t){t.r(e),t.d(e,{assets:()=>d,contentTitle:()=>o,default:()=>m,frontMatter:()=>l,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"\u4eba\u5de5\u667a\u80fd/AI\u5b9e\u6218\u9879\u76ee","title":"AI\u5b9e\u6218\u9879\u76ee","description":"\u672c\u7ae0\u8282\u63d0\u4f9b\u5b8c\u6574\u7684AI\u5b9e\u6218\u9879\u76ee\u6848\u4f8b\uff0c\u4ece\u6570\u636e\u51c6\u5907\u5230\u6a21\u578b\u90e8\u7f72\u7684\u5168\u6d41\u7a0b\u3002","source":"@site/docs/docs/\u4eba\u5de5\u667a\u80fd/AI\u5b9e\u6218\u9879\u76ee.mdx","sourceDirName":"\u4eba\u5de5\u667a\u80fd","slug":"/\u4eba\u5de5\u667a\u80fd/AI\u5b9e\u6218\u9879\u76ee","permalink":"/weuqiangcreate_website/docs/\u4eba\u5de5\u667a\u80fd/AI\u5b9e\u6218\u9879\u76ee","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":14,"frontMatter":{"sidebar_position":14,"title":"AI\u5b9e\u6218\u9879\u76ee"},"sidebar":"tutorialSidebar","previous":{"title":"\u751f\u6210\u5f0fAI (AIGC)","permalink":"/weuqiangcreate_website/docs/\u4eba\u5de5\u667a\u80fd/\u751f\u6210\u5f0fAI"},"next":{"title":"\u524d\u6cbfAI\u6280\u672f","permalink":"/weuqiangcreate_website/docs/\u4eba\u5de5\u667a\u80fd/\u524d\u6cbfAI\u6280\u672f"}}');var i=t(74848),r=t(28453);const l={sidebar_position:14,title:"AI\u5b9e\u6218\u9879\u76ee"},o="AI\u5b9e\u6218\u9879\u76ee",d={},a=[{value:"\u9879\u76ee1\uff1a\u667a\u80fd\u56fe\u50cf\u8bc6\u522b\u7cfb\u7edf",id:"\u9879\u76ee1\u667a\u80fd\u56fe\u50cf\u8bc6\u522b\u7cfb\u7edf",level:2},{value:"\u9879\u76ee\u6982\u8ff0",id:"\u9879\u76ee\u6982\u8ff0",level:3},{value:"\u6280\u672f\u6808",id:"\u6280\u672f\u6808",level:3},{value:"\u5b8c\u6574\u4ee3\u7801",id:"\u5b8c\u6574\u4ee3\u7801",level:3},{value:"Docker\u90e8\u7f72",id:"docker\u90e8\u7f72",level:3},{value:"\u9879\u76ee2\uff1a\u667a\u80fd\u5bf9\u8bdd\u673a\u5668\u4eba",id:"\u9879\u76ee2\u667a\u80fd\u5bf9\u8bdd\u673a\u5668\u4eba",level:2},{value:"\u9879\u76ee\u6982\u8ff0",id:"\u9879\u76ee\u6982\u8ff0-1",level:3},{value:"\u6838\u5fc3\u529f\u80fd",id:"\u6838\u5fc3\u529f\u80fd",level:3},{value:"\u5b8c\u6574\u5b9e\u73b0",id:"\u5b8c\u6574\u5b9e\u73b0",level:3},{value:"\u9879\u76ee3\uff1a\u6587\u672c\u60c5\u611f\u5206\u6790\u7cfb\u7edf",id:"\u9879\u76ee3\u6587\u672c\u60c5\u611f\u5206\u6790\u7cfb\u7edf",level:2},{value:"\u9879\u76ee\u6982\u8ff0",id:"\u9879\u76ee\u6982\u8ff0-2",level:3},{value:"\u5b8c\u6574\u4ee3\u7801",id:"\u5b8c\u6574\u4ee3\u7801-1",level:3},{value:"\u9879\u76ee4\uff1a\u63a8\u8350\u7cfb\u7edf",id:"\u9879\u76ee4\u63a8\u8350\u7cfb\u7edf",level:2},{value:"\u9879\u76ee\u6982\u8ff0",id:"\u9879\u76ee\u6982\u8ff0-3",level:3},{value:"\u6838\u5fc3\u4ee3\u7801",id:"\u6838\u5fc3\u4ee3\u7801",level:3},{value:"\u9879\u76ee\u90e8\u7f72",id:"\u9879\u76ee\u90e8\u7f72",level:2},{value:"\u6a21\u578b\u4f18\u5316",id:"\u6a21\u578b\u4f18\u5316",level:3},{value:"API\u670d\u52a1",id:"api\u670d\u52a1",level:3},{value:"\u603b\u7ed3",id:"\u603b\u7ed3",level:2}];function c(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components},{DocCardList:t}=e;return t||function(n,e){throw new Error("Expected "+(e?"component":"object")+" `"+n+"` to be defined: you likely forgot to import, pass, or provide it.")}("DocCardList",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"ai\u5b9e\u6218\u9879\u76ee",children:"AI\u5b9e\u6218\u9879\u76ee"})}),"\n",(0,i.jsx)(e.admonition,{title:"\u7ae0\u8282\u6982\u8ff0",type:"info",children:(0,i.jsx)(e.p,{children:"\u672c\u7ae0\u8282\u63d0\u4f9b\u5b8c\u6574\u7684AI\u5b9e\u6218\u9879\u76ee\u6848\u4f8b\uff0c\u4ece\u6570\u636e\u51c6\u5907\u5230\u6a21\u578b\u90e8\u7f72\u7684\u5168\u6d41\u7a0b\u3002"})}),"\n",(0,i.jsx)(e.h2,{id:"\u9879\u76ee1\u667a\u80fd\u56fe\u50cf\u8bc6\u522b\u7cfb\u7edf",children:"\u9879\u76ee1\uff1a\u667a\u80fd\u56fe\u50cf\u8bc6\u522b\u7cfb\u7edf"}),"\n",(0,i.jsx)(e.h3,{id:"\u9879\u76ee\u6982\u8ff0",children:"\u9879\u76ee\u6982\u8ff0"}),"\n",(0,i.jsx)(e.p,{children:"\u6784\u5efa\u4e00\u4e2a\u5b8c\u6574\u7684\u56fe\u50cf\u8bc6\u522bWeb\u5e94\u7528\uff0c\u652f\u6301\u4e0a\u4f20\u56fe\u50cf\u5e76\u8bc6\u522b\u5176\u4e2d\u7684\u7269\u4f53\u3002"}),"\n",(0,i.jsx)(e.h3,{id:"\u6280\u672f\u6808",children:"\u6280\u672f\u6808"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u540e\u7aef"}),": FastAPI"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u6a21\u578b"}),": YOLOv8"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u524d\u7aef"}),": Streamlit"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u90e8\u7f72"}),": Docker"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"\u5b8c\u6574\u4ee3\u7801",children:"\u5b8c\u6574\u4ee3\u7801"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# app.py\nfrom fastapi import FastAPI, File, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom ultralytics import YOLO\nfrom PIL import Image\nimport io\n\napp = FastAPI()\nmodel = YOLO('yolov8n.pt')\n\n@app.post(\"/detect\")\nasync def detect_objects(file: UploadFile = File(...)):\n    \"\"\"\u68c0\u6d4b\u56fe\u50cf\u4e2d\u7684\u7269\u4f53\"\"\"\n    # \u8bfb\u53d6\u56fe\u50cf\n    contents = await file.read()\n    image = Image.open(io.BytesIO(contents))\n    \n    # \u68c0\u6d4b\n    results = model(image)\n    \n    # \u89e3\u6790\u7ed3\u679c\n    detections = []\n    for result in results:\n        boxes = result.boxes\n        for box in boxes:\n            detections.append({\n                'class': result.names[int(box.cls)],\n                'confidence': float(box.conf),\n                'bbox': box.xyxy[0].tolist()\n            })\n    \n    return JSONResponse(content={'detections': detections})\n\n# streamlit_app.py\nimport streamlit as st\nimport requests\nfrom PIL import Image\n\nst.title(\"\u667a\u80fd\u56fe\u50cf\u8bc6\u522b\u7cfb\u7edf\")\n\nuploaded_file = st.file_uploader(\"\u4e0a\u4f20\u56fe\u50cf\", type=['jpg', 'png'])\n\nif uploaded_file:\n    image = Image.open(uploaded_file)\n    st.image(image, caption='\u4e0a\u4f20\u7684\u56fe\u50cf')\n    \n    if st.button('\u8bc6\u522b'):\n        # \u8c03\u7528API\n        files = {'file': uploaded_file.getvalue()}\n        response = requests.post('http://localhost:8000/detect', files=files)\n        \n        if response.status_code == 200:\n            results = response.json()['detections']\n            st.write(\"\u8bc6\u522b\u7ed3\u679c\uff1a\")\n            for det in results:\n                st.write(f\"- {det['class']}: {det['confidence']:.2%}\")\n"})}),"\n",(0,i.jsx)(e.h3,{id:"docker\u90e8\u7f72",children:"Docker\u90e8\u7f72"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-dockerfile",children:'# Dockerfile\nFROM python:3.9\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]\n'})}),"\n",(0,i.jsx)(e.h2,{id:"\u9879\u76ee2\u667a\u80fd\u5bf9\u8bdd\u673a\u5668\u4eba",children:"\u9879\u76ee2\uff1a\u667a\u80fd\u5bf9\u8bdd\u673a\u5668\u4eba"}),"\n",(0,i.jsx)(e.h3,{id:"\u9879\u76ee\u6982\u8ff0-1",children:"\u9879\u76ee\u6982\u8ff0"}),"\n",(0,i.jsx)(e.p,{children:"\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u4e00\u4e2a\u5177\u6709\u8bb0\u5fc6\u529f\u80fd\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\u3002"}),"\n",(0,i.jsx)(e.h3,{id:"\u6838\u5fc3\u529f\u80fd",children:"\u6838\u5fc3\u529f\u80fd"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"\u591a\u8f6e\u5bf9\u8bdd"}),"\n",(0,i.jsx)(e.li,{children:"\u4e0a\u4e0b\u6587\u8bb0\u5fc6"}),"\n",(0,i.jsx)(e.li,{children:"\u5de5\u5177\u8c03\u7528"}),"\n",(0,i.jsx)(e.li,{children:"\u77e5\u8bc6\u5e93\u68c0\u7d22"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"\u5b8c\u6574\u5b9e\u73b0",children:"\u5b8c\u6574\u5b9e\u73b0"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from langchain.llms import OpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import OpenAIEmbeddings\n\nclass ChatBot:\n    """\u667a\u80fd\u5bf9\u8bdd\u673a\u5668\u4eba"""\n    \n    def __init__(self):\n        # LLM\n        self.llm = OpenAI(temperature=0.7)\n        \n        # \u8bb0\u5fc6\n        self.memory = ConversationBufferMemory()\n        \n        # \u5bf9\u8bdd\u94fe\n        self.conversation = ConversationChain(\n            llm=self.llm,\n            memory=self.memory,\n            verbose=True\n        )\n        \n        # \u77e5\u8bc6\u5e93\n        self.knowledge_base = self._init_knowledge_base()\n    \n    def chat(self, user_input):\n        """\u5bf9\u8bdd"""\n        # \u68c0\u7d22\u76f8\u5173\u77e5\u8bc6\n        relevant_docs = self.knowledge_base.similarity_search(user_input, k=3)\n        context = "\\n".join([doc.page_content for doc in relevant_docs])\n        \n        # \u6784\u5efaprompt\n        prompt = f"""\u57fa\u4e8e\u4ee5\u4e0b\u77e5\u8bc6\u56de\u7b54\u95ee\u9898\uff1a\n\n\u77e5\u8bc6\uff1a\n{context}\n\n\u95ee\u9898\uff1a{user_input}\n\n\u56de\u7b54\uff1a"""\n        \n        # \u751f\u6210\u56de\u590d\n        response = self.conversation.predict(input=prompt)\n        \n        return response\n    \n    def _init_knowledge_base(self):\n        """\u521d\u59cb\u5316\u77e5\u8bc6\u5e93"""\n        documents = [\n            "\u4eba\u5de5\u667a\u80fd\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4e00\u4e2a\u5206\u652f...",\n            "\u673a\u5668\u5b66\u4e60\u662f\u5b9e\u73b0\u4eba\u5de5\u667a\u80fd\u7684\u65b9\u6cd5...",\n            "\u6df1\u5ea6\u5b66\u4e60\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc..."\n        ]\n        \n        embeddings = OpenAIEmbeddings()\n        vectorstore = FAISS.from_texts(documents, embeddings)\n        \n        return vectorstore\n\n# Web\u754c\u9762\nimport gradio as gr\n\nbot = ChatBot()\n\ndef respond(message, history):\n    response = bot.chat(message)\n    return response\n\ndemo = gr.ChatInterface(\n    respond,\n    title="\u667a\u80fd\u5bf9\u8bdd\u673a\u5668\u4eba",\n    description="\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u52a9\u624b"\n)\n\ndemo.launch()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"\u9879\u76ee3\u6587\u672c\u60c5\u611f\u5206\u6790\u7cfb\u7edf",children:"\u9879\u76ee3\uff1a\u6587\u672c\u60c5\u611f\u5206\u6790\u7cfb\u7edf"}),"\n",(0,i.jsx)(e.h3,{id:"\u9879\u76ee\u6982\u8ff0-2",children:"\u9879\u76ee\u6982\u8ff0"}),"\n",(0,i.jsx)(e.p,{children:"\u6784\u5efa\u4e00\u4e2a\u5b9e\u65f6\u6587\u672c\u60c5\u611f\u5206\u6790\u7cfb\u7edf\uff0c\u652f\u6301\u6279\u91cf\u5904\u7406\u548c\u53ef\u89c6\u5316\u3002"}),"\n",(0,i.jsx)(e.h3,{id:"\u5b8c\u6574\u4ee3\u7801-1",children:"\u5b8c\u6574\u4ee3\u7801"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import pandas as pd\nimport matplotlib.pyplot as plt\nfrom transformers import pipeline\nimport streamlit as st\n\nclass SentimentAnalysisSystem:\n    """\u60c5\u611f\u5206\u6790\u7cfb\u7edf"""\n    \n    def __init__(self):\n        self.analyzer = pipeline(\n            "sentiment-analysis",\n            model="distilbert-base-uncased-finetuned-sst-2-english"\n        )\n    \n    def analyze_text(self, text):\n        """\u5206\u6790\u5355\u6761\u6587\u672c"""\n        result = self.analyzer(text)[0]\n        return {\n            \'text\': text,\n            \'sentiment\': result[\'label\'],\n            \'score\': result[\'score\']\n        }\n    \n    def analyze_batch(self, texts):\n        """\u6279\u91cf\u5206\u6790"""\n        results = []\n        for text in texts:\n            result = self.analyze_text(text)\n            results.append(result)\n        return pd.DataFrame(results)\n    \n    def visualize(self, df):\n        """\u53ef\u89c6\u5316\u7ed3\u679c"""\n        sentiment_counts = df[\'sentiment\'].value_counts()\n        \n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n        \n        # \u997c\u56fe\n        ax1.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct=\'%1.1f%%\')\n        ax1.set_title(\'\u60c5\u611f\u5206\u5e03\')\n        \n        # \u67f1\u72b6\u56fe\n        ax2.bar(sentiment_counts.index, sentiment_counts.values)\n        ax2.set_title(\'\u60c5\u611f\u7edf\u8ba1\')\n        ax2.set_ylabel(\'\u6570\u91cf\')\n        \n        return fig\n\n# Streamlit\u5e94\u7528\ndef main():\n    st.title("\u6587\u672c\u60c5\u611f\u5206\u6790\u7cfb\u7edf")\n    \n    system = SentimentAnalysisSystem()\n    \n    # \u5355\u6587\u672c\u5206\u6790\n    st.header("\u5355\u6587\u672c\u5206\u6790")\n    text = st.text_area("\u8f93\u5165\u6587\u672c")\n    if st.button("\u5206\u6790"):\n        result = system.analyze_text(text)\n        st.write(f"\u60c5\u611f: {result[\'sentiment\']}")\n        st.write(f"\u7f6e\u4fe1\u5ea6: {result[\'score\']:.2%}")\n    \n    # \u6279\u91cf\u5206\u6790\n    st.header("\u6279\u91cf\u5206\u6790")\n    uploaded_file = st.file_uploader("\u4e0a\u4f20CSV\u6587\u4ef6", type=[\'csv\'])\n    if uploaded_file:\n        df = pd.read_csv(uploaded_file)\n        texts = df[\'text\'].tolist()\n        \n        if st.button("\u6279\u91cf\u5206\u6790"):\n            results = system.analyze_batch(texts)\n            st.dataframe(results)\n            \n            # \u53ef\u89c6\u5316\n            fig = system.visualize(results)\n            st.pyplot(fig)\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"\u9879\u76ee4\u63a8\u8350\u7cfb\u7edf",children:"\u9879\u76ee4\uff1a\u63a8\u8350\u7cfb\u7edf"}),"\n",(0,i.jsx)(e.h3,{id:"\u9879\u76ee\u6982\u8ff0-3",children:"\u9879\u76ee\u6982\u8ff0"}),"\n",(0,i.jsx)(e.p,{children:"\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e\u534f\u540c\u8fc7\u6ee4\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u6df7\u5408\u63a8\u8350\u7cfb\u7edf\u3002"}),"\n",(0,i.jsx)(e.h3,{id:"\u6838\u5fc3\u4ee3\u7801",children:"\u6838\u5fc3\u4ee3\u7801"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nclass RecommenderModel(nn.Module):\n    """\u63a8\u8350\u6a21\u578b"""\n    \n    def __init__(self, num_users, num_items, embedding_dim=50):\n        super().__init__()\n        \n        # \u7528\u6237\u548c\u7269\u54c1\u5d4c\u5165\n        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n        \n        # MLP\u5c42\n        self.fc = nn.Sequential(\n            nn.Linear(embedding_dim * 2, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, user_ids, item_ids):\n        # \u83b7\u53d6\u5d4c\u5165\n        user_emb = self.user_embedding(user_ids)\n        item_emb = self.item_embedding(item_ids)\n        \n        # \u62fc\u63a5\n        x = torch.cat([user_emb, item_emb], dim=-1)\n        \n        # \u9884\u6d4b\u8bc4\u5206\n        score = self.fc(x)\n        \n        return score\n\nclass RecommenderSystem:\n    """\u63a8\u8350\u7cfb\u7edf"""\n    \n    def __init__(self, num_users, num_items):\n        self.model = RecommenderModel(num_users, num_items)\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n        self.criterion = nn.BCELoss()\n    \n    def train(self, train_loader, epochs=10):\n        """\u8bad\u7ec3\u6a21\u578b"""\n        self.model.train()\n        \n        for epoch in range(epochs):\n            total_loss = 0\n            for user_ids, item_ids, ratings in train_loader:\n                # \u524d\u5411\u4f20\u64ad\n                predictions = self.model(user_ids, item_ids).squeeze()\n                loss = self.criterion(predictions, ratings.float())\n                \n                # \u53cd\u5411\u4f20\u64ad\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                \n                total_loss += loss.item()\n            \n            print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}")\n    \n    def recommend(self, user_id, top_k=10):\n        """\u4e3a\u7528\u6237\u63a8\u8350\u7269\u54c1"""\n        self.model.eval()\n        \n        # \u8ba1\u7b97\u6240\u6709\u7269\u54c1\u7684\u5f97\u5206\n        item_ids = torch.arange(self.model.item_embedding.num_embeddings)\n        user_ids = torch.full_like(item_ids, user_id)\n        \n        with torch.no_grad():\n            scores = self.model(user_ids, item_ids).squeeze()\n        \n        # \u83b7\u53d6top-k\n        top_items = torch.topk(scores, top_k).indices.tolist()\n        \n        return top_items\n'})}),"\n",(0,i.jsx)(e.h2,{id:"\u9879\u76ee\u90e8\u7f72",children:"\u9879\u76ee\u90e8\u7f72"}),"\n",(0,i.jsx)(e.h3,{id:"\u6a21\u578b\u4f18\u5316",children:"\u6a21\u578b\u4f18\u5316"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# \u6a21\u578b\u91cf\u5316\nimport torch.quantization as quantization\n\ndef quantize_model(model):\n    """\u91cf\u5316\u6a21\u578b"""\n    model.eval()\n    model_quantized = quantization.quantize_dynamic(\n        model,\n        {nn.Linear},\n        dtype=torch.qint8\n    )\n    return model_quantized\n\n# ONNX\u5bfc\u51fa\ndef export_to_onnx(model, dummy_input, output_path):\n    """\u5bfc\u51fa\u4e3aONNX\u683c\u5f0f"""\n    torch.onnx.export(\n        model,\n        dummy_input,\n        output_path,\n        export_params=True,\n        opset_version=11,\n        input_names=[\'input\'],\n        output_names=[\'output\']\n    )\n'})}),"\n",(0,i.jsx)(e.h3,{id:"api\u670d\u52a1",children:"API\u670d\u52a1"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass PredictionRequest(BaseModel):\n    user_id: int\n    top_k: int = 10\n\n@app.post("/recommend")\nasync def recommend(request: PredictionRequest):\n    """\u63a8\u8350\u63a5\u53e3"""\n    recommendations = recommender.recommend(\n        request.user_id,\n        request.top_k\n    )\n    return {\'recommendations\': recommendations}\n'})}),"\n",(0,i.jsx)(e.h2,{id:"\u603b\u7ed3",children:"\u603b\u7ed3"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"\u9879\u76ee\u8981\u70b9"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"\u5b8c\u6574\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b"}),"\n",(0,i.jsx)(e.li,{children:"\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30"}),"\n",(0,i.jsx)(e.li,{children:"Web\u754c\u9762\u5f00\u53d1"}),"\n",(0,i.jsx)(e.li,{children:"\u6a21\u578b\u90e8\u7f72\u548c\u4f18\u5316"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"\u5b66\u4e60\u5efa\u8bae"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"\u4ece\u7b80\u5355\u9879\u76ee\u5f00\u59cb"}),"\n",(0,i.jsx)(e.li,{children:"\u6ce8\u91cd\u5de5\u7a0b\u5b9e\u8df5"}),"\n",(0,i.jsx)(e.li,{children:"\u5173\u6ce8\u6027\u80fd\u4f18\u5316"}),"\n",(0,i.jsx)(e.li,{children:"\u6301\u7eed\u8fed\u4ee3\u6539\u8fdb"}),"\n"]}),"\n",(0,i.jsx)(t,{})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(c,{...n})}):c(n)}},28453(n,e,t){t.d(e,{R:()=>l,x:()=>o});var s=t(96540);const i={},r=s.createContext(i);function l(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:l(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);