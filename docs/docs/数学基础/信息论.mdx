---
sidebar_position: 4
title:  - 
---

#  - AI

****Transformer

##  - 

### 

****

```python
import numpy as np
import matplotlib.pyplot as plt

def entropy(p):
    """H(X) = -Σ p(x) * log(p(x))"""
    p = np.array(p)
    p = p[p > 0]  # 0
    return -np.sum(p * np.log2(p))

# 1
p_fair = [0.5, 0.5]  # 
p_biased = [0.9, 0.1]  # 
p_certain = [1.0, 0.0]  # 

print(":", entropy(p_fair))  # 1.0 bit
print(":", entropy(p_biased))  # 0.47 bit
print(":", entropy(p_certain))  # 0.0 bit

# 
p_values = np.linspace(0.01, 0.99, 100)
h_values = [-p * np.log2(p) - (1-p) * np.log2(1-p) for p in p_values]

plt.figure(figsize=(10, 6))
plt.plot(p_values, h_values, linewidth=2)
plt.xlabel(' p')
plt.ylabel(' H(p)')
plt.title('')
plt.grid(True)
plt.axvline(x=0.5, color='r', linestyle='--', label='')
plt.legend()
plt.show()
```

****
-  = 0
-  = 1
- 

### AI

```python
class DecisionTree:
    """"""
    
    def __init__(self, max_depth=3):
        self.max_depth = max_depth
    
    def entropy(self, y):
        """"""
        _, counts = np.unique(y, return_counts=True)
        probs = counts / len(y)
        return -np.sum(probs * np.log2(probs + 1e-10))
    
    def information_gain(self, X, y, feature_idx, threshold):
        """"""
        # 
        parent_entropy = self.entropy(y)
        
        # 
        left_mask = X[:, feature_idx] <= threshold
        right_mask = ~left_mask
        
        if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:
            return 0
        
        # 
        n = len(y)
        left_entropy = self.entropy(y[left_mask])
        right_entropy = self.entropy(y[right_mask])
        
        # 
        child_entropy = (np.sum(left_mask) / n * left_entropy +
                        np.sum(right_mask) / n * right_entropy)
        
        # 
        return parent_entropy - child_entropy
    
    def find_best_split(self, X, y):
        """"""
        best_gain = 0
        best_feature = None
        best_threshold = None
        
        for feature_idx in range(X.shape[1]):
            thresholds = np.unique(X[:, feature_idx])
            
            for threshold in thresholds:
                gain = self.information_gain(X, y, feature_idx, threshold)
                
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature_idx
                    best_threshold = threshold
        
        return best_feature, best_threshold, best_gain

# 
from sklearn.datasets import load_iris
iris = load_iris()
X, y = iris.data, iris.target

tree = DecisionTree()
feature, threshold, gain = tree.find_best_split(X, y)

print(f": {iris.feature_names[feature]}")
print(f": {threshold:.2f}")
print(f": {gain:.4f}")
```

##  - 

### 

****

```python
def cross_entropy(y_true, y_pred):
    """H(p, q) = -Σ p(x) * log(q(x))"""
    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)  # log(0)
    return -np.sum(y_true * np.log(y_pred))

# 
y_true = np.array([0, 0, 1])  # 2
y_pred1 = np.array([0.1, 0.2, 0.7])  # 1
y_pred2 = np.array([0.4, 0.4, 0.2])  # 2

print("1:", cross_entropy(y_true, y_pred1))  # 0.36
print("2:", cross_entropy(y_true, y_pred2))  # 1.61

# 
pred_probs = np.linspace(0.01, 0.99, 100)
ce_values = [-np.log(p) for p in pred_probs]

plt.figure(figsize=(10, 6))
plt.plot(pred_probs, ce_values, linewidth=2)
plt.xlabel('')
plt.ylabel('')
plt.title('')
plt.grid(True)
plt.show()
```

### 

```python
# 
def mse_loss(y_true, y_pred):
    """"""
    return np.mean((y_true - y_pred) ** 2)

def cross_entropy_loss(y_true, y_pred):
    """"""
    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)
    return -np.mean(y_true * np.log(y_pred))

# 
y_true = np.array([1, 0])  # 0

# 
predictions = np.linspace(0.01, 0.99, 100)
mse_losses = [mse_loss(y_true, np.array([p, 1-p])) for p in predictions]
ce_losses = [cross_entropy_loss(y_true, np.array([p, 1-p])) for p in predictions]

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(predictions, mse_losses, label='MSE', linewidth=2)
plt.xlabel('')
plt.ylabel('')
plt.title('')
plt.grid(True)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(predictions, ce_losses, label='Cross Entropy', linewidth=2, color='orange')
plt.xlabel('')
plt.ylabel('')
plt.title('')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

print("")
print("1. ")
print("2. ")
print("3. ")
```

### Softmax + 

```python
class SoftmaxCrossEntropy:
    """Softmax + """
    
    @staticmethod
    def softmax(x):
        """Softmax"""
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    @staticmethod
    def forward(logits, labels):
        """"""
        probs = SoftmaxCrossEntropy.softmax(logits)
        
        # 
        m = labels.shape[0]
        log_probs = -np.log(probs[range(m), labels])
        loss = np.sum(log_probs) / m
        
        return loss, probs
    
    @staticmethod
    def backward(probs, labels):
        """"""
        m = labels.shape[0]
        grad = probs.copy()
        grad[range(m), labels] -= 1
        grad /= m
        return grad

# 
logits = np.array([
    [2.0, 1.0, 0.1],
    [0.5, 2.5, 0.3],
    [1.0, 0.5, 2.0]
])
labels = np.array([0, 1, 2])

loss, probs = SoftmaxCrossEntropy.forward(logits, labels)
grad = SoftmaxCrossEntropy.backward(probs, labels)

print(":")
print(probs)
print(f"\n: {loss:.4f}")
print("\n:")
print(grad)
```

## KL - 

### KL

**KL**

```python
def kl_divergence(p, q):
    """KLD_KL(P||Q) = Σ p(x) * log(p(x) / q(x))"""
    p = np.array(p)
    q = np.array(q)
    q = np.clip(q, 1e-10, 1)
    return np.sum(p * np.log(p / q))

# 
p = np.array([0.5, 0.3, 0.2])  # 
q1 = np.array([0.5, 0.3, 0.2])  # 
q2 = np.array([0.4, 0.4, 0.2])  # 
q3 = np.array([0.2, 0.3, 0.5])  # 

print("KL(P||Q1):", kl_divergence(p, q1))  # 0.0
print("KL(P||Q2):", kl_divergence(p, q2))  # 0.02
print("KL(P||Q3):", kl_divergence(p, q3))  # 0.33

# KL
print("\nKL")
print("1. D_KL(P||Q) >= 0")
print("2. D_KL(P||Q) != D_KL(Q||P)")
print("3. P=Q0")

# 
print(f"\nD_KL(P||Q3) = {kl_divergence(p, q3):.4f}")
print(f"D_KL(Q3||P) = {kl_divergence(q3, p):.4f}")
```

### AIVAE

```python
class VAE:
    """"""
    
    @staticmethod
    def kl_loss(mu, log_var):
        """KLD_KL(N(μ,σ²) || N(0,1))"""
        # KL
        kl = -0.5 * np.sum(1 + log_var - mu**2 - np.exp(log_var))
        return kl
    
    @staticmethod
    def reconstruction_loss(x, x_recon):
        """"""
        return np.sum((x - x_recon) ** 2)
    
    @staticmethod
    def total_loss(x, x_recon, mu, log_var, beta=1.0):
        """ =  + β * KL"""
        recon_loss = VAE.reconstruction_loss(x, x_recon)
        kl_loss = VAE.kl_loss(mu, log_var)
        return recon_loss + beta * kl_loss

# 
x = np.random.randn(10)
x_recon = x + np.random.randn(10) * 0.1
mu = np.random.randn(5)
log_var = np.random.randn(5)

recon = VAE.reconstruction_loss(x, x_recon)
kl = VAE.kl_loss(mu, log_var)
total = VAE.total_loss(x, x_recon, mu, log_var)

print(f": {recon:.4f}")
print(f"KL: {kl:.4f}")
print(f": {total:.4f}")
```

##  - 

### 

****

```python
def mutual_information(x, y, bins=10):
    """I(X;Y) = H(X) + H(Y) - H(X,Y)"""
    # 
    hist_xy, _, _ = np.histogram2d(x, y, bins=bins)
    hist_xy = hist_xy / np.sum(hist_xy)
    
    # 
    hist_x = np.sum(hist_xy, axis=1)
    hist_y = np.sum(hist_xy, axis=0)
    
    # 
    def entropy_from_hist(hist):
        hist = hist[hist > 0]
        return -np.sum(hist * np.log2(hist))
    
    h_x = entropy_from_hist(hist_x)
    h_y = entropy_from_hist(hist_y)
    h_xy = entropy_from_hist(hist_xy.flatten())
    
    # 
    mi = h_x + h_y - h_xy
    return mi

# 
n = 1000

# 
x1 = np.random.randn(n)
y1 = np.random.randn(n)

# 
x2 = np.random.randn(n)
y2 = x2 + np.random.randn(n) * 0.1

# 
x3 = np.random.randn(n)
y3 = x3

print(":", mutual_information(x1, y1))
print(":", mutual_information(x2, y2))
print(":", mutual_information(x3, y3))
```

### 

```python
class Attention:
    """"""
    
    @staticmethod
    def scaled_dot_product_attention(Q, K, V):
        """"""
        # Q: (batch, seq_len, d_k)
        # K: (batch, seq_len, d_k)
        # V: (batch, seq_len, d_v)
        
        d_k = Q.shape[-1]
        
        # 
        scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)
        
        # Softmax
        attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
        attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)
        
        # 
        output = np.matmul(attention_weights, V)
        
        return output, attention_weights
    
    @staticmethod
    def visualize_attention(attention_weights, tokens):
        """"""
        plt.figure(figsize=(10, 8))
        plt.imshow(attention_weights, cmap='hot', interpolation='nearest')
        plt.colorbar()
        plt.xticks(range(len(tokens)), tokens, rotation=45)
        plt.yticks(range(len(tokens)), tokens)
        plt.title('')
        plt.tight_layout()
        plt.show()

# 
seq_len = 5
d_k = 64
d_v = 64

Q = np.random.randn(1, seq_len, d_k)
K = np.random.randn(1, seq_len, d_k)
V = np.random.randn(1, seq_len, d_v)

output, attention_weights = Attention.scaled_dot_product_attention(Q, K, V)

print(":", output.shape)
print(":", attention_weights.shape)
print("\n1:")
print(attention_weights[0])
print("\n:", np.sum(attention_weights[0], axis=1))

# 
tokens = ['', '', '', 'AI', '']
Attention.visualize_attention(attention_weights[0], tokens)
```

## 

AI

1. ****
2. ****
3. **KL**VAE
4. ****

**AI**

## 

1. 
2. MSE
3. VAE
4. 

[](./.mdx)

