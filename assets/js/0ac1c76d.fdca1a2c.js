"use strict";(globalThis.webpackChunkweuqiangcreate_website=globalThis.webpackChunkweuqiangcreate_website||[]).push([[24550],{62116(n,e,s){s.r(e),s.d(e,{assets:()=>a,contentTitle:()=>l,default:()=>f,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"\u4eba\u5de5\u667a\u80fd/\u8ba1\u7b97\u673a\u89c6\u89c9","title":"\u8ba1\u7b97\u673a\u89c6\u89c9","description":"\u672c\u7ae0\u8282\u4ecb\u7ecd\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6838\u5fc3\u6280\u672f\uff0c\u5305\u62ec\u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u3001\u56fe\u50cf\u5206\u5272\u7b49\u7ecf\u5178\u4efb\u52a1\u3002","source":"@site/docs/docs/\u4eba\u5de5\u667a\u80fd/\u8ba1\u7b97\u673a\u89c6\u89c9.mdx","sourceDirName":"\u4eba\u5de5\u667a\u80fd","slug":"/\u4eba\u5de5\u667a\u80fd/\u8ba1\u7b97\u673a\u89c6\u89c9","permalink":"/weuqiangcreate_website/docs/\u4eba\u5de5\u667a\u80fd/\u8ba1\u7b97\u673a\u89c6\u89c9","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"title":"\u8ba1\u7b97\u673a\u89c6\u89c9"},"sidebar":"tutorialSidebar","previous":{"title":"\u795e\u7ecf\u7f51\u7edc\u57fa\u7840","permalink":"/weuqiangcreate_website/docs/\u4eba\u5de5\u667a\u80fd/\u795e\u7ecf\u7f51\u7edc\u57fa\u7840"},"next":{"title":"\u81ea\u7136\u8bed\u8a00\u5904\u7406","permalink":"/weuqiangcreate_website/docs/\u4eba\u5de5\u667a\u80fd/\u81ea\u7136\u8bed\u8a00\u5904\u7406"}}');var o=s(74848),i=s(28453);const r={sidebar_position:7,title:"\u8ba1\u7b97\u673a\u89c6\u89c9"},l="\u8ba1\u7b97\u673a\u89c6\u89c9\uff08Computer Vision\uff09",a={},c=[{value:"\u4ec0\u4e48\u662f\u8ba1\u7b97\u673a\u89c6\u89c9",id:"\u4ec0\u4e48\u662f\u8ba1\u7b97\u673a\u89c6\u89c9",level:2},{value:"\u6838\u5fc3\u4efb\u52a1",id:"\u6838\u5fc3\u4efb\u52a1",level:3},{value:"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09",id:"\u5377\u79ef\u795e\u7ecf\u7f51\u7edccnn",level:2},{value:"CNN\u57fa\u7840\u67b6\u6784",id:"cnn\u57fa\u7840\u67b6\u6784",level:3},{value:"\u7ecf\u5178CNN\u67b6\u6784",id:"\u7ecf\u5178cnn\u67b6\u6784",level:3},{value:"\u56fe\u50cf\u5206\u7c7b",id:"\u56fe\u50cf\u5206\u7c7b",level:2},{value:"\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b",id:"\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b",level:3},{value:"\u76ee\u6807\u68c0\u6d4b",id:"\u76ee\u6807\u68c0\u6d4b",level:2},{value:"YOLO\u5b9e\u73b0",id:"yolo\u5b9e\u73b0",level:3},{value:"\u56fe\u50cf\u5206\u5272",id:"\u56fe\u50cf\u5206\u5272",level:2},{value:"\u8bed\u4e49\u5206\u5272",id:"\u8bed\u4e49\u5206\u5272",level:3},{value:"\u4eba\u8138\u8bc6\u522b",id:"\u4eba\u8138\u8bc6\u522b",level:2},{value:"\u5b9e\u6218\u9879\u76ee",id:"\u5b9e\u6218\u9879\u76ee",level:2},{value:"\u9879\u76ee1\uff1a\u56fe\u50cf\u5206\u7c7b\u5e94\u7528",id:"\u9879\u76ee1\u56fe\u50cf\u5206\u7c7b\u5e94\u7528",level:3},{value:"\u9879\u76ee2\uff1a\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",id:"\u9879\u76ee2\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",level:3},{value:"\u6700\u4f73\u5b9e\u8df5",id:"\u6700\u4f73\u5b9e\u8df5",level:2},{value:"\u6570\u636e\u589e\u5f3a",id:"\u6570\u636e\u589e\u5f3a",level:3},{value:"\u8fc1\u79fb\u5b66\u4e60",id:"\u8fc1\u79fb\u5b66\u4e60",level:3},{value:"\u603b\u7ed3",id:"\u603b\u7ed3",level:2}];function d(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components},{DocCardList:s}=e;return s||function(n,e){throw new Error("Expected "+(e?"component":"object")+" `"+n+"` to be defined: you likely forgot to import, pass, or provide it.")}("DocCardList",!0),(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"\u8ba1\u7b97\u673a\u89c6\u89c9computer-vision",children:"\u8ba1\u7b97\u673a\u89c6\u89c9\uff08Computer Vision\uff09"})}),"\n",(0,o.jsx)(e.admonition,{title:"\u7ae0\u8282\u6982\u8ff0",type:"info",children:(0,o.jsx)(e.p,{children:"\u672c\u7ae0\u8282\u4ecb\u7ecd\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6838\u5fc3\u6280\u672f\uff0c\u5305\u62ec\u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u3001\u56fe\u50cf\u5206\u5272\u7b49\u7ecf\u5178\u4efb\u52a1\u3002"})}),"\n",(0,o.jsx)(e.h2,{id:"\u4ec0\u4e48\u662f\u8ba1\u7b97\u673a\u89c6\u89c9",children:"\u4ec0\u4e48\u662f\u8ba1\u7b97\u673a\u89c6\u89c9"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09"}),' \u662f\u8ba9\u8ba1\u7b97\u673a\u80fd\u591f"\u770b\u61c2"\u56fe\u50cf\u548c\u89c6\u9891\u7684\u6280\u672f\uff0c\u662f\u4eba\u5de5\u667a\u80fd\u7684\u91cd\u8981\u5206\u652f\u3002']}),"\n",(0,o.jsx)(e.h3,{id:"\u6838\u5fc3\u4efb\u52a1",children:"\u6838\u5fc3\u4efb\u52a1"}),"\n",(0,o.jsx)(e.mermaid,{value:"graph TD\n    A[\u8ba1\u7b97\u673a\u89c6\u89c9] --\x3e B[\u56fe\u50cf\u5206\u7c7b]\n    A --\x3e C[\u76ee\u6807\u68c0\u6d4b]\n    A --\x3e D[\u56fe\u50cf\u5206\u5272]\n    A --\x3e E[\u4eba\u8138\u8bc6\u522b]\n    A --\x3e F[\u59ff\u6001\u4f30\u8ba1]\n    A --\x3e G[\u56fe\u50cf\u751f\u6210]"}),"\n",(0,o.jsx)(e.h2,{id:"\u5377\u79ef\u795e\u7ecf\u7f51\u7edccnn",children:"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09"}),"\n",(0,o.jsx)(e.h3,{id:"cnn\u57fa\u7840\u67b6\u6784",children:"CNN\u57fa\u7840\u67b6\u6784"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\n\nclass SimpleCNN(nn.Module):\n    """\u7b80\u5355\u7684CNN\u5206\u7c7b\u5668"""\n    def __init__(self, num_classes=10):\n        super().__init__()\n        \n        # \u5377\u79ef\u5c42\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        \n        # \u6c60\u5316\u5c42\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # \u5168\u8fde\u63a5\u5c42\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        \n        # \u6fc0\u6d3b\u548cDropout\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, x):\n        # \u5377\u79ef\u57571: 32x32 -> 16x16\n        x = self.pool(self.relu(self.conv1(x)))\n        \n        # \u5377\u79ef\u57572: 16x16 -> 8x8\n        x = self.pool(self.relu(self.conv2(x)))\n        \n        # \u5377\u79ef\u57573: 8x8 -> 4x4\n        x = self.pool(self.relu(self.conv3(x)))\n        \n        # \u5c55\u5e73\n        x = x.view(-1, 128 * 4 * 4)\n        \n        # \u5168\u8fde\u63a5\u5c42\n        x = self.dropout(self.relu(self.fc1(x)))\n        x = self.fc2(x)\n        \n        return x\n\n# \u4f7f\u7528\u793a\u4f8b\nmodel = SimpleCNN(num_classes=10)\nx = torch.randn(1, 3, 32, 32)  # batch_size=1, channels=3, height=32, width=32\noutput = model(x)\nprint(f"\u8f93\u51fa\u5f62\u72b6: {output.shape}")  # [1, 10]\n'})}),"\n",(0,o.jsx)(e.h3,{id:"\u7ecf\u5178cnn\u67b6\u6784",children:"\u7ecf\u5178CNN\u67b6\u6784"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"ResNet\uff08\u6b8b\u5dee\u7f51\u7edc\uff09"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class ResidualBlock(nn.Module):\n    """\u6b8b\u5dee\u5757"""\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        # \u5feb\u6377\u8fde\u63a5\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, stride),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)  # \u6b8b\u5dee\u8fde\u63a5\n        out = torch.relu(out)\n        return out\n'})}),"\n",(0,o.jsx)(e.h2,{id:"\u56fe\u50cf\u5206\u7c7b",children:"\u56fe\u50cf\u5206\u7c7b"}),"\n",(0,o.jsx)(e.h3,{id:"\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b",children:"\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from torchvision import models, transforms\nfrom PIL import Image\n\nclass ImageClassifier:\n    """\u56fe\u50cf\u5206\u7c7b\u5668"""\n    def __init__(self, model_name=\'resnet50\'):\n        # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\n        self.model = models.resnet50(pretrained=True)\n        self.model.eval()\n        \n        # \u56fe\u50cf\u9884\u5904\u7406\n        self.transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        # \u52a0\u8f7d\u7c7b\u522b\u6807\u7b7e\n        self.labels = self._load_labels()\n    \n    def predict(self, image_path, top_k=5):\n        """\u9884\u6d4b\u56fe\u50cf\u7c7b\u522b"""\n        # \u52a0\u8f7d\u548c\u9884\u5904\u7406\u56fe\u50cf\n        image = Image.open(image_path).convert(\'RGB\')\n        input_tensor = self.transform(image).unsqueeze(0)\n        \n        # \u9884\u6d4b\n        with torch.no_grad():\n            output = self.model(input_tensor)\n            probabilities = torch.softmax(output[0], dim=0)\n        \n        # \u83b7\u53d6top-k\u7ed3\u679c\n        top_probs, top_indices = torch.topk(probabilities, top_k)\n        \n        results = []\n        for prob, idx in zip(top_probs, top_indices):\n            results.append({\n                \'label\': self.labels[idx],\n                \'probability\': prob.item()\n            })\n        \n        return results\n    \n    def _load_labels(self):\n        """\u52a0\u8f7dImageNet\u6807\u7b7e"""\n        # \u5b9e\u9645\u5e94\u8be5\u4ece\u6587\u4ef6\u52a0\u8f7d\n        return [f"class_{i}" for i in range(1000)]\n\n# \u4f7f\u7528\nclassifier = ImageClassifier()\nresults = classifier.predict(\'cat.jpg\')\nfor r in results:\n    print(f"{r[\'label\']}: {r[\'probability\']:.2%}")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"\u76ee\u6807\u68c0\u6d4b",children:"\u76ee\u6807\u68c0\u6d4b"}),"\n",(0,o.jsx)(e.h3,{id:"yolo\u5b9e\u73b0",children:"YOLO\u5b9e\u73b0"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from ultralytics import YOLO\n\nclass ObjectDetector:\n    """\u76ee\u6807\u68c0\u6d4b\u5668"""\n    def __init__(self, model_path=\'yolov8n.pt\'):\n        self.model = YOLO(model_path)\n    \n    def detect(self, image_path, conf_threshold=0.5):\n        """\u68c0\u6d4b\u56fe\u50cf\u4e2d\u7684\u7269\u4f53"""\n        results = self.model(image_path, conf=conf_threshold)\n        \n        detections = []\n        for result in results:\n            boxes = result.boxes\n            for box in boxes:\n                detections.append({\n                    \'class\': result.names[int(box.cls)],\n                    \'confidence\': float(box.conf),\n                    \'bbox\': box.xyxy[0].tolist()  # [x1, y1, x2, y2]\n                })\n        \n        return detections\n    \n    def detect_video(self, video_path, output_path):\n        """\u68c0\u6d4b\u89c6\u9891\u4e2d\u7684\u7269\u4f53"""\n        results = self.model(video_path, stream=True)\n        \n        for result in results:\n            # \u5904\u7406\u6bcf\u4e00\u5e27\n            annotated_frame = result.plot()\n            # \u4fdd\u5b58\u6216\u663e\u793a\u5e27\n            pass\n\n# \u4f7f\u7528\ndetector = ObjectDetector()\ndetections = detector.detect(\'street.jpg\')\nfor det in detections:\n    print(f"{det[\'class\']}: {det[\'confidence\']:.2f} at {det[\'bbox\']}")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"\u56fe\u50cf\u5206\u5272",children:"\u56fe\u50cf\u5206\u5272"}),"\n",(0,o.jsx)(e.h3,{id:"\u8bed\u4e49\u5206\u5272",children:"\u8bed\u4e49\u5206\u5272"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch\nimport torchvision.models.segmentation as segmentation\n\nclass SemanticSegmentation:\n    """\u8bed\u4e49\u5206\u5272"""\n    def __init__(self):\n        self.model = segmentation.deeplabv3_resnet50(pretrained=True)\n        self.model.eval()\n    \n    def segment(self, image):\n        """\u5206\u5272\u56fe\u50cf"""\n        with torch.no_grad():\n            output = self.model(image)[\'out\']\n            # \u83b7\u53d6\u6bcf\u4e2a\u50cf\u7d20\u7684\u7c7b\u522b\n            segmentation_map = output.argmax(1)\n        \n        return segmentation_map\n    \n    def visualize(self, image, segmentation_map):\n        """\u53ef\u89c6\u5316\u5206\u5272\u7ed3\u679c"""\n        import matplotlib.pyplot as plt\n        \n        plt.figure(figsize=(12, 4))\n        plt.subplot(1, 2, 1)\n        plt.imshow(image.permute(1, 2, 0))\n        plt.title(\'Original\')\n        \n        plt.subplot(1, 2, 2)\n        plt.imshow(segmentation_map.squeeze())\n        plt.title(\'Segmentation\')\n        plt.show()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"\u4eba\u8138\u8bc6\u522b",children:"\u4eba\u8138\u8bc6\u522b"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import face_recognition\n\nclass FaceRecognizer:\n    """\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf"""\n    def __init__(self):\n        self.known_faces = {}\n    \n    def add_face(self, name, image_path):\n        """\u6dfb\u52a0\u5df2\u77e5\u4eba\u8138"""\n        image = face_recognition.load_image_file(image_path)\n        encoding = face_recognition.face_encodings(image)[0]\n        self.known_faces[name] = encoding\n    \n    def recognize(self, image_path):\n        """\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684\u4eba\u8138"""\n        # \u52a0\u8f7d\u56fe\u50cf\n        image = face_recognition.load_image_file(image_path)\n        \n        # \u68c0\u6d4b\u4eba\u8138\u4f4d\u7f6e\n        face_locations = face_recognition.face_locations(image)\n        \n        # \u7f16\u7801\u4eba\u8138\n        face_encodings = face_recognition.face_encodings(image, face_locations)\n        \n        results = []\n        for face_encoding, face_location in zip(face_encodings, face_locations):\n            # \u6bd4\u5bf9\u5df2\u77e5\u4eba\u8138\n            matches = face_recognition.compare_faces(\n                list(self.known_faces.values()),\n                face_encoding\n            )\n            \n            name = "Unknown"\n            if True in matches:\n                match_index = matches.index(True)\n                name = list(self.known_faces.keys())[match_index]\n            \n            results.append({\n                \'name\': name,\n                \'location\': face_location\n            })\n        \n        return results\n\n# \u4f7f\u7528\nrecognizer = FaceRecognizer()\nrecognizer.add_face("Alice", "alice.jpg")\nrecognizer.add_face("Bob", "bob.jpg")\n\nresults = recognizer.recognize("group.jpg")\nfor r in results:\n    print(f"Found {r[\'name\']} at {r[\'location\']}")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"\u5b9e\u6218\u9879\u76ee",children:"\u5b9e\u6218\u9879\u76ee"}),"\n",(0,o.jsx)(e.h3,{id:"\u9879\u76ee1\u56fe\u50cf\u5206\u7c7b\u5e94\u7528",children:"\u9879\u76ee1\uff1a\u56fe\u50cf\u5206\u7c7b\u5e94\u7528"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import streamlit as st\nfrom PIL import Image\n\ndef main():\n    st.title(\"\u56fe\u50cf\u5206\u7c7b\u5e94\u7528\")\n    \n    # \u4e0a\u4f20\u56fe\u50cf\n    uploaded_file = st.file_uploader(\"\u9009\u62e9\u56fe\u50cf\", type=['jpg', 'png'])\n    \n    if uploaded_file:\n        # \u663e\u793a\u56fe\u50cf\n        image = Image.open(uploaded_file)\n        st.image(image, caption='\u4e0a\u4f20\u7684\u56fe\u50cf')\n        \n        # \u5206\u7c7b\n        if st.button('\u5206\u7c7b'):\n            classifier = ImageClassifier()\n            results = classifier.predict(uploaded_file)\n            \n            st.write(\"\u9884\u6d4b\u7ed3\u679c\uff1a\")\n            for r in results:\n                st.write(f\"- {r['label']}: {r['probability']:.2%}\")\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"\u9879\u76ee2\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",children:"\u9879\u76ee2\uff1a\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import cv2\n\ndef real_time_detection():\n    \"\"\"\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\"\"\"\n    detector = ObjectDetector()\n    cap = cv2.VideoCapture(0)  # \u6253\u5f00\u6444\u50cf\u5934\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # \u68c0\u6d4b\n        detections = detector.detect(frame)\n        \n        # \u7ed8\u5236\u8fb9\u754c\u6846\n        for det in detections:\n            x1, y1, x2, y2 = map(int, det['bbox'])\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{det['class']} {det['confidence']:.2f}\",\n                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n        \n        # \u663e\u793a\n        cv2.imshow('Detection', frame)\n        \n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    \n    cap.release()\n    cv2.destroyAllWindows()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"\u6700\u4f73\u5b9e\u8df5",children:"\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,o.jsx)(e.h3,{id:"\u6570\u636e\u589e\u5f3a",children:"\u6570\u636e\u589e\u5f3a"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"from torchvision import transforms\n\n# \u8bad\u7ec3\u65f6\u7684\u6570\u636e\u589e\u5f3a\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n\n# \u6d4b\u8bd5\u65f6\u7684\u9884\u5904\u7406\ntest_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n"})}),"\n",(0,o.jsx)(e.h3,{id:"\u8fc1\u79fb\u5b66\u4e60",children:"\u8fc1\u79fb\u5b66\u4e60"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'def create_transfer_model(num_classes):\n    """\u521b\u5efa\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b"""\n    # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\n    model = models.resnet50(pretrained=True)\n    \n    # \u51bb\u7ed3\u524d\u9762\u7684\u5c42\n    for param in model.parameters():\n        param.requires_grad = False\n    \n    # \u66ff\u6362\u6700\u540e\u7684\u5168\u8fde\u63a5\u5c42\n    num_features = model.fc.in_features\n    model.fc = nn.Linear(num_features, num_classes)\n    \n    return model\n'})}),"\n",(0,o.jsx)(e.h2,{id:"\u603b\u7ed3",children:"\u603b\u7ed3"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"\u5173\u952e\u8981\u70b9"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"CNN\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u57fa\u7840"}),"\n",(0,o.jsx)(e.li,{children:"\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u4e0a\u624b"}),"\n",(0,o.jsx)(e.li,{children:"\u6570\u636e\u589e\u5f3a\u63d0\u5347\u6a21\u578b\u6027\u80fd"}),"\n",(0,o.jsx)(e.li,{children:"\u8fc1\u79fb\u5b66\u4e60\u8282\u7701\u8bad\u7ec3\u65f6\u95f4"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"\u5b66\u4e60\u5efa\u8bae"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"\u4ece\u7ecf\u5178CNN\u67b6\u6784\u5f00\u59cb"}),"\n",(0,o.jsx)(e.li,{children:"\u5b9e\u8df5\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1"}),"\n",(0,o.jsx)(e.li,{children:"\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u548c\u5206\u5272"}),"\n",(0,o.jsx)(e.li,{children:"\u5173\u6ce8\u6700\u65b0\u7684\u89c6\u89c9\u6a21\u578b"}),"\n"]}),"\n",(0,o.jsx)(s,{})]})}function f(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},28453(n,e,s){s.d(e,{R:()=>r,x:()=>l});var t=s(96540);const o={},i=t.createContext(o);function r(n){const e=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),t.createElement(i.Provider,{value:e},n.children)}}}]);