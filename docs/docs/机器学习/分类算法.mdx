---
sidebar_position: 0
title: 
---

## KNNK

KNN****




1. 
2. 
3. K
4. K



```python showLineNumbers
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# 
X = np.array([[1, 2], [2, 3], [2, 5], [3, 2], [3, 3], [4, 5]])  # 
y = np.array([0, 0, 1, 0, 1, 1])  # 

# K-
k = 3  # K
model = KNeighborsClassifier(n_neighbors=k).fit(X, y)

# 
new_data_point = np.array([[3, 4]])  # 

# .predicts()
predicted_class = model.predict(new_data_point)

print(":", predicted_class)
```

### 

```python showLineNumbers
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

# 
from sklearn.datasets import load_iris

# 
iris = load_iris()
# 
iris_X = iris.data
# 012
iris_y = iris.target
print(len(iris_X)) # 

# # 2,02,21
# print(iris_X[0:2])

# # 2,02,220
# print(iris_X[:2])

# # 2,02,230
# print(iris_X[:2,:])

# # 2,02,240,
# print(iris_X[:2,0])

# # 
# print(iris_y)
# # 
# print(iris_X)
# # 
# print(list(zip(iris_X,iris_y)))

# test_size0.330%
X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.3)

'''
train_test_split



41234.

X [------70%---(1)-- | -30%(2)-]
y [------70%---(3)-- | -30%(4)-]

:

X(1),  X(2),
     ↑↓              ↑↓
y(3),  y(4)

(1)(3)

(2)

(4)
'''

# 
# print(y_train)
# KNN
knn = KNeighborsClassifier()
# .fit()
knn.fit(X_train, y_train)
# .predicts()
print(knn.predict(X_test))
# 
print(y_test)
```

### 

```python showLineNumbers
right = 0
error = 0
for i in zip(knn.predict(X_test),y_test):
    #print(i)
    if i[0] == i[1]:
        right +=1
    else:
        error +=1
print(right,error)
print('{}%'.format(right/(right+error)*100))
```

### 

```python showLineNumbers
print('{}%'.format(knn.score(X_test,y_test)*100))

# 100.0%
```

### 

#### 

KNN  K  K 



#### 

```python showLineNumbers
'''
`.py`

pip install opencv-python
pip install tensorflow


1. 

2. AA

3. BC

4. 
'''
import cv2
import tensorflow as tf
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.mobilenet import preprocess_input
from tensorflow.keras.models import Model
import numpy as np


# KNN
class KNNClassifier:
    def __init__(self):
        # ABC
        self.examples = {"A": [], "B": [], "C": []}

    def add_example(self, activation, class_id):
        # 
        self.examples[class_id].append(activation)

    def predict_class(self, activation):
        # 
        distances = {}
        # 
        for class_id, examples in self.examples.items():
            # 
            distances[class_id] = np.mean(
                # np.linalg.norm  """"2
                [np.linalg.norm(act - activation) for act in examples]
            )

        # 
        predicted_class = min(distances, key=distances.get)
        # 
        confidence = 1 / (1 + distances[predicted_class])
        # 
        return predicted_class, confidence


def main():
    # KNN
    classifier = KNNClassifier()
    # 0
    webcam = cv2.VideoCapture(0)

    print("Loading MobileNet...")
    # MobileNetImageNet
    base_model = tf.keras.applications.MobileNet(weights="imagenet")
    # conv_preds
    model = Model(
        inputs=base_model.input, outputs=base_model.get_layer("conv_preds").output
    )

    print("Successfully loaded model")

    # 
    classes = ["A", "B", "C"]

    # 
    while True:
        # 
        ret, frame = webcam.read()
        # 224x224MobileNet
        frame = cv2.resize(frame, (224, 224))
        # OpenCVKeras
        img = image.img_to_array(frame)
        # (224,224,3)(1,224,224,3)
        img = np.expand_dims(img, axis=0)
        # MobileNet
        img = preprocess_input(img)

        # 
        activation = model.predict(img)

        # 1
        key = cv2.waitKey(1)
        # 'a'A
        if key == ord("a"):
            classifier.add_example(activation, "A")
        # 'b'B
        elif key == ord("b"):
            classifier.add_example(activation, "B")
        # 'c'C
        elif key == ord("c"):
            classifier.add_example(activation, "C")

        # A
        if len(classifier.examples["A"]) > 0:
            # KNN
            predicted_class, confidence = classifier.predict_class(activation)
            # 
            print(f"Prediction: {predicted_class}, Confidence: {confidence}")

        # 
        cv2.imshow("Webcam", frame)

        # ESCASCII27
        if key == 27:  # ESC key to break from the loop
            break

    # 
    webcam.release()
    # OpenCV
    cv2.destroyAllWindows()


# 
main()

```



## 





“”

 7cm 

:

 100  80 20 

```markdown
P = 80/100 = 0.8
P = 20/100 = 0.2
```

“”“”“”

 20 “”50 “”0 “”

```markdown
P| = 20/80 = 0.25
P| = 50/80 = 0.625
P| = 0/80 = 0
```

 5 “”6 “”2 “”

```markdown
P| = 5/20 = 0.25
P| = 6/20 = 0.3
P| = 2/20 = 0.1
```

“********”

```markdown
P| = P|* P|* P= 0.25 * 0.625 * 0.8 = 0.125

P| = P|* P|* P= 0.25 * 0.3 * 0.2 = 0.015
```

 P| > P|

“****************” 0“”

:

 0  2 

```markdown
P| = P|* P|* P|* P= (20+1/80)² * (50+1/80) * (0+1/80) * 0.8 = 0.0351421875

P| = P|* P|* P|* P= (5+1/20)² * (6+1/20) * (2+1/20) * 0.2 =0.012885
```

```python showLineNumbers

# 
import numpy as np

class NaiveBayes:
    def __init__(self):
        self.class_probs = {}  #  P(c)
        self.word_probs = {}   #  P(w|c)
        self.vocab = set()     # 
        self.smooth = 1        # 

    def fit(self, X, y):
        # 
        classes, class_counts = np.unique(y, return_counts=True)
        self.class_probs = {label: count / len(y) for label, count in zip(classes, class_counts)}  # 
        
        # 
        word_count = {label: {} for label in classes}  # 
        class_word_totals = {label: 0 for label in classes}  # 
        
        # 
        for text, label in zip(X, y):
            words = text.split(" ")
            for word in words:
                self.vocab.add(word)  # 
                if word not in word_count[label]:
                    word_count[label][word] = 0
                word_count[label][word] += 1  # 
                class_word_totals[label] += 1  # 1

        #  P(w|c) 
        vocab_size = len(self.vocab)  # 
        self.word_probs = {label: {} for label in classes}
        for label in classes:
            for word in self.vocab:
                count = word_count[label].get(word, 0)  # 0
                self.word_probs[label][word] = (count + self.smooth) / (
                    class_word_totals[label] + vocab_size * self.smooth
                )

    def predict(self, X):
        predictions = []  # 
        for text in X:
            words = text.split(" ")
            class_scores = {}  # 

            #  P(c|w1,w2,...,wn)
            for label in self.class_probs:
                class_scores[label] = self.class_probs[label]
                for word in words:
                    if word in self.word_probs[label]:  # 
                        class_scores[label] *= self.word_probs[label][word]
                    else:
                        # 
                        class_scores[label] *= 1/len(self.vocab)
            
            # 
            predictions.append(max(class_scores, key=class_scores.get))
        
        return predictions
    
    def score(self, X, y):
        predictions = self.predict(X)
        return np.mean(predictions == y)


# 
data = np.array([
    ("     ",""),
    ("      ",""),
    ("      ",""),
    ("     ",""),
    ("   ",""),
    ("     ",""),
    ("     ",""),
    ("    ",""),
    ("       ",""),
    ("     ",""),
    ("     ",""),
    ("     ",""),
    ("      ",""),
    ("     ",""),
    ("    ",""),
])
X = data[:, 0]  # 
y = data[:, 1]  # 

# 
model = NaiveBayes()
model.fit(X, y)

# 
print(model.score(X, y))


```

### sklearn

```python showLineNumbers
from sklearn.naive_bayes import GaussianNB
import numpy as np

# 
X = np.array([[1], [2], [3], [4], [5]])  # 
y = np.array([0, 0, 1, 1, 1])  # 

#  ()
model = GaussianNB()

# .fit() 
model.fit(X, y)

# 
new_data_point = np.array([[6]])

# .predict() 
predicted_class = model.predict(new_data_point)
# .predict_proba() 
predicted_proba = model.predict_proba(new_data_point)

print(":", predicted_class)
print(":", predicted_proba)

```


### 

```python showLineNumbers
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
import sklearn.datasets
# 
data = sklearn.datasets.load_iris()
# .data 
X = data.data
# .target 
y = data.target
# 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
#  ()
model = GaussianNB()
# 
model.fit(X_train, y_train)

```

### 

```python showLineNumbers
from sklearn.metrics import accuracy_score

# 
accuracy = accuracy_score(y_test, model.predict(X_test))
accuracy
```

### 

```python showLineNumbers
import pandas as pd

# 
# pd.DataFrame()DataFrame
# pd.concat()DataFrame
# axis=1 
df = pd.concat(
    [pd.DataFrame(X_test,columns=data.feature_names),
     pd.DataFrame(y_test,columns=['target']),
     pd.DataFrame(model.predict(X_test),columns=['predict'])
     ],axis=1 )

# targetpredict
df.loc[df['target']!=df['predict']]

```


## SVM

SVM****


- 
- 2=2D3=3D
- 2D

SVM****

 scikit-learn  svm  NumPy  X  y X y 

 SVM kernel='linear'

 fit  predict 

```python showLineNumbers
from sklearn import svm
import numpy as np

# 
X = np.array([[1, 2], [2, 3], [2, 5], [3, 2], [3, 3], [4, 5]])  # 
y = np.array([0, 0, 1, 0, 1, 1])  # 

# SVM
model = svm.SVC(kernel="linear")

# 
model.fit(X, y)

# 
new_data_point = np.array([[3, 4]])
# 
predicted_class = model.predict(new_data_point)

print(":", predicted_class)
```

### 

```python showLineNumbers
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn import svm

# 
iris = load_breast_cancer()
# 
X, y = iris.data ,iris.target

# 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
# SVM
clf = svm.SVC()
# 
clf.fit(X_train, y_train)
```

### 

```python showLineNumbers
# .score()
# accuracy_scorecross_val_score
print('{}%'.format(clf.score(X_test, y_test)*100))
```

###  normalization

```python showLineNumbers
from sklearn import preprocessing
# normalization
# 
X2 = preprocessing.scale(X)    # normalization step
# print(X2)
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.3)

clf2 = svm.SVC()
clf2.fit(X2_train, y2_train)
print('{}%'.format(clf2.score(X2_test, y2_test)*100))
# 98.83040935672514%

from sklearn import model_selection

# 
# 5
print(model_selection.cross_validate(clf,X_test, y_test,cv=5)['test_score'].mean())
print(model_selection.cross_validate(clf2,X2_test, y2_test,cv=5)['test_score'].mean())
'''
0.8947899159663866 # 
0.9825210084033614 # 
'''
```

### 

```python showLineNumbers
from __future__ import print_function
from sklearn.model_selection import  learning_curve
from sklearn.datasets import load_digits
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np
# 
digits = load_digits()
X = digits.data
y = digits.target

# learning_curve()
# 
# SVCgamma=0.01
# gammagamma
# train_sizes
# cv
# scoring
train_sizes, train_loss, test_loss= learning_curve(
        SVC(gamma=0.01), X, y, cv=10, scoring="neg_mean_squared_error",
        train_sizes=[0.1, 0.25, 0.5, 0.75, 1])

# 
train_loss_mean = -np.mean(train_loss, axis=1)
test_loss_mean = -np.mean(test_loss, axis=1)

# 
# 
# ro-go-
plt.plot(train_sizes, train_loss_mean, 'ro-',
             label="Training")
plt.plot(train_sizes, test_loss_mean, 'go-',
             label="test-Cross-validation")
plt.xlabel("Training examples")
plt.ylabel("Loss")
# ,loc="best"
plt.legend(loc="best")
plt.show()
# 
# 200
# 
# 
```
