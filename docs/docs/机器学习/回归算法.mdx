---
sidebar_position: 3
title: 回归算法
---

## 函数与导数

初等数学研究的是静态的量，高等数学研究的是变化的量。对于编程来说，理解变化率（导数）很重要，因为机器学习的核心就是"调参数让误差变小"，而导数告诉我们该往哪个方向调。

> 这部分内容参考了《高等数学（第七版）》，做了精简和改写。想深入学习的话建议找专门的数学教材。


本章提到的符号含义如下：

$$\{  \}$$ 表集合、定义域或值域

- $ A = \{1, 2, 3\} $ 表示集合 $A$ 包含元素 $1, 2, 3$
- $f(x) = \{y | y = x^2, x \in R\}$ 表示函数 $f(x)$ 的值域是所有 $x^2$ 的值，其中 $x$ 是实数。

$$\to $$ 表逻辑关系，

- 如 $a \to b$ 表示 $a$ 推导出 $b$.

- $f:X\to Y$ 表示 $f$ 是从 $X$ 到 $Y$ 的一个映射。

- 不同场景还可表示：变换、趋向、收敛、极限等。

$$ | $$ 表“使得”或“满足”，$$R_y = \{y | y \geq 0\}$$,表示一个实数集合 $$R_y$$，其中包含所有满足 $$y \geq 0$$ 条件的实数 $$y$$。

$$\in $$ 表属于，如果 $a$ 是集合 $A$ 的元素，记作 $a \in A$.

$$\subseteq$$ 表子集，如果 A 是集合 B 的子集(A 与 B 可以相等), 记作 $A \subseteq B$.

$$\subset$$ 表真子集， 如果 A 是集合 B 的真子集(A 与 B 不相等), 记作 $A \subset B$.

### 函数

#### 映射的概念

定义：设 $X$ 和 $Y$ 是两个非空集合，

如果存在一个对应关系 $f$，使得对于 $X$ 中的任意一个元素 $x$，在 $Y$ 中都有唯一确定的元素 $y$ 和它对应，

那么就称 $f$ 为从 $X$ 到 $Y$ 的一个映射，记作 $f:X\to Y$，

其中，$y$ 称为 $x$ 在映射 $f$ 下的像，记作 $y=f(x)$。而 $X$ 中的元素 $x$ 称为 $y$ 的原像。

并称 $X$ 为$f$的定义域，记作 $D_f$；

$Y$ 为$f$的值域，记作 $R_f$。

$R_f = f(X)=\{f(x) | x\in D_f\}$

构成一个映射的条件是：

1. 集合 $X$ ，即定义域 $D_f = X$
2. 集合 $Y$ ，即值域 $R_f \subseteq Y$
3. 对应法则$f$，对于 $X$ 中的任意一个元素 $x$，在 $Y$ 中都有唯一确定的元素 $y$ 和它对应

**注意**：

对每个 $x \in X$，$f(x)$ 必须是确定唯一与之对应的

对于 $y \in Y$，$y$ 可以有多个原像。

> 例如，$f(x)=x^2$，$y=1$ 的原像可以是 $x=1$ 或 $x=-1$。

值域 $R_f$ 是 $Y$ 的子集,即 $R_f \subseteq Y$，而不一定是 $R_f = Y$

### 导数的直观理解

想象你开着小电动车，车上有根透明管子，里面有个小球。管子中间是0，两头是正负无穷。车怎么动，球就怎么滚，球的位置就是导数值。

- **平路**：车在平地上匀速开，球一直在0的位置不动（常数的导数是0）
- **上坡/下坡**：上坡时球往正方向滚，下坡时往负方向滚（斜率就是导数）
- **悬崖**：车直接掉下去，函数不连续，导数不存在
- **起伏路**：车在波浪路上开，球来回晃（比如sin函数的导数是cos）

#### 幂函数求导

幂函数求导的通用法则：

$$\frac{d}{dx} x^n = nx^{n-1}$$

1.  **“指数向前”：** 将指数 $n$ 移到变量 $x$ 的前面作为系数。
2.  **“并减一”：** 将原指数 $n$ 减去1，得到新的指数 $n-1$。

* **基本形式：**
    求 $x^2$ 的导数。
    将指数2移到前面，并将指数2减去1（2-1=1），所以导数是 $2x^1 = 2x$。  
    $$\frac{d}{dx} x^2 = 2x^{2-1} = 2x$$

* **常数乘以幂函数：**
    求 $5x^3$ 的导数。
    常数5保持不变，对 $x^3$ 使用法则。将指数3移到前面，与常数5相乘，并将指数3减去1。  
    $$\frac{d}{dx} 5x^3 = 5 \cdot 3x^{3-1} = 15x^2$$

* **分式形式：**
    求 $\frac{1}{x^4}$ 的导数。  
    首先，将分式转化为指数形式：$\frac{1}{x^4} = x^{-4}$。  
    然后使用法则，将指数-4移到前面，并将指数-4减去1（$-4 - 1 = -5$）。  
    $$\frac{d}{dx} \frac{1}{x^4} = \frac{d}{dx} x^{-4} = -4x^{-4-1} = -4x^{-5} = -\frac{4}{x^5}$$


## 线性回归

线性回归就是找一条直线，让它尽可能贴近数据点。用来预测连续值，比如房价、销量这些。

单变量的情况：$Y = aX + b$（一条直线）

多变量的情况：$Y = β_0X_0 + β_1X_1 + β_2X_2 + ... + β_nX_n + ε$（一个平面或超平面）

其中 $ε$ 是误差项，因为现实世界不可能完美拟合。

### 损失函数

怎么衡量模型预测得好不好？看损失函数。

先看几种常见的误差计算方式：

| 数据1 | 数据2 | 平均值 | 平均差 | 均方误差 |
|--------|--------|--------|--------|----------|
| 0      | 0      | 0      | 0      | 0        |
| -4     | 4      | 0      | 4      | 16       |
| 7      | 1      | 4      | 4      | 25       |

直觉上，(0,0)最好，(-4,4)其次，(7,1)最差。只有均方误差（MSE）能正确反映这个顺序。

为什么要平方？因为：
1. 避免正负抵消（-4和4的平均差都是4，但离散程度明显不同）
2. 惩罚大误差（差7比差4严重得多，平方后差距更明显）

常见损失函数对比：

| 名称         | 数学表达式                                                             | 值域         | 导数表达式                                                   | 优点                                       | 缺点                                     |
|--------------|------------------------------------------------------------------------|--------------|--------------------------------------------------------------|--------------------------------------------|-----------------------------------------|
| 交叉熵损失（Cross Entropy） | $L = -\sum_{i} y_i \log(\hat{y}_i)$                                | $[0,+\infty)$ | $\frac{\partial L}{\partial \hat{y}_i} = -\frac{y_i}{\hat{y}_i}$ | 最常用，适合 one-hot 标签，梯度清晰，收敛快 | 对异常值敏感，需要防止概率为0的情况       |
| 均方误差（MSE）      | $L = \frac{1}{n} \sum_{i} (y_i - \hat{y}_i)^2$                         | $[0,+\infty)$ | $\frac{\partial L}{\partial \hat{y}_i} = -\frac{2}{n}(y_i - \hat{y}_i)$ | 简单直观，易于理解                         | 不适合分类任务，梯度在误差较大时过大     |
| 平均绝对误差（MAE）  | $L = \frac{1}{n} \sum_{i} \vert y_i - \hat{y}_i \vert$                  | $[0,+\infty)$ | $\frac{\partial L}{\partial \hat{y}_i} = -\frac{1}{n}sgn(y_i - \hat{y}_i)$ | 对异常值不敏感，稳定性好                   | 在零点不可导，优化困难                   |

### 求导与梯度下降

有了损失函数，怎么让它变小？答案是求导（也叫求梯度、反向传播）。

对于线性回归 $y = wx + b$，损失函数是：

$$loss = (y - (wx + b))^2$$

其中：
- $y$ 是真实值（数据集给的）
- $x$ 是输入特征（数据集给的）
- $w$ 和 $b$ 是我们要调的参数

要让loss变小，需要知道往哪个方向调参数。这就要分别对 $w$ 和 $b$ 求偏导：

$$\frac{\partial loss}{\partial w} \quad \text{和} \quad \frac{\partial loss}{\partial b}$$

求出来的导数告诉我们：参数往哪边调，loss会变小。

### 手写线性回归

```python showLineNumbers
import numpy as np
from matplotlib import pyplot as plt


class Line:
    def __init__(self, data):
        self.w = 1
        self.b = 0
        self.learning_rate = 0.01
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1)
        self.loss_list = []

    def get_data(self, data):
        self.X = np.array(data)[:, 0]
        self.y = np.array(data)[:, 1]

    def predict(self, x):
        return self.w * x + self.b
    
    def train(self, epoch_times):
        for epoch in range(epoch_times):
            total_loss = 0
            for x, y in zip(self.X, self.y):
                y_pred = self.predict(x)
                # Calculate gradients
                gradient_w = -2 * x * (y - y_pred)
                gradient_b = -2 * (y - y_pred)
                # Update weights
                self.w -= self.learning_rate * gradient_w
                self.b -= self.learning_rate * gradient_b
                # Calculate loss
                loss = (y - y_pred) ** 2
                total_loss += loss
            epoch_loss = total_loss / len(self.X)
            self.loss_list.append(epoch_loss)
            if epoch % 10 == 0:
                print(f"loss: {epoch_loss}")
                self.plot()
        plt.ioff()
        plt.show()

    def plot(self):
        plt.ion()  # Enable interactive mode
        self.ax2.clear()
        self.ax1.clear()
        x = np.linspace(0, 10, 100)
        self.ax1.scatter(self.X, self.y, c="g")
        self.ax1.plot(x, self.predict(x), c="b")
        self.ax2.plot(list(range(len(self.loss_list))), self.loss_list)
        plt.show()
        plt.pause(0.1)

if __name__ == "__main__":  
    # Input data
    data = [(1, 1), (1.8, 2), (2.5, 3), (4.2, 4), (5, 5), (6, 6), (7, 7)]
    s = Line(data)
    s.get_data(data)
    s.train(100)
```

### 房价预测

```python showLineNumbers
from sklearn import datasets
from sklearn.linear_model import LinearRegression

# .fetch_california_housing() 加载加利福尼亚州住房数据集
loaded_data = datasets.fetch_california_housing()
# .data 数据集中的特征数据
data_X = loaded_data.data
# .target 数据集中的标签数据
data_y = loaded_data.target
# 创建线性回归模型
model = LinearRegression()
# 拟合模型
# .fit() 方法接受两个参数：特征数据和标签数据
model.fit(data_X, data_y)
# 打印回归系数和截距
print("回归系数 (斜率):", model.coef_)
print("截距:", model.intercept_)


# 预测前四所房屋价格
# .predict() 方法接受一个参数：特征数据
print(model.predict(data_X[:4, :]))
# 真实价格
print(data_y[:4])

# 效果评估
print(model.get_params())# 获取模型参数
# //{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}
print(model.score(data_X, data_y))
# // 0.606232685199805
# 这意味着数据集中因变量的 60% 的变异性已得到考虑，而其余 40% 的变异性仍未得到解释。
```


## 逻辑回归

有时候，数据并不是一种线性状态，例如：蝌蚪前期体型很小，变态之后体型忽然增大。

这过程更接近一个S

```python showLineNumbers
# 绘制逻辑回归的不同回归系数的sigmoid函数

import matplotlib.pyplot as plt
import numpy as np

def sigmoid(x,p=1):
    # 直接返回sigmoid函数
    return 1. / (1. + np.exp(-p*x))

def plot_sigmoid(p=1):
    # param:起点，终点，间距
    x = np.arange(-8, 8, 0.2)
    y = sigmoid(x,p)
    plt.plot(x, y)
    plt.show()

if __name__ == '__main__':
    plot_sigmoid()
    plot_sigmoid(20)
    plot_sigmoid(0.5)
```

逻辑回归是一种统计模型，它使用数学中的逻辑函数或 logit 函数作为 x 和 y 之间的方程式。Logit 函数将 y 映射为 x 的 sigmoid 函数。

$f(x) = \frac{1}{1 + e^{-x}}$

多个解释变量会影响因变量的值。要对此类输入数据集建模，逻辑回归公式假设不同自变量之间存在线性关系。您可以修改 sigmoid 函数并按如下公式计算最终输出变量

$y = f(β0 + β1x1 + β2x2+… βnxn)$

符号 β 表示回归系数。当您给它一个其中包含因变量和自变量的已知值的足够大的实验数据集时，logit 模型可以反向计算这些系数值。

除了 sigmoid 还有其他常见的激活函数，以下是 Sigmoid、ReLU、Softmax、Tanh 的多维对比表，包含定义、值域、优缺点、导数等内容：

| 名称     | 数学表达式                     | 值域         | 导数表达式                                       | 优点                                       | 缺点                                           |
|----------|--------------------------------|--------------|--------------------------------------------------|--------------------------------------------|------------------------------------------------|
| Sigmoid  | $\sigma(x) = \frac{1}{1 + e^{-x}}$ | $(0, 1)$     | $\sigma'(x) = \sigma(x)(1 - \sigma(x))$         | 平滑，有概率解释                             | 梯度消失、输出非0均值                            |
| ReLU     | $\text{ReLU}(x) = \max(0, x)$     | $[0, +\infty)$ | $\text{ReLU}'(x) = \begin{cases} 1, & x > 0 \\ 0, & x \le 0 \end{cases}$ | 计算简单，收敛快                             | 不可导于0，死神经元问题                           |
| Tanh     | $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ | $(-1, 1)$    | $\tanh'(x) = 1 - \tanh^2(x)$                     | 平滑，输出均值为0                            | 梯度消失                                         |
| Softmax  | $\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$ | $(0,1)$ 且 $\sum=1$ | $\frac{\partial y_i}{\partial x_j} = y_i(\delta_{ij} - y_j)$ | 多分类概率输出，归一化                     | 对大值敏感，数值不稳定                           |

对于不同的交叉熵、均方误差和这些激活函数的导数不同，你可以使用复合求导简化这个过程。

### 手写逻辑回归

```python showLineNumbers
import numpy as np
from matplotlib import pyplot as plt

class Sline:
    def __init__(self, data):
        self.w = 0
        self.b = 0
        self.learning_rate = 0.1
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1)
        self.loss_list = []


    def get_data(self, data):
        self.X = np.array(data)[:, 0]
        self.y = np.array(data)[:, 1]

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-(self.w * x + self.b)))

    def train(self, epoch_times):
        for epoch in range(epoch_times):
            total_loss = 0
            for x, y in zip(self.X, self.y):
                y_pred = self.sigmoid(x)
                # w新 = w旧 - 学习率 * 梯度
                grad = -2 * (y - y_pred) * (1 - y_pred) * y_pred * x
                self.w = self.w - self.learning_rate * grad * x
                # b新 = b旧 - 学习率 * 梯度
                self.b = self.b - self.learning_rate * grad
                loss = (y - y_pred) ** 2
                total_loss += loss
            epoch_loss = total_loss / len(self.X)
            self.loss_list.append(epoch_loss)
            if epoch % 10 == 0:
                print(f"loss: {epoch_loss}")
                self.plot()
        plt.ioff()
        plt.show()

    def plot(self):
        plt.ion()  # 启用交互模式
        self.ax2.clear()
        self.ax1.clear()
        x = np.linspace(0, 10, 100)
        self.ax1.scatter(self.X, self.y, c="g")
        self.ax1.plot(x, self.sigmoid(x), c="b")
        self.ax2.plot(list(range(len(self.loss_list))), self.loss_list)
        plt.show()
        plt.pause(0.1)

if __name__ == "__main__":  
    # 散点输入
    data = [(1, 0), (1.8, 0), (2.5, 0), (4.2, 1), (5, 1), (6, 1), (7, 1)]
    s = Sline(data)
    s.get_data(data)
    s.train(1000)

```


### 使用sklearn框架

```python showLineNumbers
import numpy as np
from sklearn.linear_model import LogisticRegression

# 创建一些示例数据
X = np.array([[1], [2], [3], [4], [5]])  # 自变量
y = np.array([0, 0, 1, 1, 2])  # 因变量，0表示负类，1表示正类

# 创建逻辑回归模型
model = LogisticRegression()

# .fit()方法用于拟合模型，即训练模型x
model.fit(X, y)

# 预测新数据点
new_data_point = np.array([[6]])  # 要预测的新数据点
# .predict()方法预测新数据点的类别
predicted_class = model.predict(new_data_point)
# .predict_proba()方法预测新数据点的概率
predicted_probability = model.predict_proba(new_data_point)

print("预测类别:", predicted_class)
print("预测概率 (负类, 正类):", predicted_probability)
print(type(predicted_probability))
predicted_probability
```
### 手写数字分类实战

```python showLineNumbers
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# 加载数据集
digits = datasets.load_digits()
# 获取特征和目标变量
X = digits.data
y = digits.target

# 数据预处理：随机分割训练集和测试集 , 如果不指定 random_state，每次运行结果都不一样。42为约定俗成的随机数种子
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 数据标准化
scaler = StandardScaler()
# .fit_transform()方法先拟合数据，再标准化。和降维算法的语法一致
X_train = scaler.fit_transform(X_train)
# .transform()方法直接使用在测试集上进行标准化操作
X_test = scaler.transform(X_test)

# 创建Logistic Regression模型  , 如果不指定 random_state，每次运行结果都不一样。42为约定俗成的随机数种子
model = LogisticRegression(random_state=42)

# .fit()方法用于拟合模型，即训练模型
model.fit(X_train, y_train)

# .predict()方法预测新数据点的类别
y_pred = model.predict(X_test)

# 效果评估
from sklearn.metrics import accuracy_score

# accuracy_score()方法计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

# 可视化数据-数据转图片
from matplotlib import pyplot as plt
# 选出预测错误的样本
index = []
# 遍历所有样本
for i in range(len(y_pred)):
    # 判断是否相等
    if y_pred[i] != y_test[i]:
        # 如果不相等,添加到index中:预测值,真实值,图片(注意要变换形状为8*8)
        index.append((
                    y_pred[i],
                    y_test[i],
                    X_test[i].reshape((8, 8))))

# 创建一个正方形画布
# nrows:子图的行数
# ncols:子图的列数
# print(len(index)) // 10
# 因为一共有10张图片,所以行数为2,列数为5，即2*5=10
fig, ax = plt.subplots(
    nrows=3,
    ncols=5,
)
# 实例化子画布
ax = ax.flatten()
for i in range(len(index)):
    p = index[i][0] # 取出预测值
    a = index[i][1] # 取出真实值
    img = index[i][2] # 取出图片
    # 在子画布上画出图片，格式为灰度图
    ax[i].imshow(img, cmap='Greys')
    ax[i].set_title(f'{p}-{a}')
plt.show()


```