---
sidebar_position: 2
title: Spark - 
---

# Spark - Hadoop100

Spark**Spark**

## Spark

### Spark vs MapReduce

```python
# MapReduce
# 
Map →  → Shuffle →  → Reduce → 

# Spark
# 
Map → Shuffle → Reduce  ()
```

****
- Spark100
- Spark10
- Spark

### Spark

1. ****
2. ****API
3. ****
4. ****HDFSHiveHBase

## Spark

### Spark

**1Docker**

```bash
# docker-compose.yml
version: '3'
services:
  spark-master:
    image: bitnami/spark:3.4
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"
      - "7077:7077"
  
  spark-worker:
    image: bitnami/spark:3.4
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master

# 
docker-compose up -d

# Web UI: http://localhost:8080
```

**2**

```bash
# Spark
wget https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz
tar -xzf spark-3.4.0-bin-hadoop3.tgz
cd spark-3.4.0-bin-hadoop3

# Spark Shell
./bin/pyspark
```

### Spark

```python
from pyspark.sql import SparkSession

# Spark
spark = SparkSession.builder \
    .appName("HelloSpark") \
    .master("local[*]") \
    .getOrCreate()

# 
data = [1, 2, 3, 4, 5]
rdd = spark.sparkContext.parallelize(data)

# 
result = rdd.map(lambda x: x * 2).collect()
print(result)  # [2, 4, 6, 8, 10]

# 
spark.stop()
```

****
```bash
spark-submit hello_spark.py
```

## RDD - Spark

### RDD

**RDDResilient Distributed Dataset**

```python
# RDD
: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

# 3
Partition 1: [1, 2, 3, 4]
Partition 2: [5, 6, 7]
Partition 3: [8, 9, 10]

# 
```

### RDD

```python
from pyspark import SparkContext

sc = SparkContext("local", "RDD Demo")

# 1
data = [1, 2, 3, 4, 5]
rdd1 = sc.parallelize(data)

# 2
rdd2 = sc.textFile("data.txt")

# 3HDFS
rdd3 = sc.textFile("hdfs://namenode:9000/user/data/file.txt")
```

### RDDTransformation

****

```python
# 1. map
rdd = sc.parallelize([1, 2, 3, 4, 5])
rdd2 = rdd.map(lambda x: x * 2)
# [2, 4, 6, 8, 10]

# 2. filter
rdd3 = rdd.filter(lambda x: x % 2 == 0)
# [2, 4]

# 3. flatMap
rdd4 = sc.parallelize(["hello world", "spark is fast"])
words = rdd4.flatMap(lambda line: line.split())
# ["hello", "world", "spark", "is", "fast"]

# 4. distinct
rdd5 = sc.parallelize([1, 2, 2, 3, 3, 3])
rdd6 = rdd5.distinct()
# [1, 2, 3]

# 5. union
rdd7 = sc.parallelize([1, 2, 3])
rdd8 = sc.parallelize([4, 5, 6])
rdd9 = rdd7.union(rdd8)
# [1, 2, 3, 4, 5, 6]

# 6. intersection
rdd10 = sc.parallelize([1, 2, 3, 4])
rdd11 = sc.parallelize([3, 4, 5, 6])
rdd12 = rdd10.intersection(rdd11)
# [3, 4]

# 7. groupByKeykey
rdd13 = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])
rdd14 = rdd13.groupByKey()
# [("a", [1, 3]), ("b", [2])]

# 8. reduceByKeykey
rdd15 = rdd13.reduceByKey(lambda x, y: x + y)
# [("a", 4), ("b", 2)]

# 9. sortByKeykey
rdd16 = sc.parallelize([(3, "c"), (1, "a"), (2, "b")])
rdd17 = rdd16.sortByKey()
# [(1, "a"), (2, "b"), (3, "c")]

# 10. join
rdd18 = sc.parallelize([("a", 1), ("b", 2)])
rdd19 = sc.parallelize([("a", "x"), ("b", "y")])
rdd20 = rdd18.join(rdd19)
# [("a", (1, "x")), ("b", (2, "y"))]
```

### RDDAction

****

```python
rdd = sc.parallelize([1, 2, 3, 4, 5])

# 1. collect
result = rdd.collect()
print(result)  # [1, 2, 3, 4, 5]

# 2. count
count = rdd.count()
print(count)  # 5

# 3. first
first = rdd.first()
print(first)  # 1

# 4. taken
top3 = rdd.take(3)
print(top3)  # [1, 2, 3]

# 5. reduce
sum_val = rdd.reduce(lambda x, y: x + y)
print(sum_val)  # 15

# 6. foreach
rdd.foreach(lambda x: print(x))

# 7. saveAsTextFile
rdd.saveAsTextFile("output/")

# 8. countByKeykey
rdd_kv = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])
counts = rdd_kv.countByKey()
print(counts)  # {'a': 2, 'b': 1}
```

### WordCount

```python
from pyspark import SparkContext

sc = SparkContext("local", "WordCount")

# 
text = sc.textFile("data.txt")

# 
counts = text.flatMap(lambda line: line.split()) \
    .map(lambda word: (word.lower(), 1)) \
    .reduceByKey(lambda x, y: x + y) \
    .sortBy(lambda x: x[1], ascending=False)

# TOP10
top10 = counts.take(10)
for word, count in top10:
    print(f"{word}: {count}")

# 
counts.saveAsTextFile("output/wordcount")

sc.stop()
```

## DataFrame - 

### DataFrame

```python
# RDD
rdd.map(lambda x: x[0]) \
   .filter(lambda x: x > 18) \
   .groupBy(lambda x: x) \
   .count()

# DataFrame
df.select("age") \
  .filter(df.age > 18) \
  .groupBy("age") \
  .count()
```

### DataFrame

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DataFrame Demo").getOrCreate()

# 1
data = [
    ("Alice", 25, "F"),
    ("Bob", 30, "M"),
    ("Charlie", 35, "M")
]
df = spark.createDataFrame(data, ["name", "age", "gender"])

# 2
data = [
    {"name": "Alice", "age": 25, "gender": "F"},
    {"name": "Bob", "age": 30, "gender": "M"}
]
df = spark.createDataFrame(data)

# 3CSV
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# 4JSON
df = spark.read.json("data.json")

# 5Parquet
df = spark.read.parquet("data.parquet")

# 6
df = spark.read.jdbc(
    url="jdbc:mysql://localhost:3306/db",
    table="users",
    properties={"user": "root", "password": "password"}
)
```

### DataFrame

```python
# 
df.show()  # 20
df.show(5)  # 5
df.head(3)  # 3

# 
df.printSchema()  # schema
df.columns  # 
df.dtypes  # 

# 
df.count()  # 
df.describe().show()  # 
df.summary().show()  # 

# 
df.select("name", "age").show()
df.select(df.name, df.age).show()
df.select(df["name"], df["age"]).show()

# 
df.filter(df.age > 25).show()
df.filter("age > 25").show()
df.where(df.age > 25).show()

# 
df.orderBy("age").show()
df.orderBy(df.age.desc()).show()
df.sort("age", ascending=False).show()

# 
df.distinct().show()
df.dropDuplicates(["name"]).show()

# 
df.withColumnRenamed("name", "username").show()

# 
from pyspark.sql.functions import col, lit
df.withColumn("age_plus_10", col("age") + 10).show()
df.withColumn("country", lit("USA")).show()

# 
df.drop("gender").show()

# 
df.na.drop().show()  # 
df.na.fill(0).show()  # 
df.na.fill({"age": 0, "name": "Unknown"}).show()
```

### DataFrame

```python
from pyspark.sql.functions import *

# 
df.groupBy("gender").count().show()
df.groupBy("gender").agg(avg("age"), max("age"), min("age")).show()

# 
df.groupBy("gender", "city").count().show()

# 
df.agg(
    count("*").alias("total"),
    avg("age").alias("avg_age"),
    max("age").alias("max_age"),
    min("age").alias("min_age"),
    sum("salary").alias("total_salary")
).show()

# 
from pyspark.sql.window import Window

# 
window = Window.partitionBy("department").orderBy(col("salary").desc())
df.withColumn("rank", row_number().over(window)).show()

# 
window = Window.partitionBy("user_id").orderBy("date")
df.withColumn("cumsum", sum("amount").over(window)).show()
```

### 

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("UserBehavior").getOrCreate()

# 
df = spark.read.json("user_behavior.json")

# 
print(":", df.count())
df.show(5)
df.printSchema()

# 1. DAU
dau = df.select(
    to_date("timestamp").alias("date"),
    "user_id"
).distinct().groupBy("date").count().withColumnRenamed("count", "dau")

dau.orderBy("date").show()

# 2. 
action_dist = df.groupBy("action").count().orderBy("count", ascending=False)
action_dist.show()

# 3. TOP10
top_items = df.filter(df.action == "view") \
    .groupBy("item_id") \
    .count() \
    .orderBy("count", ascending=False) \
    .limit(10)

top_items.show()

# 4. 
user_activity = df.groupBy("user_id").agg(
    count("*").alias("total_actions"),
    countDistinct("item_id").alias("unique_items"),
    collect_set("action").alias("action_types")
)

# 
user_activity = user_activity.withColumn(
    "user_level",
    when(col("total_actions") >= 100, "")
    .when(col("total_actions") >= 10, "")
    .otherwise("")
)

user_activity.groupBy("user_level").count().show()

# 5. 
funnel = df.groupBy("user_id").agg(
    sum(when(col("action") == "view", 1).otherwise(0)).alias("views"),
    sum(when(col("action") == "cart", 1).otherwise(0)).alias("carts"),
    sum(when(col("action") == "purchase", 1).otherwise(0)).alias("purchases")
)

funnel_stats = funnel.agg(
    count("*").alias("total_users"),
    sum("views").alias("total_views"),
    sum("carts").alias("total_carts"),
    sum("purchases").alias("total_purchases")
)

funnel_stats.show()

# 
funnel_stats = funnel_stats.withColumn(
    "view_to_cart_rate",
    col("total_carts") / col("total_views") * 100
).withColumn(
    "cart_to_purchase_rate",
    col("total_purchases") / col("total_carts") * 100
)

funnel_stats.show()

# 
dau.write.mode("overwrite").parquet("output/dau")
top_items.write.mode("overwrite").parquet("output/top_items")
user_activity.write.mode("overwrite").parquet("output/user_activity")
```

## Spark SQL

### SQL

```python
# 
df.createOrReplaceTempView("users")

# SQL
result = spark.sql("""
    SELECT 
        gender,
        COUNT(*) as count,
        AVG(age) as avg_age,
        MAX(salary) as max_salary
    FROM users
    WHERE age > 25
    GROUP BY gender
    ORDER BY count DESC
""")

result.show()

# 
result = spark.sql("""
    WITH user_stats AS (
        SELECT 
            user_id,
            COUNT(*) as action_count,
            COUNT(DISTINCT item_id) as unique_items
        FROM user_behavior
        GROUP BY user_id
    )
    SELECT 
        CASE 
            WHEN action_count >= 100 THEN ''
            WHEN action_count >= 10 THEN ''
            ELSE ''
        END as user_level,
        COUNT(*) as user_count,
        AVG(unique_items) as avg_unique_items
    FROM user_stats
    GROUP BY user_level
""")

result.show()
```

### 

```python
# 
users = spark.read.csv("users.csv", header=True)
orders = spark.read.csv("orders.csv", header=True)

# 
result = users.join(orders, users.user_id == orders.user_id, "inner")

# 
result = users.join(orders, "user_id", "left")

# 
result = users.join(orders, "user_id", "right")

# 
result = users.join(orders, "user_id", "outer")

# 
items = spark.read.csv("items.csv", header=True)
result = users.join(orders, "user_id") \
    .join(items, "item_id")

# SQL
users.createOrReplaceTempView("users")
orders.createOrReplaceTempView("orders")

result = spark.sql("""
    SELECT 
        u.name,
        u.age,
        o.order_id,
        o.amount
    FROM users u
    LEFT JOIN orders o ON u.user_id = o.user_id
""")
```

## 

### 1. 

```python
# 
df.cache()
df.persist()

# 
df.filter(df.age > 25).count()  # 
df.filter(df.age > 30).count()  # 

# 
df.unpersist()
```

### 2. 

```python
# 
df.repartition(10)  # 
df.coalesce(2)  # 

# 
df.repartition("date")

# 
df.write.partitionBy("date", "hour").parquet("output/")
```

### 3. 

```python
# 
small_df = spark.read.csv("small.csv")
from pyspark.sql.functions import broadcast

result = large_df.join(broadcast(small_df), "key")
```

### 4. Shuffle

```python
# shuffle
df.groupBy("key").count()

# 
df.filter(df.value > 100).groupBy("key").count()

# reduceByKeygroupByKey
rdd.reduceByKey(lambda x, y: x + y)  # 
rdd.groupByKey().mapValues(sum)  # 
```

## 

Spark

1. **RDD**API
2. **DataFrame**API
3. **Spark SQL**SQL
4. ****

****
- Spark Streaming
- Spark MLlib
- Spark GraphX

**Spark**

## 

1. 
2. 
3. Twitter
4. 

Kafka

