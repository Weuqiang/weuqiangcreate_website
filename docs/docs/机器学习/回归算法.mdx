---
sidebar_position: 3
title: 
---

## 

""

> 




$$\{  \}$$ 

- $ A = \{1, 2, 3\} $  $A$  $1, 2, 3$
- $f(x) = \{y | y = x^2, x \in R\}$  $f(x)$  $x^2$  $x$ 

$$\to $$ 

-  $a \to b$  $a$  $b$.

- $f:X\to Y$  $f$  $X$  $Y$ 

- 

$$ | $$ “”“”$$R_y = \{y | y \geq 0\}$$, $$R_y$$ $$y \geq 0$$  $$y$$

$$\in $$  $a$  $A$  $a \in A$.

$$\subseteq$$  A  B (A  B ),  $A \subseteq B$.

$$\subset$$   A  B (A  B ),  $A \subset B$.

### 

#### 

 $X$  $Y$ 

 $f$ $X$  $x$ $Y$  $y$ 

 $f$  $X$  $Y$  $f:X\to Y$

$y$  $x$  $f$  $y=f(x)$ $X$  $x$  $y$ 

 $X$ $f$ $D_f$

$Y$ $f$ $R_f$

$R_f = f(X)=\{f(x) | x\in D_f\}$



1.  $X$  $D_f = X$
2.  $Y$  $R_f \subseteq Y$
3. $f$ $X$  $x$ $Y$  $y$ 

****

 $x \in X$$f(x)$ 

 $y \in Y$$y$ 

> $f(x)=x^2$$y=1$  $x=1$  $x=-1$

 $R_f$  $Y$ , $R_f \subseteq Y$ $R_f = Y$

### 

0

- ****00
- **/**
- ****
- ****sincos

#### 



$$\frac{d}{dx} x^n = nx^{n-1}$$

1.  **“”**  $n$  $x$ 
2.  **“”**  $n$ 1 $n-1$

* ****
     $x^2$ 
    2212-1=1 $2x^1 = 2x$  
    $$\frac{d}{dx} x^2 = 2x^{2-1} = 2x$$

* ****
     $5x^3$ 
    5 $x^3$ 3531  
    $$\frac{d}{dx} 5x^3 = 5 \cdot 3x^{3-1} = 15x^2$$

* ****
     $\frac{1}{x^4}$   
    $\frac{1}{x^4} = x^{-4}$  
    -4-41$-4 - 1 = -5$  
    $$\frac{d}{dx} \frac{1}{x^4} = \frac{d}{dx} x^{-4} = -4x^{-4-1} = -4x^{-5} = -\frac{4}{x^5}$$


## 



$Y = aX + b$

$Y = β_0X_0 + β_1X_1 + β_2X_2 + ... + β_nX_n + ε$

 $ε$ 

### 





| 1 | 2 |  |  |  |
|--------|--------|--------|--------|----------|
| 0      | 0      | 0      | 0      | 0        |
| -4     | 4      | 0      | 4      | 16       |
| 7      | 1      | 4      | 4      | 25       |

(0,0)(-4,4)(7,1)MSE


1. -444
2. 74



|          |                                                              |          |                                                    |                                        |                                      |
|--------------|------------------------------------------------------------------------|--------------|--------------------------------------------------------------|--------------------------------------------|-----------------------------------------|
| Cross Entropy | $L = -\sum_{i} y_i \log(\hat{y}_i)$                                | $[0,+\infty)$ | $\frac{\partial L}{\partial \hat{y}_i} = -\frac{y_i}{\hat{y}_i}$ |  one-hot  | 0       |
| MSE      | $L = \frac{1}{n} \sum_{i} (y_i - \hat{y}_i)^2$                         | $[0,+\infty)$ | $\frac{\partial L}{\partial \hat{y}_i} = -\frac{2}{n}(y_i - \hat{y}_i)$ |                          |      |
| MAE  | $L = \frac{1}{n} \sum_{i} \vert y_i - \hat{y}_i \vert$                  | $[0,+\infty)$ | $\frac{\partial L}{\partial \hat{y}_i} = -\frac{1}{n}sgn(y_i - \hat{y}_i)$ |                    |                    |

### 



 $y = wx + b$

$$loss = (y - (wx + b))^2$$


- $y$ 
- $x$ 
- $w$  $b$ 

loss $w$  $b$ 

$$\frac{\partial loss}{\partial w} \quad \text{} \quad \frac{\partial loss}{\partial b}$$

loss

### 

```python showLineNumbers
import numpy as np
from matplotlib import pyplot as plt


class Line:
    def __init__(self, data):
        self.w = 1
        self.b = 0
        self.learning_rate = 0.01
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1)
        self.loss_list = []

    def get_data(self, data):
        self.X = np.array(data)[:, 0]
        self.y = np.array(data)[:, 1]

    def predict(self, x):
        return self.w * x + self.b
    
    def train(self, epoch_times):
        for epoch in range(epoch_times):
            total_loss = 0
            for x, y in zip(self.X, self.y):
                y_pred = self.predict(x)
                # Calculate gradients
                gradient_w = -2 * x * (y - y_pred)
                gradient_b = -2 * (y - y_pred)
                # Update weights
                self.w -= self.learning_rate * gradient_w
                self.b -= self.learning_rate * gradient_b
                # Calculate loss
                loss = (y - y_pred) ** 2
                total_loss += loss
            epoch_loss = total_loss / len(self.X)
            self.loss_list.append(epoch_loss)
            if epoch % 10 == 0:
                print(f"loss: {epoch_loss}")
                self.plot()
        plt.ioff()
        plt.show()

    def plot(self):
        plt.ion()  # Enable interactive mode
        self.ax2.clear()
        self.ax1.clear()
        x = np.linspace(0, 10, 100)
        self.ax1.scatter(self.X, self.y, c="g")
        self.ax1.plot(x, self.predict(x), c="b")
        self.ax2.plot(list(range(len(self.loss_list))), self.loss_list)
        plt.show()
        plt.pause(0.1)

if __name__ == "__main__":  
    # Input data
    data = [(1, 1), (1.8, 2), (2.5, 3), (4.2, 4), (5, 5), (6, 6), (7, 7)]
    s = Line(data)
    s.get_data(data)
    s.train(100)
```

### 

```python showLineNumbers
from sklearn import datasets
from sklearn.linear_model import LinearRegression

# .fetch_california_housing() 
loaded_data = datasets.fetch_california_housing()
# .data 
data_X = loaded_data.data
# .target 
data_y = loaded_data.target
# 
model = LinearRegression()
# 
# .fit() 
model.fit(data_X, data_y)
# 
print(" ():", model.coef_)
print(":", model.intercept_)


# 
# .predict() 
print(model.predict(data_X[:4, :]))
# 
print(data_y[:4])

# 
print(model.get_params())# 
# //{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}
print(model.score(data_X, data_y))
# // 0.606232685199805
#  60%  40% 
```


## 



S

```python showLineNumbers
# sigmoid

import matplotlib.pyplot as plt
import numpy as np

def sigmoid(x,p=1):
    # sigmoid
    return 1. / (1. + np.exp(-p*x))

def plot_sigmoid(p=1):
    # param:
    x = np.arange(-8, 8, 0.2)
    y = sigmoid(x,p)
    plt.plot(x, y)
    plt.show()

if __name__ == '__main__':
    plot_sigmoid()
    plot_sigmoid(20)
    plot_sigmoid(0.5)
```

 logit  x  y Logit  y  x  sigmoid 

$f(x) = \frac{1}{1 + e^{-x}}$

 sigmoid 

$y = f(β0 + β1x1 + β2x2+… βnxn)$

 β logit 

 sigmoid  SigmoidReLUSoftmaxTanh 

|      |                      |          |                                        |                                        |                                            |
|----------|--------------------------------|--------------|--------------------------------------------------|--------------------------------------------|------------------------------------------------|
| Sigmoid  | $\sigma(x) = \frac{1}{1 + e^{-x}}$ | $(0, 1)$     | $\sigma'(x) = \sigma(x)(1 - \sigma(x))$         |                              | 0                            |
| ReLU     | $\text{ReLU}(x) = \max(0, x)$     | $[0, +\infty)$ | $\text{ReLU}'(x) = \begin{cases} 1, & x > 0 \\ 0, & x \le 0 \end{cases}$ |                              | 0                           |
| Tanh     | $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ | $(-1, 1)$    | $\tanh'(x) = 1 - \tanh^2(x)$                     | 0                            |                                          |
| Softmax  | $\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$ | $(0,1)$  $\sum=1$ | $\frac{\partial y_i}{\partial x_j} = y_i(\delta_{ij} - y_j)$ |                      |                            |



### 

```python showLineNumbers
import numpy as np
from matplotlib import pyplot as plt

class Sline:
    def __init__(self, data):
        self.w = 0
        self.b = 0
        self.learning_rate = 0.1
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1)
        self.loss_list = []


    def get_data(self, data):
        self.X = np.array(data)[:, 0]
        self.y = np.array(data)[:, 1]

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-(self.w * x + self.b)))

    def train(self, epoch_times):
        for epoch in range(epoch_times):
            total_loss = 0
            for x, y in zip(self.X, self.y):
                y_pred = self.sigmoid(x)
                # w = w -  * 
                grad = -2 * (y - y_pred) * (1 - y_pred) * y_pred * x
                self.w = self.w - self.learning_rate * grad * x
                # b = b -  * 
                self.b = self.b - self.learning_rate * grad
                loss = (y - y_pred) ** 2
                total_loss += loss
            epoch_loss = total_loss / len(self.X)
            self.loss_list.append(epoch_loss)
            if epoch % 10 == 0:
                print(f"loss: {epoch_loss}")
                self.plot()
        plt.ioff()
        plt.show()

    def plot(self):
        plt.ion()  # 
        self.ax2.clear()
        self.ax1.clear()
        x = np.linspace(0, 10, 100)
        self.ax1.scatter(self.X, self.y, c="g")
        self.ax1.plot(x, self.sigmoid(x), c="b")
        self.ax2.plot(list(range(len(self.loss_list))), self.loss_list)
        plt.show()
        plt.pause(0.1)

if __name__ == "__main__":  
    # 
    data = [(1, 0), (1.8, 0), (2.5, 0), (4.2, 1), (5, 1), (6, 1), (7, 1)]
    s = Sline(data)
    s.get_data(data)
    s.train(1000)

```


### sklearn

```python showLineNumbers
import numpy as np
from sklearn.linear_model import LogisticRegression

# 
X = np.array([[1], [2], [3], [4], [5]])  # 
y = np.array([0, 0, 1, 1, 2])  # 01

# 
model = LogisticRegression()

# .fit()x
model.fit(X, y)

# 
new_data_point = np.array([[6]])  # 
# .predict()
predicted_class = model.predict(new_data_point)
# .predict_proba()
predicted_probability = model.predict_proba(new_data_point)

print(":", predicted_class)
print(" (, ):", predicted_probability)
print(type(predicted_probability))
predicted_probability
```
### 

```python showLineNumbers
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# 
digits = datasets.load_digits()
# 
X = digits.data
y = digits.target

#  ,  random_state42
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 
scaler = StandardScaler()
# .fit_transform()
X_train = scaler.fit_transform(X_train)
# .transform()
X_test = scaler.transform(X_test)

# Logistic Regression  ,  random_state42
model = LogisticRegression(random_state=42)

# .fit()
model.fit(X_train, y_train)

# .predict()
y_pred = model.predict(X_test)

# 
from sklearn.metrics import accuracy_score

# accuracy_score()
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

# -
from matplotlib import pyplot as plt
# 
index = []
# 
for i in range(len(y_pred)):
    # 
    if y_pred[i] != y_test[i]:
        # ,index:,,(8*8)
        index.append((
                    y_pred[i],
                    y_test[i],
                    X_test[i].reshape((8, 8))))

# 
# nrows:
# ncols:
# print(len(index)) // 10
# 10,2,52*5=10
fig, ax = plt.subplots(
    nrows=3,
    ncols=5,
)
# 
ax = ax.flatten()
for i in range(len(index)):
    p = index[i][0] # 
    a = index[i][1] # 
    img = index[i][2] # 
    # 
    ax[i].imshow(img, cmap='Greys')
    ax[i].set_title(f'{p}-{a}')
plt.show()


```