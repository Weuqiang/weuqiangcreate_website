---
sidebar_position: 7
title: 
---



TransformerPyTorchTransformer



<details>
<summary>

</summary>
tokentokenizer

token

Byte-Pair Encoding (BPE)


</details>


<details>
<summary>

</summary>


- Transformertokentoken

- TransformerPositional Encoding tokentoken

- padding
</details>



<details>
<summary>
Temperature 
</summary>

Temperature token

$$
Attention(Q, K, V) = softmax(QK^T / \sqrt{d_k}) * V
$$
-  Temperature token
-  Temperature token

Temperature 0-2 Temperature 

 Temperature 0 Temperature1.5

</details>




