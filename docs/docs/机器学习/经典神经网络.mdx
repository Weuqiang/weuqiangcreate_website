---
sidebar_position: 5
title: 
---

## 





### BP

1986RomelhartMcclellandBackpropagationBP""BP

### MLP

****

 →  → 



### 

BP50×502500100250


- **** → CNN
- **** → RNNTransformer

BP

### 




-  ×  →  →  → 

:::tip
3Blue1Brown[](https://www.3blue1brown.com/topics/neural-networks)
:::


## 

28×2878428×28=784

784 →  → 100-9

### 

- ****784
- ****16
- ****100-9

### 



$$
\begin{aligned}
a_{0}^{1} &= \sigma \left( \sum_{i=1}^{784} (w_{0,i} \cdot a_{i}^{0}) + b_{0} \right)\\
a_{1}^{1} &= \sigma \left( \sum_{i=1}^{784} (w_{1,i} \cdot a_{i}^{0}) + b_{1} \right)\\
&\vdots\\
a_{15}^{1} &= \sigma \left( \sum_{i=1}^{784} (w_{15,i} \cdot a_{i}^{0}) + b_{15} \right)
\end{aligned}
$$


$$
\begin{bmatrix}
    w_{0,0} & w_{0,1} & ... & w_{0,784} \\
    w_{1,0} & w_{1,1} & ... & w_{1,784} \\
    ... & ... & ... & ... \\
    w_{15,0} & w_{15,1} & ... & w_{15,784} 
\end{bmatrix}
$$


```python showLineNumbers 
import numpy

# nb
class NeuralNetwork:
    # 
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
        # 
        self.inodes = inputnodes
        self.hnodes = hiddennodes
        self.onodes = outputnodes
        # wih  who
        #  w_i_j i  j
        # w11 w21
        # w12 w22 
        # 
        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))
        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))
        # 
        self.lr = learningrate
        #  sigmoid 
        self.activation_function = lambda x: 1 / (1 + numpy.exp(-x))
        pass
    
    # 
    def train(self, , ):
        #  2D 
        inputs = numpy.array(, ndmin=2).T
        targets = numpy.array(, ndmin=2).T
        # 
        hidden_inputs = numpy.dot(self.wih, inputs)
        # 
        hidden_outputs = self.activation_function(hidden_inputs)
        
        # 
        final_inputs = numpy.dot(self.who, hidden_outputs)
        # 
        final_outputs = self.activation_function(final_inputs)
        
        #  ( - )
        output_errors = targets - final_outputs
        # 
        hidden_errors = numpy.dot(self.who.T, output_errors)
        
        # 
        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
        
        # 
        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
        
        pass
    
    # 
    def query(self, ):
        #  2D 
        inputs = numpy.array(, ndmin=2).T
        
        # 
        hidden_inputs = numpy.dot(self.wih, inputs)
        # 
        hidden_outputs = self.activation_function(hidden_inputs)
        
        # 
        final_inputs = numpy.dot(self.who, hidden_outputs)
        # 
        final_outputs = self.activation_function(final_inputs)
        
        return final_outputs
    

input_nodes = 64
hidden_nodes = 32
output_nodes = 10

# 
learning_rate = 0.2

# 
n = NeuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)

from sklearn import datasets
from sklearn.model_selection import train_test_split
# 
digits = datasets.load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.1,random_state=0)

epochs = 2  # 
 
for i in range(epochs):
    for record in zip(y_train, X_train):
        y, X = record
        # 
        inputs = (numpy.asfarray(X) / 16.0 * 0.99) + 0.01
        #  0.01 0.99
        targets = numpy.zeros(output_nodes) + 0.01
        # all_values[0] 
        targets[int(y)] = 0.99
        n.train(inputs, targets)
# 
our_own_dataset = []

for i in zip(y_test, X_test):
    label, img_array = i
    # img_data  = 16.0 - img_array
    #  0.01  1.0
    img_data = (img_array / 16.0 * 0.99) + 0.01
    # 
    record = numpy.append(label, img_data)
    our_own_dataset.append(record)
    pass

right = 0
error = 0
for item in range(len(our_own_dataset)):
    # 
    correct_label = our_own_dataset[item][0]
    # print(correct_label)

    # 
    inputs = our_own_dataset[item][1:]
 
    # 
    outputs = n.query(inputs)

    # 
    label = numpy.argmax(outputs)
    if label != correct_label:
        error += 1
    else:
        right += 1
print(right / (right + error))

```


### 




1. ****
2. ****8×8
3. ****
4. ****




```python showLineNumbers 
from PIL import ImageFont,Image,ImageDraw
# 
c_chars = "0 1 2 3 4"
path = 'test.png'
size = (100,24)                             #
img = Image.new("RGB",size)
draw = ImageDraw.Draw(img)                  #draw
font = ImageFont.truetype("arial.ttf", 23)      #
draw.text((5,0),c_chars,font=font,fill="white") #
# img.show()
img.save(path)

# 
def sliceImg(img_path, count = 5):
    img = Image.open(img_path).convert("L")
    w, h = img.size
    eachWidth = int(w/count)
    for i in range(count):
        box = (i * eachWidth, 0, (i + 1) * eachWidth, h)
        yield img.crop(box)

# 
def exchange(img):
    target_size = (8, 8)
    resized_image = img.resize(target_size)
    # resized_image.show()
    return resized_image


out = ""
for i in sliceImg(path):
    # .flatten()
    original_array =  numpy.array(exchange(i)).flatten()
    # 0-1
    inputs = (original_array / 255.0 * 0.99) + 0.01
    # 
    outputs = n.query(inputs)
    # print(outputs)
    # 
    label = numpy.argmax(outputs)
    out+=str(label)
print(out)
```


01243

****

Apadding
Bepochs
C3

<details>
<summary></summary>

8×8383

****3

```python showLineNumbers
from PIL import ImageFont,Image,ImageDraw
# 
c_chars = "3 3 3 3 3"
path = 'test.png'
size = (100,24)                             #
img = Image.new("RGB",size)
draw = ImageDraw.Draw(img)                  #draw
font = ImageFont.truetype("arial.ttf", 23)      #
draw.text((5,0),c_chars,font=font,fill="white") #
# img.show()
img.save(path)

# 
def sliceImg(img_path, count = 5):
    img = Image.open(img_path).convert("L")
    w, h = img.size
    eachWidth = int(w/count)
    for i in range(count):
        box = (i * eachWidth, 0, (i + 1) * eachWidth, h)
        yield img.crop(box)

# 
def exchange(img):
    target_size = (8, 8)
    resized_image = img.resize(target_size)
    # resized_image.show()
    return resized_image


for i in sliceImg(path):
    # .flatten()
    original_array =  numpy.array(exchange(i)).flatten()
    # 0-1
    inputs = (original_array / 255.0 * 0.99) + 0.01
    targets = numpy.zeros(output_nodes) + 0.01
    # all_values[0] 
    targets[int(3)] = 0.99
    n.train(inputs, targets)

# 
c_chars = "0 1 2 3 4"
path = 'test.png'
size = (100,24)                             #
img = Image.new("RGB",size)
draw = ImageDraw.Draw(img)                  #draw
font = ImageFont.truetype("arial.ttf", 23)      #
draw.text((5,0),c_chars,font=font,fill="white") #
# img.show()
img.save(path)

out = ""
for i in sliceImg(path):
    # .flatten()
    original_array =  numpy.array(exchange(i)).flatten()
    # 0-1
    inputs = (original_array / 255.0 * 0.99) + 0.01
    # 
    outputs = n.query(inputs)
    # print(outputs)
    # 
    label = numpy.argmax(outputs)
    out+=str(label)
print(out)

# 
import os
os.remove(path)
```

01234

</details>

### PyTorch

```python showLineNumbers
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# 
# Compose 
transform = transforms.Compose([
    # 
    transforms.ToTensor(),
    # 0.13070.3081
    # 0.13070.3081
    # MNIST0-2550-1
    # 0.1307MNIST0.3081MNIST
    # 0.13070.3081
    transforms.Normalize((0.1307,), (0.3081,))  
])

# MNIST
# root
# train(train=True)
# download
# transform
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# 
# batch_sizebatch
# batch_size=1
# batch_size=10001000
# batch_size
# shuffle
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# 
# nn.Moduleforward
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        # nn.Module
        super(NeuralNetwork, self).__init__()
        # 1
        self.layer1 = nn.Linear(input_size, hidden_size)
        # ReLU
        self.relu = nn.ReLU()
        # 2
        self.layer2 = nn.Linear(hidden_size, output_size)

    # PyTorchforward
    # nn.Modulenn.Moduleforwardforwardloss.backward()
    def forward(self, x):
        # 
        x = x.reshape(-1, input_size)  
        # 1
        x = self.layer1(x)
        # ReLU
        x = self.relu(x)
        # 2
        x = self.layer2(x)
        return x

    # # 
    # def backward(self, x):
    #     # 1
    #     x = self.layer1.backward(x)
    #     # ReLU
    #     x = self.relu.backward(x)
    #     # 2
    #     x = self.layer2.backward(x)

# 
# 28*28=784
input_size = 28 * 28 
# 1282
hidden_size = 128
# 10  MNIST10
output_size = 10
model = NeuralNetwork(input_size, hidden_size, output_size)

# 
# 1
# 
# 
# loss = -sum(y_true * log(y_pred))
# 
# loss = sum((y_true - y_pred) ** 2)
criterion = nn.CrossEntropyLoss()
# 2
# AdamRMSprop
# SGD
# optimizer = optim.SGD(model.parameters(), lr=0.01)
# lr
# momentum
# momentumg_v = momentum * g_v + lr * grad
# gradlrmomentumg_v
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# epochs
# 
epochs = 5

for epoch in range(epochs):
    running_loss = 0.0
    for i, (images, labels) in enumerate(train_loader):
        # ,
        # BatchNorm
        # Dropout
        # 
        model.train()
        # 
        outputs = model(images)
        # 
        loss = criterion(outputs, labels)
        # 
        optimizer.zero_grad()
        # 
        '''
PyTorchbackward


1. PyTorch`grad_fn`
2. `loss``.backward()`PyTorch
3. `requires_grad=True`
4. PyTorch
        '''
        loss.backward()
        # ,weight = weight - lr * grad
        optimizer.step()
        # 
        running_loss += loss.item()
        
    # epoch
    correct = 0
    total = 0
    # 
    # 
    with torch.no_grad():
        # 
        for images, labels in test_loader:
            # 
            # BatchNorm
            # Dropout
            model.eval()
            outputs = model(images)
            # 
            # torch.maximum
            # torch.max
            # torch.maximum
            # 
            _, predicted = torch.max(outputs.data, 1)
            # ,labels.size(0) 
            #  DataLoader  batch_size 
            total += labels.size(0)
            # 
            # 
            print(predicted) # batch_size
            print(labels) # batch_size()
            
            # 
            test = (predicted == labels)
            # [ True .... False ],PythonTrue1False0
            # test.sum()
            # .item() 
            correct += test.sum().item()
    
    accuracy = correct / total
    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}')

# 
correct = 0
total = 0
# 
with torch.no_grad():
    for images, labels in test_loader:
        # 
        """
model.train()model.eval()
1. BatchNormDropout
2. PyTorchmodel.train()model.eval()

ResNetBERT
model.train()model.eval()

        """
        model.eval()
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = correct / total
print(f': {accuracy:.4f}')

```

