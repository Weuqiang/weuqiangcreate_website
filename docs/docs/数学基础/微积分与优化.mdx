---
sidebar_position: 3
title:  - 
---

#  - AI

****

##  - 

### 

****

```python
import numpy as np
import matplotlib.pyplot as plt

# y = x^2
x = np.linspace(-5, 5, 100)
y = x ** 2

# dy/dx = 2x
dy_dx = 2 * x

# 
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

ax1.plot(x, y, 'b-', linewidth=2)
ax1.set_title(': y = x²')
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.grid(True)

ax2.plot(x, dy_dx, 'r-', linewidth=2)
ax2.axhline(y=0, color='k', linestyle='--')
ax2.set_title(': dy/dx = 2x')
ax2.set_xlabel('x')
ax2.set_ylabel('dy/dx')
ax2.grid(True)

plt.tight_layout()
plt.show()
```

****
-  > 0
-  < 0
-  = 0

### 

```python
# 1. d(x^n)/dx = n*x^(n-1)
def power_derivative(x, n):
    return n * x ** (n - 1)

# 2. d(e^x)/dx = e^x
def exp_derivative(x):
    return np.exp(x)

# 3. d(ln(x))/dx = 1/x
def log_derivative(x):
    return 1 / x

# 4. d(sin(x))/dx = cos(x)
def sin_derivative(x):
    return np.cos(x)

# 
x = 2.0
h = 0.0001

# 
f = lambda x: x**3
numerical = (f(x + h) - f(x)) / h
analytical = 3 * x**2

print(f": {numerical:.6f}")
print(f": {analytical:.6f}")
print(f": {abs(numerical - analytical):.10f}")
```

###  - 

```python
def gradient_descent_demo():
    """"""
    # f(x) = (x - 3)^2
    # f'(x) = 2(x - 3)
    
    def f(x):
        return (x - 3) ** 2
    
    def df(x):
        return 2 * (x - 3)
    
    # 
    x = 0  # 
    learning_rate = 0.1
    history = [x]
    
    for i in range(20):
        gradient = df(x)
        x = x - learning_rate * gradient  # 
        history.append(x)
        
        if i % 5 == 0:
            print(f"Step {i}: x = {x:.4f}, f(x) = {f(x):.4f}, gradient = {gradient:.4f}")
    
    # 
    x_plot = np.linspace(-1, 7, 100)
    y_plot = f(x_plot)
    
    plt.figure(figsize=(10, 6))
    plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='f(x) = (x-3)²')
    plt.plot(history, [f(h) for h in history], 'ro-', markersize=8, label='')
    plt.xlabel('x')
    plt.ylabel('f(x)')
    plt.title('')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    print(f"\n: x = {x:.4f}, : x = 3")

gradient_descent_demo()
```

##  - 

### 

** =  × **

```python
#  y = f(g(x)) dy/dx = df/dg * dg/dx

# y = (2x + 1)^2
#  u = 2x + 1 y = u^2
# dy/dx = dy/du * du/dx = 2u * 2 = 4(2x + 1)

def chain_rule_example(x):
    """"""
    # 
    u = 2 * x + 1
    y = u ** 2
    
    # 
    dy_du = 2 * u  # yu
    du_dx = 2      # ux
    dy_dx = dy_du * du_dx  # 
    
    return y, dy_dx

x = 3
y, grad = chain_rule_example(x)
print(f"x={x}, y={y}, dy/dx={grad}")
# x=3, y=49, dy/dx=28

# 
h = 0.0001
f = lambda x: (2*x + 1)**2
numerical_grad = (f(x + h) - f(x)) / h
print(f": {numerical_grad:.4f}")
```

### 

```python
class SimpleNetwork:
    """"""
    
    def __init__(self):
        self.W1 = np.random.randn(2, 3) * 0.01
        self.W2 = np.random.randn(3, 1) * 0.01
    
    def forward(self, x):
        """"""
        self.x = x
        
        # 
        self.z1 = np.dot(x, self.W1)
        self.a1 = np.maximum(0, self.z1)  # ReLU
        
        # 
        self.z2 = np.dot(self.a1, self.W2)
        
        return self.z2
    
    def backward(self, grad_output):
        """"""
        # 
        # dL/dW2 = dL/dz2 * dz2/dW2
        grad_W2 = np.dot(self.a1.T, grad_output)
        
        # 
        # dL/da1 = dL/dz2 * dz2/da1
        grad_a1 = np.dot(grad_output, self.W2.T)
        
        # ReLU
        grad_z1 = grad_a1.copy()
        grad_z1[self.z1 <= 0] = 0
        
        # 
        # dL/dW1 = dL/dz1 * dz1/dW1
        grad_W1 = np.dot(self.x.T, grad_z1)
        
        return grad_W1, grad_W2
    
    def check_gradients(self):
        """"""
        x = np.array([[1.0, 2.0]])
        
        # 
        output = self.forward(x)
        
        # 
        grad_W1, grad_W2 = self.backward(np.array([[1.0]]))
        
        # 
        eps = 1e-5
        numerical_grad_W1 = np.zeros_like(self.W1)
        
        for i in range(self.W1.shape[0]):
            for j in range(self.W1.shape[1]):
                self.W1[i, j] += eps
                output_plus = self.forward(x)
                
                self.W1[i, j] -= 2 * eps
                output_minus = self.forward(x)
                
                self.W1[i, j] += eps
                
                numerical_grad_W1[i, j] = (output_plus - output_minus) / (2 * eps)
        
        # 
        diff = np.linalg.norm(grad_W1 - numerical_grad_W1)
        print(f": {diff:.10f}")
        print(":\n", grad_W1)
        print(":\n", numerical_grad_W1)

# 
net = SimpleNetwork()
net.check_gradients()
```

## 

```python
class ActivationFunctions:
    """"""
    
    @staticmethod
    def sigmoid(x):
        """Sigmoid: σ(x) = 1 / (1 + e^(-x))"""
        return 1 / (1 + np.exp(-x))
    
    @staticmethod
    def sigmoid_derivative(x):
        """Sigmoid: σ'(x) = σ(x) * (1 - σ(x))"""
        s = ActivationFunctions.sigmoid(x)
        return s * (1 - s)
    
    @staticmethod
    def tanh(x):
        """Tanh: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))"""
        return np.tanh(x)
    
    @staticmethod
    def tanh_derivative(x):
        """Tanh: tanh'(x) = 1 - tanh²(x)"""
        t = np.tanh(x)
        return 1 - t**2
    
    @staticmethod
    def relu(x):
        """ReLU: max(0, x)"""
        return np.maximum(0, x)
    
    @staticmethod
    def relu_derivative(x):
        """ReLU: 1 if x > 0 else 0"""
        return (x > 0).astype(float)
    
    @staticmethod
    def leaky_relu(x, alpha=0.01):
        """Leaky ReLU: max(αx, x)"""
        return np.where(x > 0, x, alpha * x)
    
    @staticmethod
    def leaky_relu_derivative(x, alpha=0.01):
        """Leaky ReLU"""
        return np.where(x > 0, 1, alpha)
    
    @staticmethod
    def visualize():
        """"""
        x = np.linspace(-5, 5, 1000)
        
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        
        # Sigmoid
        axes[0, 0].plot(x, ActivationFunctions.sigmoid(x))
        axes[0, 0].set_title('Sigmoid')
        axes[0, 0].grid(True)
        
        axes[1, 0].plot(x, ActivationFunctions.sigmoid_derivative(x))
        axes[1, 0].set_title('Sigmoid')
        axes[1, 0].grid(True)
        
        # Tanh
        axes[0, 1].plot(x, ActivationFunctions.tanh(x))
        axes[0, 1].set_title('Tanh')
        axes[0, 1].grid(True)
        
        axes[1, 1].plot(x, ActivationFunctions.tanh_derivative(x))
        axes[1, 1].set_title('Tanh')
        axes[1, 1].grid(True)
        
        # ReLU
        axes[0, 2].plot(x, ActivationFunctions.relu(x))
        axes[0, 2].set_title('ReLU')
        axes[0, 2].grid(True)
        
        axes[1, 2].plot(x, ActivationFunctions.relu_derivative(x))
        axes[1, 2].set_title('ReLU')
        axes[1, 2].grid(True)
        
        # Leaky ReLU
        axes[0, 3].plot(x, ActivationFunctions.leaky_relu(x))
        axes[0, 3].set_title('Leaky ReLU')
        axes[0, 3].grid(True)
        
        axes[1, 3].plot(x, ActivationFunctions.leaky_relu_derivative(x))
        axes[1, 3].set_title('Leaky ReLU')
        axes[1, 3].grid(True)
        
        plt.tight_layout()
        plt.show()

# 
ActivationFunctions.visualize()
```

##  - 

```python
class Tensor:
    """"""
    
    def __init__(self, data, requires_grad=False, _children=()):
        self.data = np.array(data, dtype=float)
        self.grad = None
        self.requires_grad = requires_grad
        self._backward = lambda: None
        self._prev = set(_children)
    
    def __repr__(self):
        return f"Tensor({self.data}, grad={self.grad})"
    
    def __add__(self, other):
        """"""
        other = other if isinstance(other, Tensor) else Tensor(other)
        out = Tensor(self.data + other.data, requires_grad=True, _children=(self, other))
        
        def _backward():
            if self.requires_grad:
                self.grad = self.grad + out.grad if self.grad is not None else out.grad.copy()
            if other.requires_grad:
                other.grad = other.grad + out.grad if other.grad is not None else out.grad.copy()
        
        out._backward = _backward
        return out
    
    def __mul__(self, other):
        """"""
        other = other if isinstance(other, Tensor) else Tensor(other)
        out = Tensor(self.data * other.data, requires_grad=True, _children=(self, other))
        
        def _backward():
            if self.requires_grad:
                grad = other.data * out.grad
                self.grad = self.grad + grad if self.grad is not None else grad.copy()
            if other.requires_grad:
                grad = self.data * out.grad
                other.grad = other.grad + grad if other.grad is not None else grad.copy()
        
        out._backward = _backward
        return out
    
    def __pow__(self, power):
        """"""
        out = Tensor(self.data ** power, requires_grad=True, _children=(self,))
        
        def _backward():
            if self.requires_grad:
                grad = power * (self.data ** (power - 1)) * out.grad
                self.grad = self.grad + grad if self.grad is not None else grad.copy()
        
        out._backward = _backward
        return out
    
    def relu(self):
        """ReLU"""
        out = Tensor(np.maximum(0, self.data), requires_grad=True, _children=(self,))
        
        def _backward():
            if self.requires_grad:
                grad = (self.data > 0).astype(float) * out.grad
                self.grad = self.grad + grad if self.grad is not None else grad.copy()
        
        out._backward = _backward
        return out
    
    def backward(self):
        """"""
        # 
        topo = []
        visited = set()
        
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        
        build_topo(self)
        
        # 
        self.grad = np.ones_like(self.data)
        for node in reversed(topo):
            node._backward()

# 
print("=== 1:  ===")
x = Tensor([2.0], requires_grad=True)
y = Tensor([3.0], requires_grad=True)

# z = x * y + x^2
z = x * y + x ** 2

z.backward()

print(f"x = {x.data}, dx = {x.grad}")  # dx = y + 2x = 3 + 4 = 7
print(f"y = {y.data}, dy = {y.grad}")  # dy = x = 2

print("\n=== 2:  ===")
# y = relu(x * w1) * w2
x = Tensor([1.0], requires_grad=True)
w1 = Tensor([2.0], requires_grad=True)
w2 = Tensor([3.0], requires_grad=True)

h = (x * w1).relu()
y = h * w2

y.backward()

print(f"x = {x.data}, dx = {x.grad}")
print(f"w1 = {w1.data}, dw1 = {w1.grad}")
print(f"w2 = {w2.data}, dw2 = {w2.grad}")
```

[](./.mdx)

