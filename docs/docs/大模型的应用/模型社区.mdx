---
sidebar_position: 1
title: 
---

## 

“”

- Framework Transformer 
- Weights

 LLaMAMetaQwenChatGLMAttention Is All You Need Transformer 

### 

#### 



- 
- 
- 
- 

 QwenChatGLM 

Qwen Qwen → Qwen-2 → Qwen-2.5 → Qwen3 



| **** | **** |
| :---- | :---- |
| :  (Qwen)/(Wan) | : Gemini/Veo |
| :  (coze)/(Seedream)  | OpenAI: GPT/Sora |
| : DeepSeek | Anthropic: Claude |
| :  (GLM) | xAI: Grok |
| :  (Hunyuan) | Meta: Llama |

### 

#### 

7B7014B14072B720



 Qwen3-7BQwen3-14BQwen3-72B

:::info


- Qwen3-14B  Qwen3-Fast
- Qwen3-72B  Qwen3-High
:::

#### 

fine-tuning

- Full Fine-tuning
- LoRA Low-Rank Adaptation



- Qwen3-Coder
- Qwen3-Math

#### 

 float64

float32 → float16 / bf16 → int8 / int4
 99.99 → 99

- 
- 
- 
- 

quantization

-fp16-int8-g16 

### 



```mermaid
graph LR
    A[Transformer<br/>Attention Is All You Need] --> B[]
    
    B --> C1[]
    C1 --> D1[]
    C1 --> D2[]
    C1 --> D3[]

    D1 & D2 & D3 --> E[<br/>Qwen → Qwen-2 → Qwen3]
    
    B --> F[]
    F --> G1[]
    F --> G2[]
    F --> G3[]

    G1 & G2 & G3 --> G4[<br/>Qwen3-7B  / Qwen3-72B]
    
    B --> H[Fine-tuning]
    H --> I1[<br/>Full Fine-tuning]
    H --> I2[LoRA<br/>Low-Rank Adaptation]

    I1 & I2 --> J1[<br/>Qwen3-Coder/Qwen3-Math]

    B --> K[Quantization]
    K --> L1[fp16<br/>]
    K --> L2[int8<br/>8]
    K --> L3[int4<br/>4]

    L1 & L2 & L3 --> M[<br/>Qwen3-fp16/Qwen3-int8]
    
    style A fill:#e1f5ff
    style B fill:#fff3e0
    style E fill:#f3e5f5
    style F fill:#e8f5e9
    style H fill:#fff9c4
    style K fill:#fce4ec
    
    classDef base fill:#e1f5ff,stroke:#0288d1,stroke-width:2px
    classDef version fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef param fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    classDef tune fill:#fff9c4,stroke:#f57c00,stroke-width:2px
    classDef quant fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    
    class A base
    class E version
    class G1,G2,G3 param
    class J1,J2 tune
    class L1,L2,L3 quant
```



|  |  |  |  |
| :---- | :---- | :---- | :---- |
|  | 2020–2022 |  | GPT-3.5 |
|  | 2023–2024 |  | GPT-4o |
|  | 2025– | MoE+ Agent | Claude-4.5 |

:::info
Qwen 3-MAX MoE1T1000B Llama 3.1405B 
:::

## 





### 

Hugging Face  `transformers` [https://huggingface.co/](https://huggingface.co/)

`modelscope`  Hugging Face  `transformers` [https://www.modelscope.cn/](https://www.modelscope.cn/)

 Qwen  `transformers`  `modelscope` `model_name`

```python showLineNumbers
types = "huggingface"  # "huggingface"  "modelscope"
if types == "huggingface":
    from transformers import AutoModelForCausalLM, AutoTokenizer
elif types == "modelscope":
    from modelscope import AutoModelForCausalLM, AutoTokenizer

model_size = "3B"  # 3B 7B 14B 32B
model_name = f"Qwen/Qwen2.5-{model_size}-Instruct"

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

while True:
    prompt = input(": ")
    if prompt == "":
        break

    messages = [
        {
            "role": "system",
            "content": "AI",
        },
        {"role": "user", "content": prompt},
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_input = tokenizer([text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(**model_input, max_new_tokens=512)
    generated_ids = [output[len(input_ids):] for input_ids, output in zip(model_input.input_ids, generated_ids)]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print(response)
```
### 



```python showLineNumbers
from modelscope.msdatasets import MsDataset
dataset = MsDataset.load('swift/Chinese-Qwen3-235B-2507-Distill-data-110k-SFT')
print(f'dataset[0]: {dataset[0]}')
```
