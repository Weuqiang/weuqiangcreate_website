---
sidebar_position: 14
title: AI
---

# AI

:::info 
AI
:::

## 1

### 

Web

### 

- ****: FastAPI
- ****: YOLOv8
- ****: Streamlit
- ****: Docker

### 

```python
# app.py
from fastapi import FastAPI, File, UploadFile
from fastapi.responses import JSONResponse
from ultralytics import YOLO
from PIL import Image
import io

app = FastAPI()
model = YOLO('yolov8n.pt')

@app.post("/detect")
async def detect_objects(file: UploadFile = File(...)):
    """"""
    # 
    contents = await file.read()
    image = Image.open(io.BytesIO(contents))
    
    # 
    results = model(image)
    
    # 
    detections = []
    for result in results:
        boxes = result.boxes
        for box in boxes:
            detections.append({
                'class': result.names[int(box.cls)],
                'confidence': float(box.conf),
                'bbox': box.xyxy[0].tolist()
            })
    
    return JSONResponse(content={'detections': detections})

# streamlit_app.py
import streamlit as st
import requests
from PIL import Image

st.title("")

uploaded_file = st.file_uploader("", type=['jpg', 'png'])

if uploaded_file:
    image = Image.open(uploaded_file)
    st.image(image, caption='')
    
    if st.button(''):
        # API
        files = {'file': uploaded_file.getvalue()}
        response = requests.post('http://localhost:8000/detect', files=files)
        
        if response.status_code == 200:
            results = response.json()['detections']
            st.write("")
            for det in results:
                st.write(f"- {det['class']}: {det['confidence']:.2%}")
```

### Docker

```dockerfile
# Dockerfile
FROM python:3.9

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

## 2

### 



### 

- 
- 
- 
- 

### 

```python
from langchain.llms import OpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

class ChatBot:
    """"""
    
    def __init__(self):
        # LLM
        self.llm = OpenAI(temperature=0.7)
        
        # 
        self.memory = ConversationBufferMemory()
        
        # 
        self.conversation = ConversationChain(
            llm=self.llm,
            memory=self.memory,
            verbose=True
        )
        
        # 
        self.knowledge_base = self._init_knowledge_base()
    
    def chat(self, user_input):
        """"""
        # 
        relevant_docs = self.knowledge_base.similarity_search(user_input, k=3)
        context = "\n".join([doc.page_content for doc in relevant_docs])
        
        # prompt
        prompt = f"""


{context}

{user_input}

"""
        
        # 
        response = self.conversation.predict(input=prompt)
        
        return response
    
    def _init_knowledge_base(self):
        """"""
        documents = [
            "...",
            "...",
            "..."
        ]
        
        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_texts(documents, embeddings)
        
        return vectorstore

# Web
import gradio as gr

bot = ChatBot()

def respond(message, history):
    response = bot.chat(message)
    return response

demo = gr.ChatInterface(
    respond,
    title="",
    description=""
)

demo.launch()
```

## 3

### 



### 

```python
import pandas as pd
import matplotlib.pyplot as plt
from transformers import pipeline
import streamlit as st

class SentimentAnalysisSystem:
    """"""
    
    def __init__(self):
        self.analyzer = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english"
        )
    
    def analyze_text(self, text):
        """"""
        result = self.analyzer(text)[0]
        return {
            'text': text,
            'sentiment': result['label'],
            'score': result['score']
        }
    
    def analyze_batch(self, texts):
        """"""
        results = []
        for text in texts:
            result = self.analyze_text(text)
            results.append(result)
        return pd.DataFrame(results)
    
    def visualize(self, df):
        """"""
        sentiment_counts = df['sentiment'].value_counts()
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # 
        ax1.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')
        ax1.set_title('')
        
        # 
        ax2.bar(sentiment_counts.index, sentiment_counts.values)
        ax2.set_title('')
        ax2.set_ylabel('')
        
        return fig

# Streamlit
def main():
    st.title("")
    
    system = SentimentAnalysisSystem()
    
    # 
    st.header("")
    text = st.text_area("")
    if st.button(""):
        result = system.analyze_text(text)
        st.write(f": {result['sentiment']}")
        st.write(f": {result['score']:.2%}")
    
    # 
    st.header("")
    uploaded_file = st.file_uploader("CSV", type=['csv'])
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        texts = df['text'].tolist()
        
        if st.button(""):
            results = system.analyze_batch(texts)
            st.dataframe(results)
            
            # 
            fig = system.visualize(results)
            st.pyplot(fig)

if __name__ == '__main__':
    main()
```

## 4

### 



### 

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

class RecommenderModel(nn.Module):
    """"""
    
    def __init__(self, num_users, num_items, embedding_dim=50):
        super().__init__()
        
        # 
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # MLP
        self.fc = nn.Sequential(
            nn.Linear(embedding_dim * 2, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, user_ids, item_ids):
        # 
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        
        # 
        x = torch.cat([user_emb, item_emb], dim=-1)
        
        # 
        score = self.fc(x)
        
        return score

class RecommenderSystem:
    """"""
    
    def __init__(self, num_users, num_items):
        self.model = RecommenderModel(num_users, num_items)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        self.criterion = nn.BCELoss()
    
    def train(self, train_loader, epochs=10):
        """"""
        self.model.train()
        
        for epoch in range(epochs):
            total_loss = 0
            for user_ids, item_ids, ratings in train_loader:
                # 
                predictions = self.model(user_ids, item_ids).squeeze()
                loss = self.criterion(predictions, ratings.float())
                
                # 
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
            
            print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}")
    
    def recommend(self, user_id, top_k=10):
        """"""
        self.model.eval()
        
        # 
        item_ids = torch.arange(self.model.item_embedding.num_embeddings)
        user_ids = torch.full_like(item_ids, user_id)
        
        with torch.no_grad():
            scores = self.model(user_ids, item_ids).squeeze()
        
        # top-k
        top_items = torch.topk(scores, top_k).indices.tolist()
        
        return top_items
```

## 

### 

```python
# 
import torch.quantization as quantization

def quantize_model(model):
    """"""
    model.eval()
    model_quantized = quantization.quantize_dynamic(
        model,
        {nn.Linear},
        dtype=torch.qint8
    )
    return model_quantized

# ONNX
def export_to_onnx(model, dummy_input, output_path):
    """ONNX"""
    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=11,
        input_names=['input'],
        output_names=['output']
    )
```

### API

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class PredictionRequest(BaseModel):
    user_id: int
    top_k: int = 10

@app.post("/recommend")
async def recommend(request: PredictionRequest):
    """"""
    recommendations = recommender.recommend(
        request.user_id,
        request.top_k
    )
    return {'recommendations': recommendations}
```

## 

****:
1. 
2. 
3. Web
4. 

****:
- 
- 
- 
- 

<DocCardList />

