---
sidebar_position: 1
title: 
---

# 

:::info 

:::

## 

### 

860

****
- **Dendrites**
- **Cell Body**
- **Axon**
- **Synapse**

****
1. 
2. 
3. 
4. 

### 

1958Frank RosenblattPerceptron

```python
import numpy as np

class Perceptron:
    """"""
    
    def __init__(self, input_size, learning_rate=0.01):
        # 
        self.weights = np.random.randn(input_size)
        self.bias = np.random.randn()
        self.learning_rate = learning_rate
    
    def activation(self, x):
        """"""
        return 1 if x >= 0 else 0
    
    def predict(self, x):
        """"""
        # 
        z = np.dot(x, self.weights) + self.bias
        # 
        return self.activation(z)
    
    def train(self, X, y, epochs=100):
        """"""
        for epoch in range(epochs):
            errors = 0
            for xi, yi in zip(X, y):
                # 
                prediction = self.predict(xi)
                # 
                error = yi - prediction
                # 
                self.weights += self.learning_rate * error * xi
                self.bias += self.learning_rate * error
                errors += abs(error)
            
            if errors == 0:
                print(f" {epoch + 1} ")
                break

# AND
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 0, 1])

perceptron = Perceptron(input_size=2)
perceptron.train(X, y)

# 
for xi, yi in zip(X, y):
    pred = perceptron.predict(xi)
    print(f": {xi}, : {yi}, : {pred}")
```

****
- 
- XOR
- AI1969-1980s

## MLP



### 

```
 → 1 → 2 → ... → 
```

****
- ****
- ****
- ****

### 

```python
import numpy as np

class NeuralNetwork:
    """"""
    
    def __init__(self, layer_sizes):
        """
        layer_sizes: 
         [2, 4, 3, 1] 
        - 2
        - 4
        - 3
        - 1
        """
        self.layer_sizes = layer_sizes
        self.num_layers = len(layer_sizes)
        
        # 
        self.weights = []
        self.biases = []
        
        for i in range(self.num_layers - 1):
            # He
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])
            b = np.zeros((1, layer_sizes[i+1]))
            
            self.weights.append(w)
            self.biases.append(b)
    
    def sigmoid(self, x):
        """Sigmoid"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def sigmoid_derivative(self, x):
        """Sigmoid"""
        s = self.sigmoid(x)
        return s * (1 - s)
    
    def forward(self, X):
        """"""
        self.activations = [X]
        self.z_values = []
        
        for i in range(self.num_layers - 1):
            # : z = xW + b
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            
            # : a = σ(z)
            a = self.sigmoid(z)
            self.activations.append(a)
        
        return self.activations[-1]
    
    def compute_loss(self, y_true, y_pred):
        """"""
        return np.mean((y_true - y_pred) ** 2)

# 
nn = NeuralNetwork([2, 4, 1])

# 
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
output = nn.forward(X)
print(":", output)
```

### 



```python
import numpy as np
import matplotlib.pyplot as plt

# 
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

def elu(x, alpha=1.0):
    return np.where(x > 0, x, alpha * (np.exp(x) - 1))

def swish(x):
    return x * sigmoid(x)

# 
x = np.linspace(-5, 5, 100)

plt.figure(figsize=(15, 10))

functions = [
    ('Sigmoid', sigmoid),
    ('Tanh', tanh),
    ('ReLU', relu),
    ('Leaky ReLU', leaky_relu),
    ('ELU', elu),
    ('Swish', swish)
]

for i, (name, func) in enumerate(functions, 1):
    plt.subplot(2, 3, i)
    plt.plot(x, func(x), linewidth=2)
    plt.title(name)
    plt.grid(True)
    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
    plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)

plt.tight_layout()
plt.show()
```

****

|  |  |  |  |  |
|---------|------|------|------|---------|
| Sigmoid | $\frac{1}{1+e^{-x}}$ | (0,1) |  |  |
| Tanh | $\frac{e^x-e^{-x}}{e^x+e^{-x}}$ | (-1,1) |  | RNN |
| ReLU | $\max(0,x)$ |  |  |  |
| Leaky ReLU | $\max(\alpha x, x)$ |  |  |  |
| ELU | $x$ if $x>0$ else $\alpha(e^x-1)$ |  |  |  |
| Swish | $x \cdot \sigma(x)$ |  |  |  |

### 



```python
class NeuralNetwork:
    # ... ()
    
    def backward(self, X, y, learning_rate=0.01):
        """"""
        m = X.shape[0]  # 
        
        # 
        delta = self.activations[-1] - y
        
        # 
        for i in range(self.num_layers - 2, -1, -1):
            # 
            dW = np.dot(self.activations[i].T, delta) / m
            db = np.sum(delta, axis=0, keepdims=True) / m
            
            # 
            self.weights[i] -= learning_rate * dW
            self.biases[i] -= learning_rate * db
            
            # 
            if i > 0:
                # 
                delta = np.dot(delta, self.weights[i].T) * self.sigmoid_derivative(self.z_values[i-1])
    
    def train(self, X, y, epochs=1000, learning_rate=0.1, verbose=True):
        """"""
        losses = []
        
        for epoch in range(epochs):
            # 
            y_pred = self.forward(X)
            
            # 
            loss = self.compute_loss(y, y_pred)
            losses.append(loss)
            
            # 
            self.backward(X, y, learning_rate)
            
            # 
            if verbose and (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}")
        
        return losses

# XOR
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

nn = NeuralNetwork([2, 4, 1])
losses = nn.train(X, y, epochs=5000, learning_rate=0.5)

# 
print("\n:")
predictions = nn.forward(X)
for i in range(len(X)):
    print(f": {X[i]}, : {y[i][0]}, : {predictions[i][0]:.4f}")

# 
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.grid(True)
plt.show()
```

### 

**BGD**
```python
# 
for epoch in range(epochs):
    gradient = compute_gradient(X, y)
    weights -= learning_rate * gradient
```

**SGD**
```python
# 
for epoch in range(epochs):
    for i in range(len(X)):
        gradient = compute_gradient(X[i], y[i])
        weights -= learning_rate * gradient
```

**Mini-batch GD**
```python
# 
batch_size = 32
for epoch in range(epochs):
    for i in range(0, len(X), batch_size):
        X_batch = X[i:i+batch_size]
        y_batch = y[i:i+batch_size]
        gradient = compute_gradient(X_batch, y_batch)
        weights -= learning_rate * gradient
```

**Momentum**
```python
velocity = 0
momentum = 0.9

for epoch in range(epochs):
    gradient = compute_gradient(X, y)
    velocity = momentum * velocity - learning_rate * gradient
    weights += velocity
```

**Adam**
```python
class Adam:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.learning_rate = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = None  # 
        self.v = None  # 
        self.t = 0     # 
    
    def update(self, weights, gradients):
        if self.m is None:
            self.m = np.zeros_like(weights)
            self.v = np.zeros_like(weights)
        
        self.t += 1
        
        # 
        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients
        self.v = self.beta2 * self.v + (1 - self.beta2) * (gradients ** 2)
        
        # 
        m_hat = self.m / (1 - self.beta1 ** self.t)
        v_hat = self.v / (1 - self.beta2 ** self.t)
        
        # 
        weights -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)
        
        return weights
```

## 

### 

****
- 
- 

****
- 
- 
- 

### 

**1. /**

```python
# 
def demonstrate_vanishing_gradient():
    # 100.5
    gradient = 1.0
    for layer in range(10):
        gradient *= 0.5
        print(f"{layer+1}: {gradient:.6f}")
    
    # 0

demonstrate_vanishing_gradient()
```

****
- ReLU
- Batch Normalization
- Residual Connection
- Gradient Clipping

**2. **

```python
# Dropout
class Dropout:
    def __init__(self, dropout_rate=0.5):
        self.dropout_rate = dropout_rate
        self.mask = None
    
    def forward(self, x, training=True):
        if training:
            # 
            self.mask = np.random.binomial(1, 1-self.dropout_rate, size=x.shape)
            return x * self.mask / (1 - self.dropout_rate)
        else:
            # 
            return x
    
    def backward(self, dout):
        return dout * self.mask / (1 - self.dropout_rate)
```

**3. **

**Batch Normalization**
```python
class BatchNormalization:
    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):
        self.epsilon = epsilon
        self.momentum = momentum
        
        # 
        self.gamma = np.ones(num_features)
        self.beta = np.zeros(num_features)
        
        # 
        self.running_mean = np.zeros(num_features)
        self.running_var = np.ones(num_features)
    
    def forward(self, x, training=True):
        if training:
            # 
            batch_mean = np.mean(x, axis=0)
            batch_var = np.var(x, axis=0)
            
            # 
            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var
            
            # 
            x_normalized = (x - batch_mean) / np.sqrt(batch_var + self.epsilon)
        else:
            # 
            x_normalized = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)
        
        # 
        out = self.gamma * x_normalized + self.beta
        return out
```

## PyTorch

```python
import torch
import torch.nn as nn
import torch.optim as optim

class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNN, self).__init__()
        
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.bn1 = nn.BatchNorm1d(hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# 
model = SimpleNN(input_size=10, hidden_size=20, output_size=2)

# 
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 
def train(model, X, y, epochs=100):
    model.train()
    for epoch in range(epochs):
        # 
        outputs = model(X)
        loss = criterion(outputs, y)
        
        # 
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# 
X = torch.randn(100, 10)
y = torch.randint(0, 2, (100,))

train(model, X, y)
```

## 

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# 
class MNISTNet(nn.Module):
    def __init__(self):
        super(MNISTNet, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        x = x.view(-1, 28*28)  # 
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

# 
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

# 
model = MNISTNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

def train_epoch(model, train_loader, criterion, optimizer):
    model.train()
    total_loss = 0
    correct = 0
    
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        pred = output.argmax(dim=1)
        correct += pred.eq(target).sum().item()
    
    return total_loss / len(train_loader), correct / len(train_loader.dataset)

def test(model, test_loader):
    model.eval()
    correct = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
    
    return correct / len(test_loader.dataset)

# 
for epoch in range(10):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)
    test_acc = test(model, test_loader)
    print(f'Epoch {epoch+1}: Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Test Acc={test_acc:.4f}')
```

## 

****
1. 
2. 
3. 
4. 
5. 

****
- ReLU
- Adam
- Batch Normalization
- Dropout
- 

****
- CNN
- RNN
- Transformer
- 

<DocCardList />

