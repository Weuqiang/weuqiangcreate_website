---
sidebar_position: 5
title: Hive - SQL on Hadoop
---

# Hive - SQL

HiveSQLHDFS**Hive**

## Hive

### Hive

****HDFSTBMapReduce

****
```python
# MapReduce - 
class Mapper:
    def map(self, key, value):
        # 100...
```

**Hive**
```sql
-- SQL - 
SELECT category, COUNT(*) as count
FROM products
GROUP BY category;
```

### Hive

```

          Client            
      JDBC/ODBC/CLI                  

              ↓

          Hive Server                
      SQL           

              ↓

          Metastore                  
                  

              ↓

          Execution Engine           
      MapReduce/Spark/Tez            

              ↓

          HDFS                       
                          

```

## 

### DockerHive

```yaml
# docker-compose.yml
version: '3'
services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    environment:
      - CLUSTER_NAME=test
    ports:
      - "9870:9870"
      - "9000:9000"

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    environment:
      - HIVE_CORE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore/metastore
    ports:
      - "10000:10000"
    depends_on:
      - hive-metastore

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    environment:
      - SERVICE_PRECONDITION=namenode:9870 datanode:9864 hive-metastore-postgresql:5432
    ports:
      - "9083:9083"

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
```

```bash
# 
docker-compose up -d

# Hive
docker exec -it hive-server bash
hive
```

### 

```sql
-- 
CREATE DATABASE IF NOT EXISTS mydb;

-- 
USE mydb;

-- 
SHOW DATABASES;

-- 
DROP DATABASE IF EXISTS mydb CASCADE;
```

## 

### 

```sql
-- 
CREATE TABLE users (
    user_id INT,
    name STRING,
    age INT,
    city STRING,
    register_date DATE
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

-- 
CREATE EXTERNAL TABLE logs (
    timestamp STRING,
    level STRING,
    message STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/user/logs/';

-- 
CREATE TABLE orders (
    order_id STRING,
    user_id STRING,
    amount DOUBLE,
    status STRING
)
PARTITIONED BY (date STRING, hour STRING)
STORED AS PARQUET;

-- 
CREATE TABLE products (
    product_id STRING,
    name STRING,
    price DOUBLE,
    category STRING
)
CLUSTERED BY (product_id) INTO 10 BUCKETS
STORED AS ORC;
```

### 

```sql
-- 
LOAD DATA LOCAL INPATH '/tmp/users.csv' 
INTO TABLE users;

-- HDFS
LOAD DATA INPATH '/user/data/users.csv' 
INTO TABLE users;

-- 
INSERT INTO TABLE users 
VALUES (1, 'Alice', 25, 'Beijing', '2024-01-01');

-- 
INSERT INTO TABLE users
SELECT user_id, name, age, city, register_date
FROM temp_users
WHERE age > 18;

-- 
INSERT OVERWRITE TABLE users
SELECT * FROM temp_users;

-- 
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;

INSERT INTO TABLE orders PARTITION(date, hour)
SELECT order_id, user_id, amount, status, 
       DATE(order_time) as date,
       HOUR(order_time) as hour
FROM raw_orders;
```

## 

### 

```sql
-- 
SELECT * FROM users LIMIT 10;

-- 
SELECT name, age, city
FROM users
WHERE age > 25 AND city = 'Beijing';

-- 
SELECT name, age
FROM users
ORDER BY age DESC
LIMIT 10;

-- 
SELECT city, COUNT(*) as user_count, AVG(age) as avg_age
FROM users
GROUP BY city
HAVING user_count > 100
ORDER BY user_count DESC;

-- 
SELECT DISTINCT city FROM users;
```

### 

```sql
-- 
SELECT u.name, o.order_id, o.amount
FROM users u
JOIN orders o ON u.user_id = o.user_id;

-- 
SELECT u.name, o.order_id, o.amount
FROM users u
LEFT JOIN orders o ON u.user_id = o.user_id;

-- 
SELECT u.name, o.order_id, p.product_name, od.quantity
FROM users u
JOIN orders o ON u.user_id = o.user_id
JOIN order_details od ON o.order_id = od.order_id
JOIN products p ON od.product_id = p.product_id;

-- Map Join
SELECT /*+ MAPJOIN(small_table) */
    a.*, b.*
FROM large_table a
JOIN small_table b ON a.key = b.key;
```

### 

```sql
-- WHERE
SELECT name, age
FROM users
WHERE user_id IN (
    SELECT user_id 
    FROM orders 
    WHERE amount > 1000
);

-- FROM
SELECT city, avg_age
FROM (
    SELECT city, AVG(age) as avg_age
    FROM users
    GROUP BY city
) t
WHERE avg_age > 30;

-- WITHCTE
WITH high_value_users AS (
    SELECT user_id, SUM(amount) as total_amount
    FROM orders
    GROUP BY user_id
    HAVING total_amount > 10000
)
SELECT u.name, h.total_amount
FROM users u
JOIN high_value_users h ON u.user_id = h.user_id;
```

### 

```sql
-- ROW_NUMBER
SELECT 
    name,
    city,
    age,
    ROW_NUMBER() OVER (PARTITION BY city ORDER BY age DESC) as rank
FROM users;

-- RANK
SELECT 
    product_name,
    category,
    sales,
    RANK() OVER (PARTITION BY category ORDER BY sales DESC) as rank
FROM products;

-- LAG/LEAD
SELECT 
    date,
    sales,
    LAG(sales, 1) OVER (ORDER BY date) as prev_sales,
    LEAD(sales, 1) OVER (ORDER BY date) as next_sales
FROM daily_sales;

-- 
SELECT 
    date,
    amount,
    SUM(amount) OVER (ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as cumsum
FROM orders;

-- 
SELECT 
    date,
    value,
    AVG(value) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as ma7
FROM metrics;
```

## 

### 

```sql
-- 
CREATE TABLE logs (
    timestamp STRING,
    level STRING,
    message STRING
)
PARTITIONED BY (date STRING, hour STRING)
STORED AS PARQUET;

-- 
ALTER TABLE logs ADD PARTITION (date='2024-01-01', hour='00');

-- 
SHOW PARTITIONS logs;

-- 
ALTER TABLE logs DROP PARTITION (date='2024-01-01', hour='00');

-- 
SELECT * FROM logs
WHERE date='2024-01-01' AND hour='00';

-- 
SELECT COUNT(*) FROM logs
WHERE date BETWEEN '2024-01-01' AND '2024-01-07';
```

### 

```sql
-- 
CREATE TABLE users_bucketed (
    user_id INT,
    name STRING,
    age INT
)
CLUSTERED BY (user_id) INTO 10 BUCKETS
STORED AS ORC;

-- 
SET hive.enforce.bucketing=true;

INSERT INTO TABLE users_bucketed
SELECT user_id, name, age FROM users;

-- 
SELECT * FROM users_bucketed
TABLESAMPLE(BUCKET 1 OUT OF 10 ON user_id);
```

## 

### 1. 

```sql
-- Parquet
CREATE TABLE data_parquet (
    id INT,
    name STRING,
    value DOUBLE
)
STORED AS PARQUET;

-- ORC
CREATE TABLE data_orc (
    id INT,
    name STRING,
    value DOUBLE
)
STORED AS ORC
TBLPROPERTIES ("orc.compress"="SNAPPY");

-- 
-- TextFile: 1GB
-- Parquet: 200MB (5x)
-- ORC: 150MB (6.7x)
```

### 2. 

```sql
-- TezMapReduce3-10
SET hive.execution.engine=tez;

-- Spark
SET hive.execution.engine=spark;
```

### 3. 

```sql
-- 
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.reduce.enabled=true;
```

### 4. CBO

```sql
-- 
ANALYZE TABLE users COMPUTE STATISTICS;
ANALYZE TABLE users COMPUTE STATISTICS FOR COLUMNS;

-- CBO
SET hive.cbo.enable=true;
SET hive.compute.query.using.stats=true;
```

## 

### 

```sql
-- 1. 

-- 
CREATE TABLE users (
    user_id STRING,
    name STRING,
    age INT,
    gender STRING,
    city STRING,
    register_date DATE
)
STORED AS PARQUET;

-- 
CREATE TABLE products (
    product_id STRING,
    name STRING,
    category STRING,
    price DOUBLE
)
STORED AS PARQUET;

-- 
CREATE TABLE orders (
    order_id STRING,
    user_id STRING,
    product_id STRING,
    quantity INT,
    amount DOUBLE,
    order_time TIMESTAMP
)
PARTITIONED BY (date STRING)
STORED AS PARQUET;

-- 2. 

-- 
SELECT 
    date,
    COUNT(DISTINCT order_id) as order_count,
    COUNT(DISTINCT user_id) as user_count,
    SUM(amount) as total_amount,
    AVG(amount) as avg_amount
FROM orders
GROUP BY date
ORDER BY date;

-- 
SELECT 
    p.name,
    p.category,
    COUNT(o.order_id) as order_count,
    SUM(o.quantity) as total_quantity,
    SUM(o.amount) as total_amount
FROM orders o
JOIN products p ON o.product_id = p.product_id
WHERE o.date >= '2024-01-01'
GROUP BY p.name, p.category
ORDER BY total_amount DESC
LIMIT 20;

-- RFM
WITH user_rfm AS (
    SELECT 
        user_id,
        DATEDIFF(CURRENT_DATE, MAX(order_time)) as recency,
        COUNT(order_id) as frequency,
        SUM(amount) as monetary
    FROM orders
    GROUP BY user_id
)
SELECT 
    user_id,
    recency,
    frequency,
    monetary,
    CASE 
        WHEN recency <= 30 AND frequency >= 10 AND monetary >= 10000 THEN ''
        WHEN recency <= 90 AND frequency >= 5 AND monetary >= 5000 THEN ''
        ELSE ''
    END as user_level
FROM user_rfm;

-- 
WITH first_order AS (
    SELECT 
        user_id,
        MIN(date) as first_date
    FROM orders
    GROUP BY user_id
),
retention AS (
    SELECT 
        f.first_date,
        DATEDIFF(o.date, f.first_date) as days_since_first,
        COUNT(DISTINCT o.user_id) as retained_users
    FROM first_order f
    JOIN orders o ON f.user_id = o.user_id
    GROUP BY f.first_date, DATEDIFF(o.date, f.first_date)
)
SELECT 
    first_date,
    days_since_first,
    retained_users,
    retained_users * 100.0 / FIRST_VALUE(retained_users) OVER (PARTITION BY first_date ORDER BY days_since_first) as retention_rate
FROM retention
ORDER BY first_date, days_since_first;

-- 
SELECT 
    a.product_id as product_a,
    b.product_id as product_b,
    COUNT(*) as co_purchase_count
FROM (
    SELECT order_id, product_id FROM orders
) a
JOIN (
    SELECT order_id, product_id FROM orders
) b ON a.order_id = b.order_id AND a.product_id < b.product_id
GROUP BY a.product_id, b.product_id
HAVING co_purchase_count >= 10
ORDER BY co_purchase_count DESC
LIMIT 50;
```

### 

```sql
-- 
CREATE TABLE user_behavior (
    user_id STRING,
    action STRING,  -- view, click, cart, purchase
    item_id STRING,
    timestamp TIMESTAMP
)
PARTITIONED BY (date STRING)
STORED AS PARQUET;

-- 
WITH funnel AS (
    SELECT 
        date,
        COUNT(DISTINCT CASE WHEN action = 'view' THEN user_id END) as view_users,
        COUNT(DISTINCT CASE WHEN action = 'click' THEN user_id END) as click_users,
        COUNT(DISTINCT CASE WHEN action = 'cart' THEN user_id END) as cart_users,
        COUNT(DISTINCT CASE WHEN action = 'purchase' THEN user_id END) as purchase_users
    FROM user_behavior
    GROUP BY date
)
SELECT 
    date,
    view_users,
    click_users,
    cart_users,
    purchase_users,
    click_users * 100.0 / view_users as view_to_click_rate,
    cart_users * 100.0 / click_users as click_to_cart_rate,
    purchase_users * 100.0 / cart_users as cart_to_purchase_rate
FROM funnel
ORDER BY date;
```

## UDF

### Python UDF

```python
# my_udf.py
import sys

# 
for line in sys.stdin:
    line = line.strip()
    # 
    result = line.upper()
    print(result)
```

```sql
-- 
ADD FILE /path/to/my_udf.py;

-- UDF
SELECT TRANSFORM(name)
USING 'python my_udf.py'
AS upper_name
FROM users;
```

### Java UDF

```java
// MyUDF.java
import org.apache.hadoop.hive.ql.exec.UDF;

public class MyUDF extends UDF {
    public String evaluate(String input) {
        if (input == null) return null;
        return input.toUpperCase();
    }
}
```

```sql
-- 
CREATE FUNCTION my_upper AS 'com.example.MyUDF'
USING JAR 'hdfs:///user/hive/lib/my-udf.jar';

-- 
SELECT my_upper(name) FROM users;
```

## 

Hive

1. **SQL**MapReduce
2. ****
3. ****ParquetORC
4. ****UDF

**Hive**

## 

1. 
2. 
3. 
4. UDF

HBase

