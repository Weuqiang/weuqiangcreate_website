---
sidebar_position: 7
title: 
---

## 

AI ML AI ——

 ML  →  →  → 



```python showLineNumbers
# 
target_number = 17.0

# 
weight = 1

# 
learning_rate = 0.01  # 
epochs = 1000        # 

# 
for epoch in range(epochs):
    # :  (weight * weight  target_number)
    prediction = weight * weight
    
    # :  (MSE)
    loss = (prediction - target_number) ** 2

    # : lossweight
    gradient = 2 * (prediction - target_number) * 2 * weight
    
    #  ()
    weight = weight - learning_rate * gradient

# 
print(f"\n: {weight}")
print(f": {target_number - prediction}")
```



1. ****
2. ****
3. ****
4. ****
5. ****



<Highlight></Highlight><HoverText text="“All models are wrong, but some are useful.”" explanation="——·"/>

<Highlight></Highlight>

<Highlight></Highlight>

## PyTorch

PyTorch 

YOLOv11Hugging Face TransformersStable Diffusion  PyTorch PyTorch 

PyTorch  GPU/NPU  CUDA PyTorch  CUDA 

:::info

**CUDA**NVIDIA  GPU  C++  NVIDIA  GPU GPUCUDAGPU

**cuDNN**cuDNN “” CUDA GPU“” CUDA ——cuDNN2

**CUDA Toolkit (NVIDIA )** CUDA 
- NVIDIA 
- **NVCC**NVIDIA CUDA  CUDA Toolkit  CUDA  NVIDIA GPU 
-  CUDA IDE
-  CUDA 
- 

**CUDA Toolkit (PyTorch )** CUDA 
-  CUDA 
- 
-  PyTorch  CUDA 
:::

WSL2  CUDA  3090 TiWindows  WSL2 

```bash showLineNumbers
# Windows
name: NVIDIA GeForce RTX 3090 Ti
write speed: 5063.91 MB/s
read speed: 5565.53 MB/s

# WSL2 Ubuntu-24.04
name: NVIDIA GeForce RTX 3090 Ti
write speed: 2632.96 MB/s  # 
read speed: 4429.29 MB/s   # 
```

 Windows  Linux 
### torch

<MarkmapHooks initialMarkdown={`

# torch

## 

### torch

- torchPyTorchSigmoidtorch.sigmoid​ReLUtorch.reluTanhtorch.tanh​PyTorchtorch.mm​torch.select​
- PyTorchTypeError​

### torch.Tensor

- torch.Tensortorch

### torch.sparse

- torch.sparseCOOCoordinate​

### torch.cuda

- torch.cudaCUDACUDAGPUGPU​GPUGPUStream​GPUKernel

### **torch.nn**

- torch.nnPyTorchnn.ConvNdN=123nn.Linearnn.Moduleforward​torch.nntorch.nn.MSELoss​torch.nn.CrossEntropyLoss

### torch.nn.functional

- torch.nn.functionalPyTorchtorch.nntorch.nn.functionalnn.ConvNdN=123torch.nn.functional.convNdN=123​torch.nn.functionaltorch.nn.functional.relu6torch.nn.functional.elu

### torch.nn.init

- torch.nn.inittorch.nn.init.uniform_torch.nn.init.normal_

### **torch.optim**

- torch.optimtorch.optim.SGD​torch.optim.AdagradAdaGrad​torch.optim.RMSpropRMSProptorch.optim.AdamAdamtorch.optim.lr_schedulertorch.optim.lr_scheduler.StepLRtorch.optim.lr_scheduler.CosineAnnealingLR

### torch.autograd

- torch.autogradPyTorchtorch.autograd.backwardtorch.autograd.grad

### ~torch.distributed~

- torch.distributedPyTorchPyTorchMPIGlooNCCLPyTorchReduce​BroadcastGather​​​PyTorchTCP​

### ~torch.distributions~

- torch.distributionsPyTorchReinforcement Learning​Policy​Policy Gradient​torch.distributionstorch.distributions.Categoricaltorch.distributions.CategoricalPyTorch​torch.distributions.Normal

### torch.hub

- torch.hubtorch.hub.listtorch.hub.loadtorch.hubPyTorch

### torch.jit

- torch.jitPyTorchJust-In-Time CompilerJITPyTorchIntermediate Representation​JITPyTorchC++JITONNXtorch.jitScriptModuleTracing​torch.nn.GRUPyTorchJIT​

### torch.multiprocessing

- torch.multiprocessingPyTorchAPI​CPUGPUAPIPythonAPImultiprocessingLockQueue

### **torch.random**

- torch.randomget_rng_stateset_rng_statemanual_seedinitial_seed

### torch.onnx

- torch.onnxPyTorchONNXONNXPyTorchPyTorch

## 

### torch.utils.bottleneck

- torch.utils.bottleneck

### torch.utils.checkpoint

- torch.utils.checkpoint

### torch.utils.cpp_extension

- torch.utils.cpp_extensionPyTorchC++CppExtensionC++CUDAExtensionC++/CUDAC++

### torch.utils.data

- torch.utils.dataDatasetDataLoaderShuffleSample​

### torch.utils.dlpack

- torch.utils.dlpackPyTorchDLPack

### torch.utils.tensorboard

- torch.utils.tensorboardPyTorchTensorBoardTensorBoardTensorFlowTensorBoard​PyTorchTensorBoardPyTorch

`} />



### torchvision

torchvision  PyTorch 



`pip install torchvision`

|     |                          |  | (\*) |
| ------------- | -------------------------------- | -------- | ------------------------ |
| MNIST         | torchvision.datasets.MNIST       |      | 70000\*784               |
| CIFAR-10      | torchvision.datasets.CIFAR10    |      | 60000\*3072              |
| Fashion MNIST | torchvision.datasets.FashionMNIST|      | 70000\*784               |

```python showLineNumbers
import torchvision.datasets as datasets
# 
train_dataset = datasets.MNIST(root='./data', train=True, download=True)
test_dataset = datasets.MNIST(root='./data', train=False, download=True)
```

### torchtext

//, NLP IMDB 

`pip install torchtext`

```python showLineNumbers
  # 
  from torchtext.data import Field, TabularDataset
  
  TEXT = Field(tokenize='spacy')
  LABEL = Field(sequential=False)
  train_data = TabularDataset(path='train.csv', format='csv', 
                             fields=[('text', TEXT), ('label', LABEL)])
``` 


### Ignite 

//

`pip install pytorch-ignite`

```python showLineNumbers
from ignite.engine import Events, create_supervised_trainer

trainer = create_supervised_trainer(model, optimizer, loss_fn)

@trainer.on(Events.ITERATION_COMPLETED(every=100))
def log_loss(engine):
    print(f"Epoch {engine.state.epoch}, Loss: {engine.state.output:.2f}")
```

## 

### 




<MarkmapHooks initialMarkdown={`
# 

## 

### 
- 

### 
- 

### Best KS
- 

### 
- 

## /

### 
- SVM

### 
- KNN

### 
- 

## 

### 
- 

### 
- XGBoostLightGBM

### 

#### 
- 

#### 
- 

#### 
- 

### 
- 

### 

#### 
- /

#### 
- -

### 
- 

### 
- 

## 

### 3-Sigma
- 

### BOX-COX
- 

### 
- 
`} />


### 

BMIAI

$$
BMI = \text{}_{\text{kg}} / (\text{}_{\text{m}})^2
$$

<MarkmapHooks initialMarkdown={`
# 

## 

### 

- 
- 
- 

### 

- log//
- 

### 

- 

## 

### filter

- 

#### 

- Relief
- 
- 
- 
- 

### wrapper

- 

#### 

- LVMLas Vegas Wrapper

### embedded

- 

#### 

- Lasso

## 

- PCA
- LDA
- LCA
- 
`} />

### 



<MarkmapHooks initialMarkdown={`
# 

## 

- Lasso
- Ridge
- 

## 

- 
- 

## 

- 
- 
- 

## 

- 
- 
- 

## 

### 

- 
- 

### 

- 
- 
- 

### 

- 
- 
`} />

### 



<MarkmapHooks initialMarkdown={`
# 
## 
### 
#### 
#### 
### 
#### 
#### 
### 
#### log
## boosting/bagging
### XGBoost, Adaboost, GBDT
## stacking/blending
### 
`} />


<DocCardList />