---
sidebar_position: 11
title: AI
---

# AIMultimodal AI

:::info 
AI-CLIPGPT-4V
:::

## AI

**AI** 

### 

****
- 
- 

****
- 
- 
- 

****
- 
- 
- 

## AI

### 2015-2019

**Image Captioning**
```python
# CNN+RNN
class ImageCaptioning(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size):
        super().__init__()
        # CNNResNet
        self.encoder = models.resnet50(pretrained=True)
        self.encoder.fc = nn.Linear(2048, embed_size)
        
        # LSTM
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)
    
    def forward(self, images, captions):
        # 
        features = self.encoder(images).unsqueeze(1)
        
        # 
        embeddings = self.embedding(captions)
        
        # 
        inputs = torch.cat([features, embeddings], dim=1)
        
        # LSTM
        hiddens, _ = self.lstm(inputs)
        outputs = self.fc(hiddens)
        
        return outputs
```

**VQA**
- 
- 

### CLIP2021

**CLIPContrastive Language-Image Pre-training**

OpenAI2021CLIP

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class CLIP(nn.Module):
    """CLIP"""
    
    def __init__(self, image_encoder, text_encoder, embed_dim=512):
        super().__init__()
        self.image_encoder = image_encoder
        self.text_encoder = text_encoder
        
        # 
        self.image_projection = nn.Linear(image_encoder.output_dim, embed_dim)
        self.text_projection = nn.Linear(text_encoder.output_dim, embed_dim)
        
        # 
        self.temperature = nn.Parameter(torch.ones([]) * 0.07)
    
    def encode_image(self, images):
        """"""
        features = self.image_encoder(images)
        embeddings = self.image_projection(features)
        # L2
        embeddings = F.normalize(embeddings, dim=-1)
        return embeddings
    
    def encode_text(self, texts):
        """"""
        features = self.text_encoder(texts)
        embeddings = self.text_projection(features)
        # L2
        embeddings = F.normalize(embeddings, dim=-1)
        return embeddings
    
    def forward(self, images, texts):
        # 
        image_embeddings = self.encode_image(images)
        text_embeddings = self.encode_text(texts)
        
        # 
        logits = (image_embeddings @ text_embeddings.T) / self.temperature
        
        return logits
    
    def contrastive_loss(self, logits):
        """"""
        batch_size = logits.shape[0]
        labels = torch.arange(batch_size, device=logits.device)
        
        # 
        loss_i2t = F.cross_entropy(logits, labels)
        
        # 
        loss_t2i = F.cross_entropy(logits.T, labels)
        
        # 
        loss = (loss_i2t + loss_t2i) / 2
        
        return loss

# CLIP
def zero_shot_classification(model, image, class_names):
    """"""
    # 
    image_features = model.encode_image(image)
    
    # 
    text_prompts = [f"a photo of a {name}" for name in class_names]
    text_features = model.encode_text(text_prompts)
    
    # 
    similarities = (image_features @ text_features.T).softmax(dim=-1)
    
    # 
    pred_idx = similarities.argmax().item()
    return class_names[pred_idx], similarities[0, pred_idx].item()
```

**CLIP**
- 
- 
- DALL-E
- 

### 2022-

**FlamingoDeepMind, 2022**
- Few-shot
- 

**GPT-4VOpenAI, 2023**
- 
- 
- OCR

**GeminiGoogle, 2023**
- 
- 

## 

### 1. -

**Contrastive Learning**

```python
def clip_loss(image_embeddings, text_embeddings, temperature=0.07):
    """
    CLIP
    
    Args:
        image_embeddings: (batch_size, embed_dim)
        text_embeddings: (batch_size, embed_dim)
    """
    # 
    image_embeddings = F.normalize(image_embeddings, dim=-1)
    text_embeddings = F.normalize(text_embeddings, dim=-1)
    
    # 
    logits = torch.matmul(image_embeddings, text_embeddings.T) / temperature
    
    # 
    batch_size = image_embeddings.shape[0]
    labels = torch.arange(batch_size, device=logits.device)
    
    # 
    loss_i2t = F.cross_entropy(logits, labels)
    loss_t2i = F.cross_entropy(logits.T, labels)
    
    loss = (loss_i2t + loss_t2i) / 2
    
    return loss
```

**Masked Modeling**

```python
def masked_image_modeling(model, images, mask_ratio=0.75):
    """
    MAE
    """
    batch_size, channels, height, width = images.shape
    
    # patches
    patch_size = 16
    num_patches = (height // patch_size) * (width // patch_size)
    
    # mask
    num_masked = int(mask_ratio * num_patches)
    mask_indices = torch.randperm(num_patches)[:num_masked]
    
    # 
    encoded = model.encode(images, mask_indices)
    
    # 
    reconstructed = model.decode(encoded)
    
    # mask
    loss = F.mse_loss(reconstructed[:, mask_indices], images[:, mask_indices])
    
    return loss
```

### 2. 

```python
class CrossModalAttention(nn.Module):
    """"""
    
    def __init__(self, dim, num_heads=8):
        super().__init__()
        self.num_heads = num_heads
        self.scale = (dim // num_heads) ** -0.5
        
        self.q_proj = nn.Linear(dim, dim)
        self.k_proj = nn.Linear(dim, dim)
        self.v_proj = nn.Linear(dim, dim)
        self.out_proj = nn.Linear(dim, dim)
    
    def forward(self, query_modality, key_value_modality):
        """
        Args:
            query_modality: (batch, seq_len_q, dim) - 
            key_value_modality: (batch, seq_len_kv, dim) - 
        """
        batch_size = query_modality.shape[0]
        
        # 
        Q = self.q_proj(query_modality)
        K = self.k_proj(key_value_modality)
        V = self.v_proj(key_value_modality)
        
        # 
        Q = Q.view(batch_size, -1, self.num_heads, -1).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, -1).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, -1).transpose(1, 2)
        
        # 
        attn = (Q @ K.transpose(-2, -1)) * self.scale
        attn = F.softmax(attn, dim=-1)
        
        # 
        out = attn @ V
        out = out.transpose(1, 2).contiguous().view(batch_size, -1, -1)
        out = self.out_proj(out)
        
        return out


class MultimodalTransformer(nn.Module):
    """Transformer"""
    
    def __init__(self, dim, num_heads, num_layers):
        super().__init__()
        
        self.layers = nn.ModuleList([
            nn.ModuleDict({
                'self_attn': nn.MultiheadAttention(dim, num_heads),
                'cross_attn': CrossModalAttention(dim, num_heads),
                'ffn': nn.Sequential(
                    nn.Linear(dim, dim * 4),
                    nn.GELU(),
                    nn.Linear(dim * 4, dim)
                ),
                'norm1': nn.LayerNorm(dim),
                'norm2': nn.LayerNorm(dim),
                'norm3': nn.LayerNorm(dim),
            })
            for _ in range(num_layers)
        ])
    
    def forward(self, text_features, image_features):
        """
        Args:
            text_features: (batch, text_len, dim)
            image_features: (batch, image_len, dim)
        """
        for layer in self.layers:
            # 
            text_features = text_features + layer['self_attn'](
                layer['norm1'](text_features),
                layer['norm1'](text_features),
                layer['norm1'](text_features)
            )[0]
            
            # 
            text_features = text_features + layer['cross_attn'](
                layer['norm2'](text_features),
                layer['norm2'](image_features)
            )
            
            # 
            text_features = text_features + layer['ffn'](layer['norm3'](text_features))
        
        return text_features
```

### 3. 

**Early Fusion**
```python
def early_fusion(image_features, text_features):
    """"""
    # 
    combined = torch.cat([image_features, text_features], dim=1)
    # 
    fused_features = encoder(combined)
    return fused_features
```

**Late Fusion**
```python
def late_fusion(image_features, text_features):
    """"""
    # 
    image_encoded = image_encoder(image_features)
    text_encoded = text_encoder(text_features)
    
    # 
    fused = torch.cat([image_encoded, text_encoded], dim=-1)
    # 
    # fused = alpha * image_encoded + (1 - alpha) * text_encoded
    
    return fused
```

**Hybrid Fusion**
```python
def hybrid_fusion(image_features, text_features):
    """"""
    # 
    early_fused = early_fusion(image_features, text_features)
    
    # 
    image_mid = image_encoder.mid_layers(image_features)
    text_mid = text_encoder.mid_layers(text_features)
    mid_fused = cross_attention(image_mid, text_mid)
    
    # 
    late_fused = late_fusion(image_mid, text_mid)
    
    # 
    final = torch.cat([early_fused, mid_fused, late_fused], dim=-1)
    return final
```

## 

### CLIPOpenAI

```python
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import requests

# 
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# 
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 
inputs = processor(
    text=["a photo of a cat", "a photo of a dog"],
    images=image,
    return_tensors="pt",
    padding=True
)

# 
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image
probs = logits_per_image.softmax(dim=1)

print(f": {probs}")
```

### BLIPSalesforce

```python
from transformers import BlipProcessor, BlipForConditionalGeneration

# 
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# 
inputs = processor(image, return_tensors="pt")
out = model.generate(**inputs)
caption = processor.decode(out[0], skip_special_tokens=True)
print(f": {caption}")

# 
question = "What is in the image?"
inputs = processor(image, question, return_tensors="pt")
out = model.generate(**inputs)
answer = processor.decode(out[0], skip_special_tokens=True)
print(f": {answer}")
```

### LLaVA

```python
from llava.model import LlavaLlamaForCausalLM
from llava.conversation import conv_templates

# 
model = LlavaLlamaForCausalLM.from_pretrained("liuhaotian/llava-v1.5-7b")

# 
conv = conv_templates["llava_v1"].copy()
conv.append_message(conv.roles[0], "What is shown in this image?")
conv.append_message(conv.roles[1], None)

# 
prompt = conv.get_prompt()
inputs = tokenizer([prompt], return_tensors="pt")
output = model.generate(**inputs, images=image_tensor)
response = tokenizer.decode(output[0])

print(response)
```

### GPT-4VOpenAI

```python
from openai import OpenAI

client = OpenAI()

# 
response = client.chat.completions.create(
    model="gpt-4-vision-preview",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": ""},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://example.com/image.jpg",
                    },
                },
            ],
        }
    ],
    max_tokens=300,
)

print(response.choices[0].message.content)
```

## 

### 1. 

```python
class ImageCaptioningSystem:
    """"""
    
    def __init__(self, model_name="Salesforce/blip-image-captioning-large"):
        self.processor = BlipProcessor.from_pretrained(model_name)
        self.model = BlipForConditionalGeneration.from_pretrained(model_name)
    
    def generate_caption(self, image, max_length=50):
        """"""
        inputs = self.processor(image, return_tensors="pt")
        outputs = self.model.generate(**inputs, max_length=max_length)
        caption = self.processor.decode(outputs[0], skip_special_tokens=True)
        return caption
    
    def generate_detailed_caption(self, image):
        """"""
        # 
        text = "a detailed description of"
        inputs = self.processor(image, text, return_tensors="pt")
        outputs = self.model.generate(**inputs, max_length=100)
        caption = self.processor.decode(outputs[0], skip_special_tokens=True)
        return caption

# 
system = ImageCaptioningSystem()
caption = system.generate_caption(image)
print(f": {caption}")
```

### 2. VQA

```python
class VisualQASystem:
    """"""
    
    def __init__(self):
        self.model = BlipForQuestionAnswering.from_pretrained(
            "Salesforce/blip-vqa-base"
        )
        self.processor = BlipProcessor.from_pretrained(
            "Salesforce/blip-vqa-base"
        )
    
    def answer_question(self, image, question):
        """"""
        inputs = self.processor(image, question, return_tensors="pt")
        outputs = self.model.generate(**inputs)
        answer = self.processor.decode(outputs[0], skip_special_tokens=True)
        return answer
    
    def batch_qa(self, image, questions):
        """"""
        answers = []
        for question in questions:
            answer = self.answer_question(image, question)
            answers.append(answer)
        return answers

# 
vqa = VisualQASystem()
questions = [
    "What color is the cat?",
    "How many cats are there?",
    "What is the cat doing?"
]
answers = vqa.batch_qa(image, questions)
for q, a in zip(questions, answers):
    print(f"Q: {q}\nA: {a}\n")
```

### 3. 

```python
class ImageSearchEngine:
    """"""
    
    def __init__(self):
        self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.image_database = []
        self.image_embeddings = None
    
    def add_images(self, images):
        """"""
        self.image_database.extend(images)
        
        # 
        inputs = self.processor(images=images, return_tensors="pt")
        with torch.no_grad():
            embeddings = self.model.get_image_features(**inputs)
        
        if self.image_embeddings is None:
            self.image_embeddings = embeddings
        else:
            self.image_embeddings = torch.cat([self.image_embeddings, embeddings])
    
    def search_by_text(self, query, top_k=5):
        """"""
        # 
        inputs = self.processor(text=[query], return_tensors="pt")
        with torch.no_grad():
            text_embedding = self.model.get_text_features(**inputs)
        
        # 
        similarities = (text_embedding @ self.image_embeddings.T).squeeze()
        
        # top-k
        top_indices = similarities.argsort(descending=True)[:top_k]
        
        results = [(self.image_database[i], similarities[i].item()) 
                   for i in top_indices]
        
        return results
    
    def search_by_image(self, query_image, top_k=5):
        """"""
        # 
        inputs = self.processor(images=query_image, return_tensors="pt")
        with torch.no_grad():
            query_embedding = self.model.get_image_features(**inputs)
        
        # 
        similarities = (query_embedding @ self.image_embeddings.T).squeeze()
        
        # top-k
        top_indices = similarities.argsort(descending=True)[1:top_k+1]
        
        results = [(self.image_database[i], similarities[i].item()) 
                   for i in top_indices]
        
        return results

# 
search_engine = ImageSearchEngine()
search_engine.add_images(image_list)

# 
results = search_engine.search_by_text("a cat sitting on a couch")
for img, score in results:
    print(f": {score:.4f}")
```

### 4. 

```python
from diffusers import StableDiffusionPipeline

class TextToImageGenerator:
    """"""
    
    def __init__(self, model_id="stabilityai/stable-diffusion-2-1"):
        self.pipe = StableDiffusionPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16
        )
        self.pipe = self.pipe.to("cuda")
    
    def generate(self, prompt, negative_prompt="", num_images=1, 
                 guidance_scale=7.5, num_inference_steps=50):
        """"""
        images = self.pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_images_per_prompt=num_images,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps
        ).images
        
        return images
    
    def generate_with_controlnet(self, prompt, control_image):
        """ControlNet"""
        # ControlNet
        pass

# 
generator = TextToImageGenerator()
images = generator.generate(
    prompt="a beautiful sunset over the ocean, highly detailed",
    negative_prompt="blurry, low quality",
    num_images=4
)

for i, img in enumerate(images):
    img.save(f"generated_{i}.png")
```

## 

### 1. 

```python
class MultimodalDataset(torch.utils.data.Dataset):
    """"""
    
    def __init__(self, image_paths, captions, processor):
        self.image_paths = image_paths
        self.captions = captions
        self.processor = processor
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # 
        image = Image.open(self.image_paths[idx]).convert('RGB')
        caption = self.captions[idx]
        
        # 
        encoding = self.processor(
            images=image,
            text=caption,
            return_tensors="pt",
            padding="max_length",
            truncation=True
        )
        
        # batch
        encoding = {k: v.squeeze() for k, v in encoding.items()}
        
        return encoding
```

### 2. 

```python
def train_multimodal_model(model, train_loader, val_loader, epochs=10):
    """"""
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)
    
    for epoch in range(epochs):
        # 
        model.train()
        train_loss = 0
        for batch in train_loader:
            optimizer.zero_grad()
            
            outputs = model(**batch)
            loss = outputs.loss
            
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # 
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in val_loader:
                outputs = model(**batch)
                val_loss += outputs.loss.item()
        
        scheduler.step()
        
        print(f"Epoch {epoch+1}/{epochs}")
        print(f"Train Loss: {train_loss/len(train_loader):.4f}")
        print(f"Val Loss: {val_loss/len(val_loader):.4f}")
```

### 3. 

```python
@torch.no_grad()
def optimized_inference(model, image, text):
    """"""
    # 
    model = model.half()
    
    # 
    if isinstance(image, list):
        batch_size = len(image)
    else:
        batch_size = 1
        image = [image]
        text = [text]
    
    # 
    inputs = processor(images=image, text=text, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    # 
    outputs = model(**inputs)
    
    return outputs
```

## 

****
- 
- 

****
- ++++3D
- 

**Embodied AI**
- 
- 
- 

****
- 
- 
- 

## 

****:
1. AI
2. CLIP-
3. 
4. VQA

****:
- CLIP
- 
- 
- 

<DocCardList />

