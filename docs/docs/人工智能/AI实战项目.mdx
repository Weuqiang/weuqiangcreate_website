---
sidebar_position: 14
title: AI实战项目
---

# AI实战项目

:::info 章节概述
本章节提供完整的AI实战项目案例，从数据准备到模型部署的全流程。
:::

## 项目1：智能图像识别系统

### 项目概述

构建一个完整的图像识别Web应用，支持上传图像并识别其中的物体。

### 技术栈

- **后端**: FastAPI
- **模型**: YOLOv8
- **前端**: Streamlit
- **部署**: Docker

### 完整代码

```python
# app.py
from fastapi import FastAPI, File, UploadFile
from fastapi.responses import JSONResponse
from ultralytics import YOLO
from PIL import Image
import io

app = FastAPI()
model = YOLO('yolov8n.pt')

@app.post("/detect")
async def detect_objects(file: UploadFile = File(...)):
    """检测图像中的物体"""
    # 读取图像
    contents = await file.read()
    image = Image.open(io.BytesIO(contents))
    
    # 检测
    results = model(image)
    
    # 解析结果
    detections = []
    for result in results:
        boxes = result.boxes
        for box in boxes:
            detections.append({
                'class': result.names[int(box.cls)],
                'confidence': float(box.conf),
                'bbox': box.xyxy[0].tolist()
            })
    
    return JSONResponse(content={'detections': detections})

# streamlit_app.py
import streamlit as st
import requests
from PIL import Image

st.title("智能图像识别系统")

uploaded_file = st.file_uploader("上传图像", type=['jpg', 'png'])

if uploaded_file:
    image = Image.open(uploaded_file)
    st.image(image, caption='上传的图像')
    
    if st.button('识别'):
        # 调用API
        files = {'file': uploaded_file.getvalue()}
        response = requests.post('http://localhost:8000/detect', files=files)
        
        if response.status_code == 200:
            results = response.json()['detections']
            st.write("识别结果：")
            for det in results:
                st.write(f"- {det['class']}: {det['confidence']:.2%}")
```

### Docker部署

```dockerfile
# Dockerfile
FROM python:3.9

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

## 项目2：智能对话机器人

### 项目概述

基于大语言模型构建一个具有记忆功能的对话机器人。

### 核心功能

- 多轮对话
- 上下文记忆
- 工具调用
- 知识库检索

### 完整实现

```python
from langchain.llms import OpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

class ChatBot:
    """智能对话机器人"""
    
    def __init__(self):
        # LLM
        self.llm = OpenAI(temperature=0.7)
        
        # 记忆
        self.memory = ConversationBufferMemory()
        
        # 对话链
        self.conversation = ConversationChain(
            llm=self.llm,
            memory=self.memory,
            verbose=True
        )
        
        # 知识库
        self.knowledge_base = self._init_knowledge_base()
    
    def chat(self, user_input):
        """对话"""
        # 检索相关知识
        relevant_docs = self.knowledge_base.similarity_search(user_input, k=3)
        context = "\n".join([doc.page_content for doc in relevant_docs])
        
        # 构建prompt
        prompt = f"""基于以下知识回答问题：

知识：
{context}

问题：{user_input}

回答："""
        
        # 生成回复
        response = self.conversation.predict(input=prompt)
        
        return response
    
    def _init_knowledge_base(self):
        """初始化知识库"""
        documents = [
            "人工智能是计算机科学的一个分支...",
            "机器学习是实现人工智能的方法...",
            "深度学习使用神经网络..."
        ]
        
        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_texts(documents, embeddings)
        
        return vectorstore

# Web界面
import gradio as gr

bot = ChatBot()

def respond(message, history):
    response = bot.chat(message)
    return response

demo = gr.ChatInterface(
    respond,
    title="智能对话机器人",
    description="基于大语言模型的智能助手"
)

demo.launch()
```

## 项目3：文本情感分析系统

### 项目概述

构建一个实时文本情感分析系统，支持批量处理和可视化。

### 完整代码

```python
import pandas as pd
import matplotlib.pyplot as plt
from transformers import pipeline
import streamlit as st

class SentimentAnalysisSystem:
    """情感分析系统"""
    
    def __init__(self):
        self.analyzer = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english"
        )
    
    def analyze_text(self, text):
        """分析单条文本"""
        result = self.analyzer(text)[0]
        return {
            'text': text,
            'sentiment': result['label'],
            'score': result['score']
        }
    
    def analyze_batch(self, texts):
        """批量分析"""
        results = []
        for text in texts:
            result = self.analyze_text(text)
            results.append(result)
        return pd.DataFrame(results)
    
    def visualize(self, df):
        """可视化结果"""
        sentiment_counts = df['sentiment'].value_counts()
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # 饼图
        ax1.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')
        ax1.set_title('情感分布')
        
        # 柱状图
        ax2.bar(sentiment_counts.index, sentiment_counts.values)
        ax2.set_title('情感统计')
        ax2.set_ylabel('数量')
        
        return fig

# Streamlit应用
def main():
    st.title("文本情感分析系统")
    
    system = SentimentAnalysisSystem()
    
    # 单文本分析
    st.header("单文本分析")
    text = st.text_area("输入文本")
    if st.button("分析"):
        result = system.analyze_text(text)
        st.write(f"情感: {result['sentiment']}")
        st.write(f"置信度: {result['score']:.2%}")
    
    # 批量分析
    st.header("批量分析")
    uploaded_file = st.file_uploader("上传CSV文件", type=['csv'])
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        texts = df['text'].tolist()
        
        if st.button("批量分析"):
            results = system.analyze_batch(texts)
            st.dataframe(results)
            
            # 可视化
            fig = system.visualize(results)
            st.pyplot(fig)

if __name__ == '__main__':
    main()
```

## 项目4：推荐系统

### 项目概述

构建一个基于协同过滤和深度学习的混合推荐系统。

### 核心代码

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

class RecommenderModel(nn.Module):
    """推荐模型"""
    
    def __init__(self, num_users, num_items, embedding_dim=50):
        super().__init__()
        
        # 用户和物品嵌入
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # MLP层
        self.fc = nn.Sequential(
            nn.Linear(embedding_dim * 2, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, user_ids, item_ids):
        # 获取嵌入
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        
        # 拼接
        x = torch.cat([user_emb, item_emb], dim=-1)
        
        # 预测评分
        score = self.fc(x)
        
        return score

class RecommenderSystem:
    """推荐系统"""
    
    def __init__(self, num_users, num_items):
        self.model = RecommenderModel(num_users, num_items)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        self.criterion = nn.BCELoss()
    
    def train(self, train_loader, epochs=10):
        """训练模型"""
        self.model.train()
        
        for epoch in range(epochs):
            total_loss = 0
            for user_ids, item_ids, ratings in train_loader:
                # 前向传播
                predictions = self.model(user_ids, item_ids).squeeze()
                loss = self.criterion(predictions, ratings.float())
                
                # 反向传播
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
            
            print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}")
    
    def recommend(self, user_id, top_k=10):
        """为用户推荐物品"""
        self.model.eval()
        
        # 计算所有物品的得分
        item_ids = torch.arange(self.model.item_embedding.num_embeddings)
        user_ids = torch.full_like(item_ids, user_id)
        
        with torch.no_grad():
            scores = self.model(user_ids, item_ids).squeeze()
        
        # 获取top-k
        top_items = torch.topk(scores, top_k).indices.tolist()
        
        return top_items
```

## 项目部署

### 模型优化

```python
# 模型量化
import torch.quantization as quantization

def quantize_model(model):
    """量化模型"""
    model.eval()
    model_quantized = quantization.quantize_dynamic(
        model,
        {nn.Linear},
        dtype=torch.qint8
    )
    return model_quantized

# ONNX导出
def export_to_onnx(model, dummy_input, output_path):
    """导出为ONNX格式"""
    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=11,
        input_names=['input'],
        output_names=['output']
    )
```

### API服务

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class PredictionRequest(BaseModel):
    user_id: int
    top_k: int = 10

@app.post("/recommend")
async def recommend(request: PredictionRequest):
    """推荐接口"""
    recommendations = recommender.recommend(
        request.user_id,
        request.top_k
    )
    return {'recommendations': recommendations}
```

## 总结

**项目要点**:
1. 完整的数据处理流程
2. 模型训练和评估
3. Web界面开发
4. 模型部署和优化

**学习建议**:
- 从简单项目开始
- 注重工程实践
- 关注性能优化
- 持续迭代改进

<DocCardList />

