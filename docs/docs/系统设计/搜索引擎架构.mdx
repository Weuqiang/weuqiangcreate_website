---
sidebar_position: 6
title: æœç´¢å¼•æ“æ¶æ„è®¾è®¡
tags: [ç³»ç»Ÿè®¾è®¡, æœç´¢å¼•æ“, Elasticsearch, å…¨æ–‡æ£€ç´¢]
---

# æœç´¢å¼•æ“æ¶æ„è®¾è®¡

æœç´¢å¼•æ“æ˜¯äº’è”ç½‘çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼Œæœ¬æ–‡å°†è¯¦ç»†è®²è§£å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½ã€é«˜å¯ç”¨çš„æœç´¢å¼•æ“ç³»ç»Ÿã€‚

## éœ€æ±‚åˆ†æ

### åŠŸèƒ½éœ€æ±‚

```javascript
const requirements = {
  // æ ¸å¿ƒåŠŸèƒ½
  core: {
    crawling: true,        // ç½‘é¡µçˆ¬å–
    indexing: true,        // ç´¢å¼•æ„å»º
    searching: true,       // æœç´¢æŸ¥è¯¢
    ranking: true          // ç»“æœæ’åº
  },
  
  // æœç´¢åŠŸèƒ½
  search: {
    fullText: true,        // å…¨æ–‡æœç´¢
    fuzzy: true,           // æ¨¡ç³Šæœç´¢
    autocomplete: true,    // è‡ªåŠ¨è¡¥å…¨
    suggestion: true,      // æœç´¢å»ºè®®
    filter: true,          // è¿‡æ»¤æ¡ä»¶
    facet: true           // åˆ†é¢æœç´¢
  },
  
  // æ‰©å±•åŠŸèƒ½
  extended: {
    imageSearch: true,     // å›¾ç‰‡æœç´¢
    voiceSearch: true,     // è¯­éŸ³æœç´¢
    personalization: true, // ä¸ªæ€§åŒ–
    analytics: true        // æœç´¢åˆ†æ
  }
};
```

### éåŠŸèƒ½éœ€æ±‚

```python
# å®¹é‡ä¼°ç®—
ç½‘é¡µæ€»æ•° = 10_000_000_000        # 100äº¿ç½‘é¡µ
æ¯æ—¥æ–°å¢ = 100_000_000           # 1äº¿æ–°å¢
æ¯æ—¥æœç´¢ = 5_000_000_000         # 50äº¿æœç´¢

# QPSè®¡ç®—
æœç´¢QPS = æ¯æ—¥æœç´¢ / 86400
å³°å€¼QPS = æœç´¢QPS * 3

print(f"å¹³å‡QPS: {æœç´¢QPS:,.0f}")
print(f"å³°å€¼QPS: {å³°å€¼QPS:,.0f}")

# å­˜å‚¨ä¼°ç®—
æ¯é¡µå¤§å° = 100 * 1024            # 100KB
ç´¢å¼•è†¨èƒ€ç‡ = 0.3                 # ç´¢å¼•å åŸæ–‡30%
æ€»å­˜å‚¨_PB = (ç½‘é¡µæ€»æ•° * æ¯é¡µå¤§å° * (1 + ç´¢å¼•è†¨èƒ€ç‡)) / (1024 ** 5)

print(f"å­˜å‚¨éœ€æ±‚: {æ€»å­˜å‚¨_PB:.2f} PB")

# å“åº”æ—¶é—´
P99å»¶è¿Ÿ = 200                    # 200ms
P95å»¶è¿Ÿ = 100                    # 100ms
```

## æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              çˆ¬è™«ç³»ç»Ÿ                    â”‚
â”‚  Crawler â†’ URL Queue â†’ Parser           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            ç´¢å¼•ç³»ç»Ÿ                      â”‚
â”‚  Indexer â†’ Inverted Index â†’ Storage     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            æŸ¥è¯¢ç³»ç»Ÿ                      â”‚
â”‚  Query Parser â†’ Searcher â†’ Ranker       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŠ€æœ¯é€‰å‹

```javascript
const techStack = {
  // æœç´¢å¼•æ“
  searchEngine: {
    elasticsearch: 'åˆ†å¸ƒå¼æœç´¢',
    solr: 'ä¼ä¸šçº§æœç´¢',
    custom: 'è‡ªç ”å¼•æ“'
  },
  
  // çˆ¬è™«æ¡†æ¶
  crawler: {
    scrapy: 'Pythonçˆ¬è™«',
    puppeteer: 'JSæ¸²æŸ“',
    selenium: 'åŠ¨æ€é¡µé¢'
  },
  
  // å­˜å‚¨
  storage: {
    hdfs: 'åŸå§‹ç½‘é¡µ',
    hbase: 'ç»“æ„åŒ–æ•°æ®',
    redis: 'ç¼“å­˜çƒ­ç‚¹'
  },
  
  // æ¶ˆæ¯é˜Ÿåˆ—
  queue: {
    kafka: 'URLé˜Ÿåˆ—',
    rabbitmq: 'ä»»åŠ¡åˆ†å‘'
  }
};
```

## çˆ¬è™«ç³»ç»Ÿ

### åˆ†å¸ƒå¼çˆ¬è™«

```python
import asyncio
import aiohttp
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import hashlib

class DistributedCrawler:
    def __init__(self, redis_client, kafka_producer):
        self.redis = redis_client
        self.kafka = kafka_producer
        self.visited = set()
        self.max_depth = 5
        
    async def crawl(self, start_urls):
        """å¼€å§‹çˆ¬å–"""
        # åˆå§‹åŒ–URLé˜Ÿåˆ—
        for url in start_urls:
            await self.add_url(url, depth=0)
        
        # å¯åŠ¨å¤šä¸ªworker
        workers = [
            asyncio.create_task(self.worker(i))
            for i in range(10)
        ]
        
        await asyncio.gather(*workers)
    
    async def worker(self, worker_id):
        """çˆ¬è™«å·¥ä½œè¿›ç¨‹"""
        async with aiohttp.ClientSession() as session:
            while True:
                # ä»é˜Ÿåˆ—è·å–URL
                url_data = await self.get_next_url()
                
                if not url_data:
                    await asyncio.sleep(1)
                    continue
                
                url, depth = url_data
                
                # çˆ¬å–é¡µé¢
                try:
                    html = await self.fetch(session, url)
                    
                    # è§£æé¡µé¢
                    links, content = await self.parse(html, url)
                    
                    # ä¿å­˜å†…å®¹
                    await self.save_page(url, content)
                    
                    # æ·»åŠ æ–°é“¾æ¥
                    if depth < self.max_depth:
                        for link in links:
                            await self.add_url(link, depth + 1)
                    
                except Exception as e:
                    print(f"Error crawling {url}: {e}")
    
    async def fetch(self, session, url):
        """è·å–ç½‘é¡µå†…å®¹"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; MyBot/1.0)'
        }
        
        async with session.get(url, headers=headers, timeout=10) as response:
            if response.status == 200:
                return await response.text()
            else:
                raise Exception(f"HTTP {response.status}")
    
    async def parse(self, html, base_url):
        """è§£æç½‘é¡µ"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå–é“¾æ¥
        links = []
        for a in soup.find_all('a', href=True):
            link = urljoin(base_url, a['href'])
            if self.is_valid_url(link):
                links.append(link)
        
        # æå–å†…å®¹
        content = {
            'title': soup.title.string if soup.title else '',
            'text': soup.get_text(),
            'meta': self.extract_meta(soup),
            'headings': [h.get_text() for h in soup.find_all(['h1', 'h2', 'h3'])]
        }
        
        return links, content
    
    def is_valid_url(self, url):
        """éªŒè¯URL"""
        parsed = urlparse(url)
        return (
            parsed.scheme in ['http', 'https'] and
            parsed.netloc and
            not any(ext in url for ext in ['.pdf', '.jpg', '.png'])
        )
    
    async def add_url(self, url, depth):
        """æ·»åŠ URLåˆ°é˜Ÿåˆ—"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        
        # å»é‡
        if await self.redis.sismember('visited', url_hash):
            return
        
        await self.redis.sadd('visited', url_hash)
        
        # æ·»åŠ åˆ°é˜Ÿåˆ—ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
        priority = self.calculate_priority(url, depth)
        await self.redis.zadd('url_queue', {url: priority})
    
    async def get_next_url(self):
        """è·å–ä¸‹ä¸€ä¸ªURL"""
        result = await self.redis.zpopmax('url_queue')
        if result:
            url, priority = result[0]
            depth = int(priority) % 10
            return url, depth
        return None
    
    def calculate_priority(self, url, depth):
        """è®¡ç®—URLä¼˜å…ˆçº§"""
        # æ·±åº¦è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜
        priority = (10 - depth) * 1000
        
        # åŸŸåæƒé‡
        domain = urlparse(url).netloc
        domain_rank = self.get_domain_rank(domain)
        priority += domain_rank
        
        return priority
    
    async def save_page(self, url, content):
        """ä¿å­˜é¡µé¢åˆ°Kafka"""
        page_data = {
            'url': url,
            'content': content,
            'crawled_at': time.time()
        }
        
        await self.kafka.send('crawled_pages', page_data)
```

### URLå»é‡

```python
class URLDeduplicator:
    def __init__(self):
        # å¸ƒéš†è¿‡æ»¤å™¨
        self.bloom_filter = BloomFilter(size=100000000, hash_count=7)
        
    def is_duplicate(self, url):
        """æ£€æŸ¥URLæ˜¯å¦é‡å¤"""
        url_hash = self.normalize_url(url)
        
        if self.bloom_filter.contains(url_hash):
            # å¯èƒ½é‡å¤ï¼ŒæŸ¥è¯¢æ•°æ®åº“ç¡®è®¤
            return self.check_in_db(url_hash)
        
        # ä¸€å®šä¸é‡å¤
        self.bloom_filter.add(url_hash)
        return False
    
    def normalize_url(self, url):
        """URLè§„èŒƒåŒ–"""
        parsed = urlparse(url)
        
        # ç§»é™¤fragment
        url = url.split('#')[0]
        
        # ç§»é™¤é»˜è®¤ç«¯å£
        if parsed.port == 80 or parsed.port == 443:
            url = url.replace(f':{parsed.port}', '')
        
        # æ’åºæŸ¥è¯¢å‚æ•°
        if parsed.query:
            params = sorted(parsed.query.split('&'))
            url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{'&'.join(params)}"
        
        return hashlib.md5(url.encode()).hexdigest()
```

## ç´¢å¼•ç³»ç»Ÿ

### å€’æ’ç´¢å¼•

```javascript
class InvertedIndex {
  constructor() {
    this.index = new Map(); // term -> postings list
    this.documents = new Map(); // docId -> document
    this.docCount = 0;
  }
  
  // æ·»åŠ æ–‡æ¡£
  addDocument(docId, content) {
    this.docCount++;
    this.documents.set(docId, content);
    
    // åˆ†è¯
    const tokens = this.tokenize(content);
    
    // æ„å»ºå€’æ’ç´¢å¼•
    const termFreq = new Map();
    
    for (let i = 0; i < tokens.length; i++) {
      const term = tokens[i];
      
      // è¯é¢‘ç»Ÿè®¡
      termFreq.set(term, (termFreq.get(term) || 0) + 1);
      
      // æ›´æ–°å€’æ’ç´¢å¼•
      if (!this.index.has(term)) {
        this.index.set(term, []);
      }
      
      const postings = this.index.get(term);
      let posting = postings.find(p => p.docId === docId);
      
      if (!posting) {
        posting = {
          docId,
          positions: [],
          tf: 0
        };
        postings.push(posting);
      }
      
      posting.positions.push(i);
      posting.tf++;
    }
    
    // è®¡ç®—TF-IDF
    this.updateTFIDF(docId, termFreq);
  }
  
  // åˆ†è¯
  tokenize(text) {
    // ç®€å•åˆ†è¯ï¼ˆå®é™…åº”ä½¿ç”¨ä¸“ä¸šåˆ†è¯å™¨ï¼‰
    return text
      .toLowerCase()
      .replace(/[^\w\s]/g, ' ')
      .split(/\s+/)
      .filter(token => token.length > 0);
  }
  
  // æ›´æ–°TF-IDF
  updateTFIDF(docId, termFreq) {
    for (const [term, tf] of termFreq) {
      const postings = this.index.get(term);
      const posting = postings.find(p => p.docId === docId);
      
      // TF: è¯é¢‘
      const termFrequency = tf / termFreq.size;
      
      // IDF: é€†æ–‡æ¡£é¢‘ç‡
      const docFrequency = postings.length;
      const idf = Math.log(this.docCount / docFrequency);
      
      // TF-IDF
      posting.tfidf = termFrequency * idf;
    }
  }
  
  // æœç´¢
  search(query, limit = 10) {
    const tokens = this.tokenize(query);
    
    // è·å–æ‰€æœ‰ç›¸å…³æ–‡æ¡£
    const docScores = new Map();
    
    for (const term of tokens) {
      const postings = this.index.get(term) || [];
      
      for (const posting of postings) {
        const score = docScores.get(posting.docId) || 0;
        docScores.set(posting.docId, score + posting.tfidf);
      }
    }
    
    // æ’åºå¹¶è¿”å›
    return Array.from(docScores.entries())
      .sort((a, b) => b[1] - a[1])
      .slice(0, limit)
      .map(([docId, score]) => ({
        docId,
        score,
        document: this.documents.get(docId)
      }));
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const index = new InvertedIndex();

index.addDocument(1, 'The quick brown fox jumps over the lazy dog');
index.addDocument(2, 'A quick brown dog outpaces a quick fox');
index.addDocument(3, 'The lazy cat sleeps all day');

const results = index.search('quick fox');
console.log(results);
```

### Elasticsearché›†æˆ

```javascript
class ElasticsearchIndexer {
  constructor() {
    this.client = new Client({
      node: 'http://localhost:9200'
    });
  }
  
  async createIndex(indexName) {
    await this.client.indices.create({
      index: indexName,
      body: {
        settings: {
          number_of_shards: 5,
          number_of_replicas: 1,
          analysis: {
            analyzer: {
              custom_analyzer: {
                type: 'custom',
                tokenizer: 'standard',
                filter: ['lowercase', 'stop', 'snowball']
              }
            }
          }
        },
        mappings: {
          properties: {
            url: { type: 'keyword' },
            title: {
              type: 'text',
              analyzer: 'custom_analyzer',
              fields: {
                keyword: { type: 'keyword' }
              }
            },
            content: {
              type: 'text',
              analyzer: 'custom_analyzer'
            },
            meta_description: { type: 'text' },
            headings: { type: 'text' },
            crawled_at: { type: 'date' },
            page_rank: { type: 'float' }
          }
        }
      }
    });
  }
  
  async indexDocument(indexName, doc) {
    await this.client.index({
      index: indexName,
      body: doc
    });
  }
  
  async bulkIndex(indexName, docs) {
    const body = docs.flatMap(doc => [
      { index: { _index: indexName } },
      doc
    ]);
    
    const { body: bulkResponse } = await this.client.bulk({
      refresh: true,
      body
    });
    
    if (bulkResponse.errors) {
      console.error('Bulk indexing errors:', bulkResponse.errors);
    }
  }
  
  async search(indexName, query, options = {}) {
    const {
      from = 0,
      size = 10,
      filters = {},
      sort = []
    } = options;
    
    const { body } = await this.client.search({
      index: indexName,
      body: {
        from,
        size,
        query: {
          bool: {
            must: [
              {
                multi_match: {
                  query,
                  fields: ['title^3', 'content', 'headings^2'],
                  type: 'best_fields',
                  fuzziness: 'AUTO'
                }
              }
            ],
            filter: Object.entries(filters).map(([field, value]) => ({
              term: { [field]: value }
            }))
          }
        },
        highlight: {
          fields: {
            title: {},
            content: {
              fragment_size: 150,
              number_of_fragments: 3
            }
          }
        },
        sort: sort.length > 0 ? sort : [
          { _score: 'desc' },
          { page_rank: 'desc' }
        ]
      }
    });
    
    return {
      total: body.hits.total.value,
      results: body.hits.hits.map(hit => ({
        id: hit._id,
        score: hit._score,
        source: hit._source,
        highlight: hit.highlight
      }))
    };
  }
}
```

## æŸ¥è¯¢å¤„ç†

### æŸ¥è¯¢è§£æå™¨

```javascript
class QueryParser {
  parse(queryString) {
    const query = {
      terms: [],
      phrases: [],
      filters: {},
      operators: []
    };
    
    // æå–çŸ­è¯­ï¼ˆå¼•å·å†…å®¹ï¼‰
    const phraseRegex = /"([^"]+)"/g;
    let match;
    while ((match = phraseRegex.exec(queryString)) !== null) {
      query.phrases.push(match[1]);
      queryString = queryString.replace(match[0], '');
    }
    
    // æå–è¿‡æ»¤æ¡ä»¶
    const filterRegex = /(\w+):(\w+)/g;
    while ((match = filterRegex.exec(queryString)) !== null) {
      query.filters[match[1]] = match[2];
      queryString = queryString.replace(match[0], '');
    }
    
    // æå–æ“ä½œç¬¦
    const operatorRegex = /(AND|OR|NOT)/gi;
    while ((match = operatorRegex.exec(queryString)) !== null) {
      query.operators.push(match[1].toUpperCase());
      queryString = queryString.replace(match[0], '');
    }
    
    // æå–æ™®é€šè¯é¡¹
    query.terms = queryString
      .trim()
      .split(/\s+/)
      .filter(term => term.length > 0);
    
    return query;
  }
  
  buildElasticsearchQuery(parsedQuery) {
    const must = [];
    const should = [];
    const mustNot = [];
    
    // çŸ­è¯­æŸ¥è¯¢
    for (const phrase of parsedQuery.phrases) {
      must.push({
        match_phrase: {
          content: phrase
        }
      });
    }
    
    // è¯é¡¹æŸ¥è¯¢
    for (const term of parsedQuery.terms) {
      should.push({
        multi_match: {
          query: term,
          fields: ['title^3', 'content', 'headings^2']
        }
      });
    }
    
    // è¿‡æ»¤æ¡ä»¶
    const filter = Object.entries(parsedQuery.filters).map(([field, value]) => ({
      term: { [field]: value }
    }));
    
    return {
      bool: {
        must,
        should,
        must_not: mustNot,
        filter,
        minimum_should_match: should.length > 0 ? 1 : 0
      }
    };
  }
}
```

### è‡ªåŠ¨è¡¥å…¨

```javascript
class Autocomplete {
  constructor(redis) {
    this.redis = redis;
  }
  
  async suggest(prefix, limit = 10) {
    // ä½¿ç”¨Redisçš„Sorted Setå®ç°
    const key = `autocomplete:${prefix.toLowerCase()}`;
    
    // è·å–å»ºè®®
    const suggestions = await this.redis.zrevrange(key, 0, limit - 1, 'WITHSCORES');
    
    return suggestions
      .filter((_, i) => i % 2 === 0)
      .map((term, i) => ({
        term,
        score: suggestions[i * 2 + 1]
      }));
  }
  
  async addTerm(term, score = 1) {
    // ä¸ºæ¯ä¸ªå‰ç¼€æ·»åŠ è¯é¡¹
    for (let i = 1; i <= term.length; i++) {
      const prefix = term.substring(0, i).toLowerCase();
      const key = `autocomplete:${prefix}`;
      
      await this.redis.zincrby(key, score, term);
    }
  }
  
  async updateFromSearchLog() {
    // ä»æœç´¢æ—¥å¿—æ›´æ–°çƒ­é—¨æœç´¢è¯
    const searches = await this.getRecentSearches();
    
    for (const [term, count] of searches) {
      await this.addTerm(term, count);
    }
  }
}
```

## æ’åºç®—æ³•

### PageRank

```python
import numpy as np

class PageRank:
    def __init__(self, damping_factor=0.85, max_iterations=100, tolerance=1e-6):
        self.d = damping_factor
        self.max_iter = max_iterations
        self.tol = tolerance
    
    def calculate(self, graph):
        """
        è®¡ç®—PageRank
        graph: é‚»æ¥çŸ©é˜µï¼Œgraph[i][j] = 1 è¡¨ç¤ºé¡µé¢ié“¾æ¥åˆ°é¡µé¢j
        """
        n = len(graph)
        
        # åˆå§‹åŒ–PageRankå€¼
        pr = np.ones(n) / n
        
        # è®¡ç®—å‡ºé“¾æ•°
        out_degree = np.sum(graph, axis=1)
        out_degree[out_degree == 0] = 1  # é¿å…é™¤é›¶
        
        # è¿­ä»£è®¡ç®—
        for iteration in range(self.max_iter):
            pr_new = np.zeros(n)
            
            for i in range(n):
                # è®¡ç®—ä»å…¶ä»–é¡µé¢ä¼ é€’æ¥çš„PageRank
                for j in range(n):
                    if graph[j][i] == 1:
                        pr_new[i] += pr[j] / out_degree[j]
                
                # åº”ç”¨é˜»å°¼å› å­
                pr_new[i] = (1 - self.d) / n + self.d * pr_new[i]
            
            # æ£€æŸ¥æ”¶æ•›
            if np.sum(np.abs(pr_new - pr)) < self.tol:
                print(f"Converged after {iteration + 1} iterations")
                break
            
            pr = pr_new
        
        return pr

# ä½¿ç”¨ç¤ºä¾‹
graph = np.array([
    [0, 1, 1, 0],
    [0, 0, 1, 1],
    [1, 0, 0, 1],
    [0, 0, 1, 0]
])

pagerank = PageRank()
scores = pagerank.calculate(graph)
print("PageRank scores:", scores)
```

### BM25æ’åº

```javascript
class BM25Ranker {
  constructor(k1 = 1.5, b = 0.75) {
    this.k1 = k1; // è¯é¢‘é¥±å’Œå‚æ•°
    this.b = b;   // é•¿åº¦å½’ä¸€åŒ–å‚æ•°
  }
  
  score(query, document, avgDocLength, docCount) {
    const tokens = this.tokenize(query);
    let score = 0;
    
    for (const term of tokens) {
      // è¯é¢‘
      const tf = this.termFrequency(term, document);
      
      // æ–‡æ¡£é¢‘ç‡
      const df = this.documentFrequency(term);
      
      // IDF
      const idf = Math.log((docCount - df + 0.5) / (df + 0.5) + 1);
      
      // æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–
      const docLength = document.length;
      const norm = 1 - this.b + this.b * (docLength / avgDocLength);
      
      // BM25å¾—åˆ†
      score += idf * (tf * (this.k1 + 1)) / (tf + this.k1 * norm);
    }
    
    return score;
  }
  
  termFrequency(term, document) {
    const tokens = this.tokenize(document);
    return tokens.filter(t => t === term).length;
  }
  
  tokenize(text) {
    return text.toLowerCase().split(/\s+/);
  }
}
```

## æ€§èƒ½ä¼˜åŒ–

### 1. æŸ¥è¯¢ç¼“å­˜

```javascript
class QueryCache {
  constructor(redis, ttl = 3600) {
    this.redis = redis;
    this.ttl = ttl;
  }
  
  async get(query) {
    const key = `query:${this.hashQuery(query)}`;
    const cached = await this.redis.get(key);
    
    if (cached) {
      return JSON.parse(cached);
    }
    
    return null;
  }
  
  async set(query, results) {
    const key = `query:${this.hashQuery(query)}`;
    await this.redis.setex(key, this.ttl, JSON.stringify(results));
  }
  
  hashQuery(query) {
    return crypto.createHash('md5').update(JSON.stringify(query)).digest('hex');
  }
}
```

### 2. ç´¢å¼•åˆ†ç‰‡

```javascript
class IndexSharding {
  constructor(shardCount = 10) {
    this.shardCount = shardCount;
  }
  
  getShardId(docId) {
    return docId % this.shardCount;
  }
  
  async search(query, limit = 10) {
    // å¹¶è¡Œæœç´¢æ‰€æœ‰åˆ†ç‰‡
    const shardPromises = [];
    
    for (let i = 0; i < this.shardCount; i++) {
      shardPromises.push(
        this.searchShard(i, query, limit)
      );
    }
    
    const shardResults = await Promise.all(shardPromises);
    
    // åˆå¹¶ç»“æœ
    return this.mergeResults(shardResults, limit);
  }
  
  mergeResults(shardResults, limit) {
    // åˆå¹¶æ‰€æœ‰åˆ†ç‰‡çš„ç»“æœ
    const allResults = shardResults.flat();
    
    // æŒ‰åˆ†æ•°æ’åº
    allResults.sort((a, b) => b.score - a.score);
    
    // è¿”å›top N
    return allResults.slice(0, limit);
  }
}
```

## ç›‘æ§å‘Šè­¦

```javascript
class SearchMonitoring {
  async monitor() {
    // 1. æŸ¥è¯¢QPS
    const qps = await this.getQueryQPS();
    
    // 2. å¹³å‡å“åº”æ—¶é—´
    const avgLatency = await this.getAverageLatency();
    
    // 3. ç´¢å¼•å¤§å°
    const indexSize = await this.getIndexSize();
    
    // 4. ç¼“å­˜å‘½ä¸­ç‡
    const cacheHitRate = await this.getCacheHitRate();
    
    // 5. é”™è¯¯ç‡
    const errorRate = await this.getErrorRate();
    
    // è®°å½•æŒ‡æ ‡
    await this.recordMetrics({
      qps,
      avgLatency,
      indexSize,
      cacheHitRate,
      errorRate
    });
    
    // å‘Šè­¦
    if (avgLatency > 200) {
      await this.alert('High search latency', { avgLatency });
    }
    
    if (errorRate > 0.01) {
      await this.alert('High error rate', { errorRate });
    }
  }
}
```

## æ€»ç»“

æœç´¢å¼•æ“æ¶æ„è®¾è®¡çš„æ ¸å¿ƒï¼š
- ğŸ•·ï¸ **çˆ¬è™«ç³»ç»Ÿ**ï¼šåˆ†å¸ƒå¼çˆ¬å– + URLå»é‡
- ğŸ“š **ç´¢å¼•ç³»ç»Ÿ**ï¼šå€’æ’ç´¢å¼• + TF-IDF + BM25
- ğŸ” **æŸ¥è¯¢å¤„ç†**ï¼šæŸ¥è¯¢è§£æ + è‡ªåŠ¨è¡¥å…¨
- ğŸ“Š **æ’åºç®—æ³•**ï¼šPageRank + ç›¸å…³æ€§æ’åº
- ğŸš€ **æ€§èƒ½ä¼˜åŒ–**ï¼šæŸ¥è¯¢ç¼“å­˜ + ç´¢å¼•åˆ†ç‰‡
- ğŸ“ˆ **ç›‘æ§å‘Šè­¦**ï¼šå®æ—¶ç›‘æ§ + æ€§èƒ½åˆ†æ

è®°ä½ï¼š**æœç´¢å¼•æ“çš„æœ¬è´¨æ˜¯é«˜æ•ˆçš„ä¿¡æ¯æ£€ç´¢å’Œç›¸å…³æ€§æ’åºï¼**

