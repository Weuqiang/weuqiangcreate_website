---
sidebar_position: 3
title: Kafka - 实时数据流的高速公路
---

# Kafka - 消息队列之王

Kafka是大数据领域最流行的消息队列。这一章教你**真正用Kafka构建实时数据管道**。

## 第一部分：理解Kafka

### 为什么需要Kafka？

**问题**：电商网站每秒产生10万条用户行为日志，如何实时处理？

**传统方式**：
```python
# 直接写数据库 - 数据库扛不住！
for event in events:
    db.insert(event)  # 太慢了
```

**Kafka方式**：
```python
# 先写入Kafka，异步处理
producer.send('user_events', event)  # 快！

# 多个消费者并行处理
consumer1.subscribe(['user_events'])  # 实时推荐
consumer2.subscribe(['user_events'])  # 数据分析
consumer3.subscribe(['user_events'])  # 监控告警
```

### Kafka核心概念

```
┌─────────────────────────────────────┐
│          Producer（生产者）          │
│      产生数据的应用                  │
└─────────────────────────────────────┘
              ↓
┌─────────────────────────────────────┐
│          Kafka Cluster              │
│  ┌──────────┬──────────┬──────────┐ │
│  │ Topic 1  │ Topic 2  │ Topic 3  │ │
│  │ Part 0   │ Part 0   │ Part 0   │ │
│  │ Part 1   │ Part 1   │ Part 1   │ │
│  └──────────┴──────────┴──────────┘ │
└─────────────────────────────────────┘
              ↓
┌─────────────────────────────────────┐
│         Consumer（消费者）           │
│      消费数据的应用                  │
└─────────────────────────────────────┘
```

**核心概念**：
- **Topic**：消息的类别（如：用户行为、订单、日志）
- **Partition**：Topic的分区，实现并行处理
- **Producer**：生产消息
- **Consumer**：消费消息
- **Consumer Group**：多个消费者组成的组，实现负载均衡

## 第二部分：快速开始

### 使用Docker搭建Kafka

```yaml
# docker-compose.yml
version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
```

```bash
# 启动
docker-compose up -d

# 查看日志
docker-compose logs -f kafka

# 进入容器
docker exec -it kafka bash

# 创建Topic
kafka-topics --create --topic test --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1

# 查看Topic
kafka-topics --list --bootstrap-server localhost:9092

# 查看Topic详情
kafka-topics --describe --topic test --bootstrap-server localhost:9092
```

### Python操作Kafka

**安装**：
```bash
pip install kafka-python
```

**生产者**：
```python
from kafka import KafkaProducer
import json
import time

# 创建生产者
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# 发送消息
for i in range(10):
    message = {
        'user_id': f'user_{i}',
        'action': 'click',
        'timestamp': time.time()
    }
    
    # 发送到topic
    future = producer.send('user_events', message)
    
    # 等待发送完成
    result = future.get(timeout=10)
    print(f"发送成功: partition={result.partition}, offset={result.offset}")
    
    time.sleep(1)

# 确保所有消息发送完成
producer.flush()
producer.close()
```

**消费者**：
```python
from kafka import KafkaConsumer
import json

# 创建消费者
consumer = KafkaConsumer(
    'user_events',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',  # 从最早的消息开始
    enable_auto_commit=True,
    group_id='my-group',
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

# 消费消息
print("开始消费消息...")
for message in consumer:
    print(f"收到消息: {message.value}")
    print(f"  partition: {message.partition}")
    print(f"  offset: {message.offset}")
    print(f"  timestamp: {message.timestamp}")
```

## 第三部分：Kafka核心特性

### 1. 分区（Partition）

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# 指定分区
producer.send('user_events', value={'msg': 'test'}, partition=0)

# 按key分区（相同key的消息会发到同一分区）
for i in range(10):
    user_id = f'user_{i % 3}'  # 只有3个用户
    producer.send(
        'user_events',
        key=user_id.encode('utf-8'),
        value={'user_id': user_id, 'action': 'click'}
    )

producer.flush()
producer.close()
```

**为什么要分区？**
- 并行处理：多个消费者同时消费不同分区
- 负载均衡：数据分散到多个分区
- 顺序保证：同一分区内消息有序

### 2. 消费者组（Consumer Group）

```python
from kafka import KafkaConsumer
import json
import threading

def consume(consumer_id, group_id):
    """消费者函数"""
    consumer = KafkaConsumer(
        'user_events',
        bootstrap_servers=['localhost:9092'],
        group_id=group_id,
        auto_offset_reset='earliest',
        value_deserializer=lambda m: json.loads(m.decode('utf-8'))
    )
    
    print(f"消费者 {consumer_id} 启动")
    
    for message in consumer:
        print(f"[消费者{consumer_id}] partition={message.partition}, offset={message.offset}")
        # 模拟处理
        time.sleep(0.1)

# 启动3个消费者（同一个组）
threads = []
for i in range(3):
    t = threading.Thread(target=consume, args=(i, 'my-group'))
    t.start()
    threads.append(t)

for t in threads:
    t.join()
```

**消费者组的特点**：
- 同一组内的消费者共享消息
- 每个分区只能被组内一个消费者消费
- 实现负载均衡和故障转移

### 3. 偏移量（Offset）管理

```python
from kafka import KafkaConsumer, TopicPartition

consumer = KafkaConsumer(
    bootstrap_servers=['localhost:9092'],
    group_id='my-group',
    enable_auto_commit=False  # 手动提交
)

# 订阅topic
consumer.subscribe(['user_events'])

# 消费消息
for message in consumer:
    print(f"消息: {message.value}")
    
    # 处理消息
    try:
        process_message(message)
        
        # 手动提交offset
        consumer.commit()
        print(f"提交offset: {message.offset}")
    except Exception as e:
        print(f"处理失败: {e}")
        # 不提交，下次重新消费

# 查看当前offset
partitions = consumer.assignment()
for partition in partitions:
    offset = consumer.position(partition)
    print(f"Partition {partition.partition}: offset={offset}")

# 重置offset到指定位置
tp = TopicPartition('user_events', 0)
consumer.seek(tp, 100)  # 从offset 100开始消费
```

## 第四部分：实战项目

### 项目1：实时用户行为分析

**架构**：
```
用户行为 → Kafka → Spark Streaming → Redis → Dashboard
```

**生产者：模拟用户行为**

```python
from kafka import KafkaProducer
import json
import time
import random

class UserBehaviorProducer:
    """用户行为数据生产者"""
    
    def __init__(self):
        self.producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        self.actions = ['view', 'click', 'cart', 'purchase']
        self.items = [f'item_{i}' for i in range(100)]
        self.users = [f'user_{i}' for i in range(1000)]
    
    def generate_event(self):
        """生成一个用户行为事件"""
        return {
            'user_id': random.choice(self.users),
            'action': random.choice(self.actions),
            'item_id': random.choice(self.items),
            'timestamp': int(time.time() * 1000),
            'price': random.randint(10, 1000)
        }
    
    def run(self, events_per_second=100):
        """持续生成事件"""
        print(f"开始生成事件，速率: {events_per_second}/秒")
        
        interval = 1.0 / events_per_second
        
        try:
            while True:
                event = self.generate_event()
                self.producer.send('user_behavior', event)
                time.sleep(interval)
        except KeyboardInterrupt:
            print("停止生成")
        finally:
            self.producer.close()

# 运行
if __name__ == '__main__':
    producer = UserBehaviorProducer()
    producer.run(events_per_second=100)
```

**消费者：实时统计**

```python
from kafka import KafkaConsumer
import json
from collections import defaultdict
import time
import threading

class RealTimeAnalyzer:
    """实时分析器"""
    
    def __init__(self):
        self.consumer = KafkaConsumer(
            'user_behavior',
            bootstrap_servers=['localhost:9092'],
            group_id='analyzer',
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
        # 统计数据
        self.stats = {
            'total_events': 0,
            'action_counts': defaultdict(int),
            'revenue': 0,
            'active_users': set()
        }
        
        self.lock = threading.Lock()
    
    def process_event(self, event):
        """处理单个事件"""
        with self.lock:
            self.stats['total_events'] += 1
            self.stats['action_counts'][event['action']] += 1
            self.stats['active_users'].add(event['user_id'])
            
            if event['action'] == 'purchase':
                self.stats['revenue'] += event['price']
    
    def print_stats(self):
        """打印统计信息"""
        while True:
            time.sleep(5)
            
            with self.lock:
                print("\n" + "="*50)
                print(f"总事件数: {self.stats['total_events']}")
                print(f"活跃用户: {len(self.stats['active_users'])}")
                print(f"总收入: ¥{self.stats['revenue']}")
                print("\n行为分布:")
                for action, count in self.stats['action_counts'].items():
                    percentage = count / self.stats['total_events'] * 100
                    print(f"  {action}: {count} ({percentage:.1f}%)")
                print("="*50)
    
    def run(self):
        """运行分析器"""
        # 启动统计打印线程
        stats_thread = threading.Thread(target=self.print_stats, daemon=True)
        stats_thread.start()
        
        # 消费消息
        print("开始实时分析...")
        for message in self.consumer:
            self.process_event(message.value)

# 运行
if __name__ == '__main__':
    analyzer = RealTimeAnalyzer()
    analyzer.run()
```

### 项目2：实时日志收集系统

```python
import logging
from kafka import KafkaProducer
import json
import socket

class KafkaLogHandler(logging.Handler):
    """Kafka日志处理器"""
    
    def __init__(self, topic, bootstrap_servers=['localhost:9092']):
        super().__init__()
        
        self.topic = topic
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        self.hostname = socket.gethostname()
    
    def emit(self, record):
        """发送日志到Kafka"""
        try:
            log_entry = {
                'timestamp': record.created,
                'level': record.levelname,
                'logger': record.name,
                'message': record.getMessage(),
                'hostname': self.hostname,
                'module': record.module,
                'function': record.funcName,
                'line': record.lineno
            }
            
            if record.exc_info:
                log_entry['exception'] = self.format(record)
            
            self.producer.send(self.topic, log_entry)
        except Exception as e:
            print(f"发送日志失败: {e}")
    
    def close(self):
        """关闭处理器"""
        self.producer.close()
        super().close()

# 使用示例
logger = logging.getLogger('my_app')
logger.setLevel(logging.INFO)

# 添加Kafka处理器
kafka_handler = KafkaLogHandler('app_logs')
logger.addHandler(kafka_handler)

# 记录日志
logger.info("应用启动")
logger.warning("内存使用率: 80%")
logger.error("数据库连接失败")

try:
    1 / 0
except Exception as e:
    logger.exception("发生异常")
```

**日志消费者：实时监控**

```python
from kafka import KafkaConsumer
import json

class LogMonitor:
    """日志监控器"""
    
    def __init__(self):
        self.consumer = KafkaConsumer(
            'app_logs',
            bootstrap_servers=['localhost:9092'],
            group_id='log_monitor',
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
        self.error_count = 0
        self.warning_count = 0
    
    def process_log(self, log):
        """处理日志"""
        level = log['level']
        message = log['message']
        
        # 彩色输出
        colors = {
            'INFO': '\033[92m',     # 绿色
            'WARNING': '\033[93m',  # 黄色
            'ERROR': '\033[91m',    # 红色
            'CRITICAL': '\033[95m'  # 紫色
        }
        reset = '\033[0m'
        
        color = colors.get(level, '')
        print(f"{color}[{level}] {log['hostname']} - {message}{reset}")
        
        # 统计
        if level == 'ERROR' or level == 'CRITICAL':
            self.error_count += 1
            if self.error_count >= 10:
                self.send_alert("错误日志过多！")
                self.error_count = 0
        
        elif level == 'WARNING':
            self.warning_count += 1
    
    def send_alert(self, message):
        """发送告警"""
        print(f"\n🚨 告警: {message}\n")
    
    def run(self):
        """运行监控"""
        print("开始监控日志...")
        for message in self.consumer:
            self.process_log(message.value)

# 运行
if __name__ == '__main__':
    monitor = LogMonitor()
    monitor.run()
```

## 第五部分：性能优化

### 1. 批量发送

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    # 批量配置
    batch_size=16384,  # 16KB
    linger_ms=10,      # 等待10ms收集更多消息
    compression_type='gzip'  # 压缩
)

# 发送大量消息
for i in range(10000):
    producer.send('test', {'id': i})

# 等待所有消息发送完成
producer.flush()
```

### 2. 异步发送

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

def on_send_success(record_metadata):
    """发送成功回调"""
    print(f"发送成功: topic={record_metadata.topic}, partition={record_metadata.partition}")

def on_send_error(excp):
    """发送失败回调"""
    print(f"发送失败: {excp}")

# 异步发送
for i in range(100):
    producer.send('test', {'id': i}) \
        .add_callback(on_send_success) \
        .add_errback(on_send_error)

producer.flush()
```

### 3. 消费者性能优化

```python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'test',
    bootstrap_servers=['localhost:9092'],
    group_id='my-group',
    # 性能配置
    fetch_min_bytes=1024,      # 最小拉取1KB
    fetch_max_wait_ms=500,     # 最多等待500ms
    max_poll_records=500,      # 每次最多拉取500条
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

# 批量处理
batch = []
batch_size = 100

for message in consumer:
    batch.append(message.value)
    
    if len(batch) >= batch_size:
        # 批量处理
        process_batch(batch)
        batch = []
        
        # 提交offset
        consumer.commit()
```

## 总结

Kafka是实时数据流的核心：

1. **高吞吐**：每秒百万级消息
2. **持久化**：消息存储在磁盘
3. **分布式**：水平扩展
4. **可靠性**：副本机制

记住：**Kafka不是数据库，是数据管道**！

## 练习题

1. 实现一个实时推荐系统
2. 构建日志收集和分析平台
3. 实现消息重试机制
4. 搭建Kafka集群

下一章：Flink实时计算（即将推出）

