---
sidebar_position: 7
title: 
tags: [, , , ]
---

# 



## 

### 

```javascript
const requirements = {
  // 
  scenarios: {
    feed: true,           // 
    related: true,        // 
    personalized: true,   // 
    hotTrending: true     // 
  },
  
  // 
  algorithms: {
    collaborative: true,  // 
    contentBased: true,   // 
    hybrid: true,         // 
    deepLearning: true    // 
  },
  
  // 
  business: {
    realtime: true,       // 
    coldStart: true,      // 
    diversity: true,      // 
    explainability: true  // 
  }
};
```

### 

```python
# 
 = 100_000_000          # 1
 = 10_000_000           # 1000
 = 500_000_000    # 5

# QPS
QPS =  / 86400
QPS = QPS * 3

print(f"QPS: {QPS:,.0f}")
print(f"QPS: {QPS:,.0f}")

# 
P99 = 100                 # 100ms
P95 = 50                  # 50ms
```

## 

### 

```

                                
              

                 

                                
              

                 

                                
                      

```

### 

```javascript
const techStack = {
  // 
  offline: {
    spark: '',
    tensorflow: '',
    pytorch: ''
  },
  
  // 
  online: {
    redis: '',
    faiss: '',
    tensorflow_serving: ''
  },
  
  // 
  storage: {
    hbase: '',
    mysql: '',
    hdfs: ''
  }
};
```

## 

### 

```python
import numpy as np
from scipy.spatial.distance import cosine

class UserBasedCF:
    def __init__(self):
        self.user_item_matrix = None
        self.user_similarity = None
    
    def fit(self, ratings):
        """
        
        ratings: DataFrame with columns [user_id, item_id, rating]
        """
        # -
        self.user_item_matrix = ratings.pivot(
            index='user_id',
            columns='item_id',
            values='rating'
        ).fillna(0)
        
        # 
        self.user_similarity = self.calculate_similarity()
    
    def calculate_similarity(self):
        """"""
        n_users = len(self.user_item_matrix)
        similarity = np.zeros((n_users, n_users))
        
        for i in range(n_users):
            for j in range(i + 1, n_users):
                # 
                sim = 1 - cosine(
                    self.user_item_matrix.iloc[i],
                    self.user_item_matrix.iloc[j]
                )
                similarity[i][j] = sim
                similarity[j][i] = sim
        
        return similarity
    
    def recommend(self, user_id, k=10, n=10):
        """
        
        k: 
        n: 
        """
        # k
        user_idx = self.user_item_matrix.index.get_loc(user_id)
        similar_users = np.argsort(self.user_similarity[user_idx])[-k-1:-1]
        
        # 
        rated_items = set(
            self.user_item_matrix.columns[
                self.user_item_matrix.iloc[user_idx] > 0
            ]
        )
        
        # 
        item_scores = {}
        
        for item in self.user_item_matrix.columns:
            if item in rated_items:
                continue
            
            # 
            weighted_sum = 0
            similarity_sum = 0
            
            for similar_user_idx in similar_users:
                rating = self.user_item_matrix.iloc[similar_user_idx][item]
                if rating > 0:
                    sim = self.user_similarity[user_idx][similar_user_idx]
                    weighted_sum += sim * rating
                    similarity_sum += sim
            
            if similarity_sum > 0:
                item_scores[item] = weighted_sum / similarity_sum
        
        # n
        recommendations = sorted(
            item_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:n]
        
        return recommendations
```

### 

```python
class ItemBasedCF:
    def __init__(self):
        self.user_item_matrix = None
        self.item_similarity = None
    
    def fit(self, ratings):
        """"""
        # -
        self.user_item_matrix = ratings.pivot(
            index='user_id',
            columns='item_id',
            values='rating'
        ).fillna(0)
        
        # 
        self.item_similarity = self.calculate_similarity()
    
    def calculate_similarity(self):
        """"""
        n_items = len(self.user_item_matrix.columns)
        similarity = np.zeros((n_items, n_items))
        
        for i in range(n_items):
            for j in range(i + 1, n_items):
                # 
                sim = 1 - cosine(
                    self.user_item_matrix.iloc[:, i],
                    self.user_item_matrix.iloc[:, j]
                )
                similarity[i][j] = sim
                similarity[j][i] = sim
        
        return similarity
    
    def recommend(self, user_id, n=10):
        """"""
        user_idx = self.user_item_matrix.index.get_loc(user_id)
        user_ratings = self.user_item_matrix.iloc[user_idx]
        
        # 
        rated_items = user_ratings[user_ratings > 0]
        
        # 
        item_scores = {}
        
        for item_idx, item in enumerate(self.user_item_matrix.columns):
            if item in rated_items.index:
                continue
            
            # 
            weighted_sum = 0
            similarity_sum = 0
            
            for rated_item in rated_items.index:
                rated_item_idx = self.user_item_matrix.columns.get_loc(rated_item)
                sim = self.item_similarity[item_idx][rated_item_idx]
                rating = rated_items[rated_item]
                
                weighted_sum += sim * rating
                similarity_sum += abs(sim)
            
            if similarity_sum > 0:
                item_scores[item] = weighted_sum / similarity_sum
        
        # n
        recommendations = sorted(
            item_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:n]
        
        return recommendations
```

## 

### SVD

```python
from scipy.sparse.linalg import svds

class MatrixFactorization:
    def __init__(self, n_factors=50):
        self.n_factors = n_factors
        self.user_factors = None
        self.item_factors = None
    
    def fit(self, ratings):
        """SVD"""
        # 
        user_item_matrix = ratings.pivot(
            index='user_id',
            columns='item_id',
            values='rating'
        ).fillna(0)
        
        # SVD
        U, sigma, Vt = svds(user_item_matrix.values, k=self.n_factors)
        
        # 
        self.user_factors = U
        self.item_factors = Vt.T
        self.sigma = np.diag(sigma)
        
        self.user_ids = user_item_matrix.index
        self.item_ids = user_item_matrix.columns
    
    def predict(self, user_id, item_id):
        """"""
        user_idx = self.user_ids.get_loc(user_id)
        item_idx = self.item_ids.get_loc(item_id)
        
        prediction = np.dot(
            np.dot(self.user_factors[user_idx], self.sigma),
            self.item_factors[item_idx]
        )
        
        return prediction
    
    def recommend(self, user_id, n=10):
        """"""
        user_idx = self.user_ids.get_loc(user_id)
        
        # 
        user_vector = np.dot(self.user_factors[user_idx], self.sigma)
        predictions = np.dot(user_vector, self.item_factors.T)
        
        # n
        top_items = np.argsort(predictions)[-n:][::-1]
        
        return [(self.item_ids[i], predictions[i]) for i in top_items]
```

## 

### Neural Collaborative Filtering

```python
import tensorflow as tf

class NCF(tf.keras.Model):
    def __init__(self, n_users, n_items, embedding_dim=64, layers=[128, 64, 32]):
        super(NCF, self).__init__()
        
        # 
        self.user_embedding = tf.keras.layers.Embedding(
            n_users, embedding_dim
        )
        self.item_embedding = tf.keras.layers.Embedding(
            n_items, embedding_dim
        )
        
        # MLP
        self.mlp_layers = []
        for units in layers:
            self.mlp_layers.append(
                tf.keras.layers.Dense(units, activation='relu')
            )
        
        # 
        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')
    
    def call(self, inputs):
        user_id, item_id = inputs
        
        # 
        user_vec = self.user_embedding(user_id)
        item_vec = self.item_embedding(item_id)
        
        # 
        concat = tf.concat([user_vec, item_vec], axis=-1)
        
        # MLP
        x = concat
        for layer in self.mlp_layers:
            x = layer(x)
        
        # 
        output = self.output_layer(x)
        
        return output

# 
def train_ncf(train_data, n_users, n_items):
    model = NCF(n_users, n_items)
    
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    model.fit(
        [train_data['user_id'], train_data['item_id']],
        train_data['label'],
        epochs=10,
        batch_size=256
    )
    
    return model
```

## 

### 

```javascript
class MultiRecallStrategy {
  constructor() {
    this.strategies = [
      new CollaborativeRecall(),
      new ContentBasedRecall(),
      new HotItemRecall(),
      new UserHistoryRecall()
    ];
  }
  
  async recall(userId, limit = 1000) {
    // 
    const recallPromises = this.strategies.map(strategy =>
      strategy.recall(userId, limit / this.strategies.length)
    );
    
    const results = await Promise.all(recallPromises);
    
    // 
    const itemSet = new Set();
    const items = [];
    
    for (const result of results) {
      for (const item of result) {
        if (!itemSet.has(item.id)) {
          itemSet.add(item.id);
          items.push(item);
        }
      }
    }
    
    return items;
  }
}

// 
class CollaborativeRecall {
  async recall(userId, limit) {
    // 
    const similarUsers = await this.getSimilarUsers(userId, 100);
    
    const items = [];
    for (const similarUser of similarUsers) {
      const userItems = await this.getUserItems(similarUser.id);
      items.push(...userItems);
    }
    
    // 
    return this.deduplicate(items).slice(0, limit);
  }
}

// 
class ContentBasedRecall {
  async recall(userId, limit) {
    // 
    const userTags = await this.getUserTags(userId);
    
    // 
    const items = [];
    for (const tag of userTags) {
      const tagItems = await this.getItemsByTag(tag.name, tag.weight);
      items.push(...tagItems);
    }
    
    return items.slice(0, limit);
  }
}
```

## 

### LR

```python
from sklearn.linear_model import LogisticRegression

class LRRanker:
    def __init__(self):
        self.model = LogisticRegression()
    
    def train(self, features, labels):
        """"""
        self.model.fit(features, labels)
    
    def predict(self, features):
        """"""
        return self.model.predict_proba(features)[:, 1]
    
    def rank(self, items, user_features):
        """"""
        # 
        features = []
        for item in items:
            feature = self.build_features(user_features, item)
            features.append(feature)
        
        # 
        scores = self.predict(features)
        
        # 
        ranked_items = sorted(
            zip(items, scores),
            key=lambda x: x[1],
            reverse=True
        )
        
        return ranked_items
    
    def build_features(self, user_features, item):
        """"""
        features = []
        
        # 
        features.extend([
            user_features['age'],
            user_features['gender'],
            user_features['active_days']
        ])
        
        # 
        features.extend([
            item['popularity'],
            item['quality_score'],
            item['freshness']
        ])
        
        # 
        features.append(
            user_features['age'] * item['popularity']
        )
        
        return features
```

### GBDT+LR

```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import OneHotEncoder

class GBDTLRRanker:
    def __init__(self):
        self.gbdt = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=5
        )
        self.lr = LogisticRegression()
        self.encoder = OneHotEncoder()
    
    def train(self, X_train, y_train):
        """"""
        # GBDT
        self.gbdt.fit(X_train, y_train)
        
        # GBDT
        leaf_indices = self.gbdt.apply(X_train)
        leaf_features = self.encoder.fit_transform(leaf_indices)
        
        # LR
        self.lr.fit(leaf_features, y_train)
    
    def predict(self, X):
        """"""
        leaf_indices = self.gbdt.apply(X)
        leaf_features = self.encoder.transform(leaf_indices)
        return self.lr.predict_proba(leaf_features)[:, 1]
```

## 

### 

```javascript
class ColdStartHandler {
  async handleNewUser(userId) {
    // 1. 
    const hotItems = await this.getHotItems(20);
    
    // 2. 
    const diverseItems = await this.getDiverseItems(20);
    
    // 3. 
    const interestGuide = await this.getInterestGuide();
    
    return {
      recommendations: [...hotItems, ...diverseItems],
      interestGuide
    };
  }
  
  async updateUserProfile(userId, selectedInterests) {
    // 
    await redis.hset(`user:${userId}:profile`, {
      interests: JSON.stringify(selectedInterests),
      coldStart: false
    });
    
    // 
    return await this.getInterestBasedRecommendations(
      userId,
      selectedInterests
    );
  }
}
```

### 

```javascript
class NewItemHandler {
  async handleNewItem(itemId) {
    // 1. 
    const features = await this.extractFeatures(itemId);
    
    // 2. 
    const similarItems = await this.findSimilarItems(features);
    
    // 3. 
    const targetUsers = await this.getTargetUsers(similarItems);
    
    // 4. 
    await this.startABTest(itemId, targetUsers);
  }
  
  async extractFeatures(itemId) {
    const item = await db.items.findOne({ id: itemId });
    
    return {
      category: item.category,
      tags: item.tags,
      price: item.price,
      brand: item.brand
    };
  }
}
```

## 

### 

```javascript
class RealtimeRecommender {
  constructor() {
    this.userBehaviorStream = new KafkaConsumer('user-behavior');
    this.featureStore = new Redis();
  }
  
  async start() {
    // 
    this.userBehaviorStream.on('message', async (message) => {
      const behavior = JSON.parse(message.value);
      await this.processBehavior(behavior);
    });
  }
  
  async processBehavior(behavior) {
    const { userId, itemId, action, timestamp } = behavior;
    
    // 1. 
    await this.updateUserFeatures(userId, behavior);
    
    // 2. 
    await this.updateItemFeatures(itemId, behavior);
    
    // 3. 
    if (action === 'click' || action === 'purchase') {
      await this.triggerRecommendation(userId);
    }
  }
  
  async updateUserFeatures(userId, behavior) {
    // 
    await this.featureStore.hincrby(
      `user:${userId}:realtime`,
      `${behavior.action}_count`,
      1
    );
    
    // 
    await this.featureStore.lpush(
      `user:${userId}:recent_items`,
      behavior.itemId
    );
    await this.featureStore.ltrim(
      `user:${userId}:recent_items`,
      0,
      99
    );
  }
}
```

## 

```python
class RecommenderMetrics:
    @staticmethod
    def precision_at_k(recommended, relevant, k):
        """@K"""
        recommended_k = recommended[:k]
        hits = len(set(recommended_k) & set(relevant))
        return hits / k
    
    @staticmethod
    def recall_at_k(recommended, relevant, k):
        """@K"""
        recommended_k = recommended[:k]
        hits = len(set(recommended_k) & set(relevant))
        return hits / len(relevant) if relevant else 0
    
    @staticmethod
    def ndcg_at_k(recommended, relevant, k):
        """NDCG@K"""
        dcg = 0
        for i, item in enumerate(recommended[:k]):
            if item in relevant:
                dcg += 1 / np.log2(i + 2)
        
        idcg = sum(1 / np.log2(i + 2) for i in range(min(k, len(relevant))))
        
        return dcg / idcg if idcg > 0 else 0
    
    @staticmethod
    def diversity(recommended):
        """"""
        categories = [item['category'] for item in recommended]
        return len(set(categories)) / len(categories)
```

## 


-  ****CF + CF + 
-  ****NCF + Wide&Deep + DeepFM
-  **** + 
-  **** + 
-  **** + 
-  **** +  + NDCG

****

