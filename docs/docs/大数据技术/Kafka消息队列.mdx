---
sidebar_position: 3
title: Kafka - 
---

# Kafka - 

Kafka**Kafka**

## Kafka

### Kafka

****10

****
```python
#  - 
for event in events:
    db.insert(event)  # 
```

**Kafka**
```python
# Kafka
producer.send('user_events', event)  # 

# 
consumer1.subscribe(['user_events'])  # 
consumer2.subscribe(['user_events'])  # 
consumer3.subscribe(['user_events'])  # 
```

### Kafka

```

          Producer          
                        

              ↓

          Kafka Cluster              
   
   Topic 1   Topic 2   Topic 3   
   Part 0    Part 0    Part 0    
   Part 1    Part 1    Part 1    
   

              ↓

         Consumer           
                        

```

****
- **Topic**
- **Partition**Topic
- **Producer**
- **Consumer**
- **Consumer Group**

## 

### DockerKafka

```yaml
# docker-compose.yml
version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
```

```bash
# 
docker-compose up -d

# 
docker-compose logs -f kafka

# 
docker exec -it kafka bash

# Topic
kafka-topics --create --topic test --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1

# Topic
kafka-topics --list --bootstrap-server localhost:9092

# Topic
kafka-topics --describe --topic test --bootstrap-server localhost:9092
```

### PythonKafka

****
```bash
pip install kafka-python
```

****
```python
from kafka import KafkaProducer
import json
import time

# 
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# 
for i in range(10):
    message = {
        'user_id': f'user_{i}',
        'action': 'click',
        'timestamp': time.time()
    }
    
    # topic
    future = producer.send('user_events', message)
    
    # 
    result = future.get(timeout=10)
    print(f": partition={result.partition}, offset={result.offset}")
    
    time.sleep(1)

# 
producer.flush()
producer.close()
```

****
```python
from kafka import KafkaConsumer
import json

# 
consumer = KafkaConsumer(
    'user_events',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',  # 
    enable_auto_commit=True,
    group_id='my-group',
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

# 
print("...")
for message in consumer:
    print(f": {message.value}")
    print(f"  partition: {message.partition}")
    print(f"  offset: {message.offset}")
    print(f"  timestamp: {message.timestamp}")
```

## Kafka

### 1. Partition

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# 
producer.send('user_events', value={'msg': 'test'}, partition=0)

# keykey
for i in range(10):
    user_id = f'user_{i % 3}'  # 3
    producer.send(
        'user_events',
        key=user_id.encode('utf-8'),
        value={'user_id': user_id, 'action': 'click'}
    )

producer.flush()
producer.close()
```

****
- 
- 
- 

### 2. Consumer Group

```python
from kafka import KafkaConsumer
import json
import threading

def consume(consumer_id, group_id):
    """"""
    consumer = KafkaConsumer(
        'user_events',
        bootstrap_servers=['localhost:9092'],
        group_id=group_id,
        auto_offset_reset='earliest',
        value_deserializer=lambda m: json.loads(m.decode('utf-8'))
    )
    
    print(f" {consumer_id} ")
    
    for message in consumer:
        print(f"[{consumer_id}] partition={message.partition}, offset={message.offset}")
        # 
        time.sleep(0.1)

# 3
threads = []
for i in range(3):
    t = threading.Thread(target=consume, args=(i, 'my-group'))
    t.start()
    threads.append(t)

for t in threads:
    t.join()
```

****
- 
- 
- 

### 3. Offset

```python
from kafka import KafkaConsumer, TopicPartition

consumer = KafkaConsumer(
    bootstrap_servers=['localhost:9092'],
    group_id='my-group',
    enable_auto_commit=False  # 
)

# topic
consumer.subscribe(['user_events'])

# 
for message in consumer:
    print(f": {message.value}")
    
    # 
    try:
        process_message(message)
        
        # offset
        consumer.commit()
        print(f"offset: {message.offset}")
    except Exception as e:
        print(f": {e}")
        # 

# offset
partitions = consumer.assignment()
for partition in partitions:
    offset = consumer.position(partition)
    print(f"Partition {partition.partition}: offset={offset}")

# offset
tp = TopicPartition('user_events', 0)
consumer.seek(tp, 100)  # offset 100
```

## 

### 1

****
```
 → Kafka → Spark Streaming → Redis → Dashboard
```

****

```python
from kafka import KafkaProducer
import json
import time
import random

class UserBehaviorProducer:
    """"""
    
    def __init__(self):
        self.producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        self.actions = ['view', 'click', 'cart', 'purchase']
        self.items = [f'item_{i}' for i in range(100)]
        self.users = [f'user_{i}' for i in range(1000)]
    
    def generate_event(self):
        """"""
        return {
            'user_id': random.choice(self.users),
            'action': random.choice(self.actions),
            'item_id': random.choice(self.items),
            'timestamp': int(time.time() * 1000),
            'price': random.randint(10, 1000)
        }
    
    def run(self, events_per_second=100):
        """"""
        print(f": {events_per_second}/")
        
        interval = 1.0 / events_per_second
        
        try:
            while True:
                event = self.generate_event()
                self.producer.send('user_behavior', event)
                time.sleep(interval)
        except KeyboardInterrupt:
            print("")
        finally:
            self.producer.close()

# 
if __name__ == '__main__':
    producer = UserBehaviorProducer()
    producer.run(events_per_second=100)
```

****

```python
from kafka import KafkaConsumer
import json
from collections import defaultdict
import time
import threading

class RealTimeAnalyzer:
    """"""
    
    def __init__(self):
        self.consumer = KafkaConsumer(
            'user_behavior',
            bootstrap_servers=['localhost:9092'],
            group_id='analyzer',
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
        # 
        self.stats = {
            'total_events': 0,
            'action_counts': defaultdict(int),
            'revenue': 0,
            'active_users': set()
        }
        
        self.lock = threading.Lock()
    
    def process_event(self, event):
        """"""
        with self.lock:
            self.stats['total_events'] += 1
            self.stats['action_counts'][event['action']] += 1
            self.stats['active_users'].add(event['user_id'])
            
            if event['action'] == 'purchase':
                self.stats['revenue'] += event['price']
    
    def print_stats(self):
        """"""
        while True:
            time.sleep(5)
            
            with self.lock:
                print("\n" + "="*50)
                print(f": {self.stats['total_events']}")
                print(f": {len(self.stats['active_users'])}")
                print(f": ¥{self.stats['revenue']}")
                print("\n:")
                for action, count in self.stats['action_counts'].items():
                    percentage = count / self.stats['total_events'] * 100
                    print(f"  {action}: {count} ({percentage:.1f}%)")
                print("="*50)
    
    def run(self):
        """"""
        # 
        stats_thread = threading.Thread(target=self.print_stats, daemon=True)
        stats_thread.start()
        
        # 
        print("...")
        for message in self.consumer:
            self.process_event(message.value)

# 
if __name__ == '__main__':
    analyzer = RealTimeAnalyzer()
    analyzer.run()
```

### 2

```python
import logging
from kafka import KafkaProducer
import json
import socket

class KafkaLogHandler(logging.Handler):
    """Kafka"""
    
    def __init__(self, topic, bootstrap_servers=['localhost:9092']):
        super().__init__()
        
        self.topic = topic
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        self.hostname = socket.gethostname()
    
    def emit(self, record):
        """Kafka"""
        try:
            log_entry = {
                'timestamp': record.created,
                'level': record.levelname,
                'logger': record.name,
                'message': record.getMessage(),
                'hostname': self.hostname,
                'module': record.module,
                'function': record.funcName,
                'line': record.lineno
            }
            
            if record.exc_info:
                log_entry['exception'] = self.format(record)
            
            self.producer.send(self.topic, log_entry)
        except Exception as e:
            print(f": {e}")
    
    def close(self):
        """"""
        self.producer.close()
        super().close()

# 
logger = logging.getLogger('my_app')
logger.setLevel(logging.INFO)

# Kafka
kafka_handler = KafkaLogHandler('app_logs')
logger.addHandler(kafka_handler)

# 
logger.info("")
logger.warning(": 80%")
logger.error("")

try:
    1 / 0
except Exception as e:
    logger.exception("")
```

****

```python
from kafka import KafkaConsumer
import json

class LogMonitor:
    """"""
    
    def __init__(self):
        self.consumer = KafkaConsumer(
            'app_logs',
            bootstrap_servers=['localhost:9092'],
            group_id='log_monitor',
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
        self.error_count = 0
        self.warning_count = 0
    
    def process_log(self, log):
        """"""
        level = log['level']
        message = log['message']
        
        # 
        colors = {
            'INFO': '\033[92m',     # 
            'WARNING': '\033[93m',  # 
            'ERROR': '\033[91m',    # 
            'CRITICAL': '\033[95m'  # 
        }
        reset = '\033[0m'
        
        color = colors.get(level, '')
        print(f"{color}[{level}] {log['hostname']} - {message}{reset}")
        
        # 
        if level == 'ERROR' or level == 'CRITICAL':
            self.error_count += 1
            if self.error_count >= 10:
                self.send_alert("")
                self.error_count = 0
        
        elif level == 'WARNING':
            self.warning_count += 1
    
    def send_alert(self, message):
        """"""
        print(f"\n : {message}\n")
    
    def run(self):
        """"""
        print("...")
        for message in self.consumer:
            self.process_log(message.value)

# 
if __name__ == '__main__':
    monitor = LogMonitor()
    monitor.run()
```

## 

### 1. 

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    # 
    batch_size=16384,  # 16KB
    linger_ms=10,      # 10ms
    compression_type='gzip'  # 
)

# 
for i in range(10000):
    producer.send('test', {'id': i})

# 
producer.flush()
```

### 2. 

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

def on_send_success(record_metadata):
    """"""
    print(f": topic={record_metadata.topic}, partition={record_metadata.partition}")

def on_send_error(excp):
    """"""
    print(f": {excp}")

# 
for i in range(100):
    producer.send('test', {'id': i}) \
        .add_callback(on_send_success) \
        .add_errback(on_send_error)

producer.flush()
```

### 3. 

```python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'test',
    bootstrap_servers=['localhost:9092'],
    group_id='my-group',
    # 
    fetch_min_bytes=1024,      # 1KB
    fetch_max_wait_ms=500,     # 500ms
    max_poll_records=500,      # 500
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

# 
batch = []
batch_size = 100

for message in consumer:
    batch.append(message.value)
    
    if len(batch) >= batch_size:
        # 
        process_batch(batch)
        batch = []
        
        # offset
        consumer.commit()
```

## 

Kafka

1. ****
2. ****
3. ****
4. ****

**Kafka**

## 

1. 
2. 
3. 
4. Kafka

Flink

