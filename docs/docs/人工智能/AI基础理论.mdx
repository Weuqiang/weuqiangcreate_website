---
sidebar_position: 0
title: AI
---

# 

:::info 
AI
:::

## 

**Artificial Intelligence, AI** 

### AI

```mermaid
graph TD
    A[ AI] --> B[ ML]
    B --> C[ DL]
    
    A --> D[]
    A --> E[]
    
    B --> F[]
    B --> G[]
    B --> H[]
    
    C --> I[ CNN]
    C --> J[ RNN]
    C --> K[Transformer]
```

**1. AI**
- 
- ""
- 

**2. ML**
- AI
- 
- SVM

**3. DL**
- 
- 
- CNNRNNTransformer

## AI

### 1956-1980s

**1956 - AI**
- ""
- 

****
- 
- MYCIN
- 

****
- 
- 
- 

### 1980s-2000s

**1986 - **
- RumelhartBP
- 

****
- LeNet-51998
- SVM
- 

**AI**
- 1990s
- 
- 

### 2006-

**2006 - **
- HintonDBN
- 

**2012 - ImageNet**
- AlexNetImageNet
- 

****

|  |  |  |
|------|------|------|
| 2014 | GAN | AI |
| 2014 | Seq2Seq |  |
| 2017 | Transformer | NLP |
| 2018 | BERT | NLP |
| 2020 | GPT-3 |  |
| 2022 | ChatGPT | AI |
| 2023 | GPT-4Claude |  |

### AGI

**AGI**
- 
- 
- 

## AI

### 1. Agent



```python
class Agent:
    def __init__(self):
        self.state = None
    
    def perceive(self, environment):
        """"""
        self.state = environment.get_state()
    
    def decide(self):
        """"""
        action = self.policy(self.state)
        return action
    
    def act(self, action, environment):
        """"""
        environment.apply_action(action)
```

****
- ****
- ****
- ****
- ****
- ****

### 2. 

****
- 
- 
- 
- 
- 

****

```python
# BFS
def bfs(start, goal):
    queue = [start]
    visited = {start}
    
    while queue:
        node = queue.pop(0)
        if node == goal:
            return True
        
        for neighbor in node.neighbors:
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append(neighbor)
    
    return False

# DFS
def dfs(node, goal, visited=None):
    if visited is None:
        visited = set()
    
    if node == goal:
        return True
    
    visited.add(node)
    for neighbor in node.neighbors:
        if neighbor not in visited:
            if dfs(neighbor, goal, visited):
                return True
    
    return False

# A*
import heapq

def a_star(start, goal, heuristic):
    open_set = [(0, start)]
    came_from = {}
    g_score = {start: 0}
    
    while open_set:
        _, current = heapq.heappop(open_set)
        
        if current == goal:
            return reconstruct_path(came_from, current)
        
        for neighbor in current.neighbors:
            tentative_g = g_score[current] + cost(current, neighbor)
            
            if neighbor not in g_score or tentative_g < g_score[neighbor]:
                came_from[neighbor] = current
                g_score[neighbor] = tentative_g
                f_score = tentative_g + heuristic(neighbor, goal)
                heapq.heappush(open_set, (f_score, neighbor))
    
    return None
```

### 3. 

****

****
```python
# 
# P: 
# Q: 
# : P → Q ()

class PropositionalLogic:
    def __init__(self):
        self.facts = set()
        self.rules = []
    
    def add_fact(self, fact):
        self.facts.add(fact)
    
    def add_rule(self, premise, conclusion):
        self.rules.append((premise, conclusion))
    
    def infer(self):
        """"""
        changed = True
        while changed:
            changed = False
            for premise, conclusion in self.rules:
                if premise in self.facts and conclusion not in self.facts:
                    self.facts.add(conclusion)
                    changed = True
```

****
```prolog
% Prolog
% 
parent(tom, bob).
parent(tom, liz).
parent(bob, ann).

% 
grandparent(X, Z) :- parent(X, Y), parent(Y, Z).

% 
?- grandparent(tom, ann).
% : true
```

### 4. 

****

```python
import numpy as np

class BayesianNetwork:
    """"""
    
    def __init__(self):
        # P(Rain) = 0.2
        self.p_rain = 0.2
        
        # P(Sprinkler|Rain)
        self.p_sprinkler_given_rain = {
            True: 0.01,   # 
            False: 0.4    # 
        }
        
        # P(GrassWet|Rain, Sprinkler)
        self.p_grass_wet = {
            (True, True): 0.99,
            (True, False): 0.8,
            (False, True): 0.9,
            (False, False): 0.0
        }
    
    def query(self, grass_wet=True):
        """"""
        # P(Rain|GrassWet) = P(GrassWet|Rain) * P(Rain) / P(GrassWet)
        
        #  P(GrassWet|Rain)
        p_wet_given_rain = 0
        for sprinkler in [True, False]:
            p_sprinkler = self.p_sprinkler_given_rain[True]
            if not sprinkler:
                p_sprinkler = 1 - p_sprinkler
            p_wet_given_rain += (
                self.p_grass_wet[(True, sprinkler)] * p_sprinkler
            )
        
        #  P(GrassWet|NoRain)
        p_wet_given_no_rain = 0
        for sprinkler in [True, False]:
            p_sprinkler = self.p_sprinkler_given_rain[False]
            if not sprinkler:
                p_sprinkler = 1 - p_sprinkler
            p_wet_given_no_rain += (
                self.p_grass_wet[(False, sprinkler)] * p_sprinkler
            )
        
        #  P(GrassWet)
        p_grass_wet = (
            p_wet_given_rain * self.p_rain +
            p_wet_given_no_rain * (1 - self.p_rain)
        )
        
        # 
        p_rain_given_wet = (
            p_wet_given_rain * self.p_rain / p_grass_wet
        )
        
        return p_rain_given_wet

# 
bn = BayesianNetwork()
prob = bn.query(grass_wet=True)
print(f": {prob:.2%}")
```

## AI

### 1. 

****

```python
import numpy as np

# 
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

# 
print(v1 + v2)  # [5 7 9]

# 
print(np.dot(v1, v2))  # 32

# 
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# 
print(A @ B)
# [[19 22]
#  [43 50]]

# 
print(A.T)
# [[1 3]
#  [2 4]]

# 
eigenvalues, eigenvectors = np.linalg.eig(A)
print(":", eigenvalues)
print(":\n", eigenvectors)
```

****
- 
- 
- 

### 2. 

****

```python
import numpy as np
from scipy import stats

# 
# 
mu, sigma = 0, 1
x = np.linspace(-3, 3, 100)
pdf = stats.norm.pdf(x, mu, sigma)

# 
data = np.random.randn(1000)
mean = np.mean(data)
variance = np.var(data)
std = np.std(data)

print(f": {mean:.2f}")
print(f": {variance:.2f}")
print(f": {std:.2f}")

# 
# P(A|B) = P(A∩B) / P(B)
def conditional_probability(p_a_and_b, p_b):
    return p_a_and_b / p_b

# 
# P(A|B) = P(B|A) * P(A) / P(B)
def bayes_theorem(p_b_given_a, p_a, p_b):
    return (p_b_given_a * p_a) / p_b
```

****
- 
- 
- 

### 3. 

****

```python
import numpy as np

# 
def numerical_gradient(f, x, h=1e-5):
    """fx"""
    grad = np.zeros_like(x)
    
    for i in range(x.size):
        tmp = x[i]
        
        # f(x+h)
        x[i] = tmp + h
        fxh1 = f(x)
        
        # f(x-h)
        x[i] = tmp - h
        fxh2 = f(x)
        
        # 
        grad[i] = (fxh1 - fxh2) / (2 * h)
        x[i] = tmp
    
    return grad

#  f(x,y) = x^2 + y^2 (3,4)
def f(x):
    return x[0]**2 + x[1]**2

x = np.array([3.0, 4.0])
grad = numerical_gradient(f, x)
print(f": {grad}")  # [6. 8.]
```

****

$$
\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}
$$

```python
# 
# y = (x^2 + 1)^3
#  u = x^2 + 1,  y = u^3
# dy/dx = dy/du * du/dx = 3u^2 * 2x = 6x(x^2 + 1)^2

def chain_rule_example(x):
    u = x**2 + 1
    y = u**3
    
    # 
    dy_du = 3 * u**2
    du_dx = 2 * x
    dy_dx = dy_du * du_dx
    
    return dy_dx

x = 2.0
gradient = chain_rule_example(x)
print(f"x={x}: {gradient}")
```

****
- 
- 
- 

### 4. 

**Entropy**

```python
import numpy as np

def entropy(probabilities):
    """ H(X) = -Σ p(x) log p(x)"""
    # 0log(0)
    p = probabilities[probabilities > 0]
    return -np.sum(p * np.log2(p))

# 
p_fair = np.array([0.5, 0.5])  # 
p_biased = np.array([0.9, 0.1])  # 

print(f": {entropy(p_fair):.2f} bits")  # 1.00
print(f": {entropy(p_biased):.2f} bits")  # 0.47
```

**Cross Entropy**

```python
def cross_entropy(p, q):
    """
     H(p,q) = -Σ p(x) log q(x)
    p: 
    q: 
    """
    return -np.sum(p * np.log(q + 1e-10))  # log(0)

# 
y_true = np.array([1, 0, 0])  # one-hot
y_pred = np.array([0.7, 0.2, 0.1])  # 

loss = cross_entropy(y_true, y_pred)
print(f": {loss:.4f}")
```

**KLKullback-Leibler Divergence**

```python
def kl_divergence(p, q):
    """
    KL D_KL(p||q) = Σ p(x) log(p(x)/q(x))
    
    """
    return np.sum(p * np.log((p + 1e-10) / (q + 1e-10)))

p = np.array([0.5, 0.3, 0.2])
q = np.array([0.4, 0.4, 0.2])

kl = kl_divergence(p, q)
print(f"KL: {kl:.4f}")
```

****
- 
- 
- KL

## AI

### 1. 

**Supervised Learning**
- 
- 
- 

**Unsupervised Learning**
- 
- 
- 

**Reinforcement Learning**
- 
- 
- AI

**Semi-Supervised Learning**
-  + 
- 

**Self-Supervised Learning**
- 
- BERT

### 2. -

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from sklearn.tree import DecisionTreeRegressor

# 
np.random.seed(0)
X = np.sort(np.random.rand(100, 1) * 10, axis=0)
y = np.sin(X).ravel() + np.random.randn(100) * 0.1

# 
models = {
    '': DecisionTreeRegressor(max_depth=1),
    '': DecisionTreeRegressor(max_depth=3),
    '': DecisionTreeRegressor(max_depth=10)
}

# 
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

for ax, (title, model) in zip(axes, models.items()):
    model.fit(X, y)
    X_test = np.linspace(0, 10, 100).reshape(-1, 1)
    y_pred = model.predict(X_test)
    
    ax.scatter(X, y, alpha=0.5)
    ax.plot(X_test, y_pred, 'r-', linewidth=2)
    ax.set_title(title)
    ax.set_xlabel('X')
    ax.set_ylabel('y')

plt.tight_layout()
plt.show()
```

****
- **Bias**
- **Variance**
- ****

### 3. 

**L1Lasso**

$$
Loss = MSE + \lambda \sum_{i=1}^{n} |w_i|
$$

**L2Ridge**

$$
Loss = MSE + \lambda \sum_{i=1}^{n} w_i^2
$$

```python
from sklearn.linear_model import Ridge, Lasso
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# 
X, y = make_regression(n_samples=100, n_features=20, noise=10)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# L2
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
print(f"Ridge R²: {ridge.score(X_test, y_test):.4f}")

# L1
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
print(f"Lasso R²: {lasso.score(X_test, y_test):.4f}")

# 
print(f": {np.sum(lasso.coef_ != 0)}")
```

### 4. 

```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# 
iris = load_iris()
X, y = iris.data, iris.target

# 
model = RandomForestClassifier(n_estimators=100)

# K
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')

print(f": {scores}")
print(f": {scores.mean():.4f} (+/- {scores.std():.4f})")
```

## 

### 1. 

```mermaid
graph LR
    A[] --> B[]
    B --> C[]
    C --> D[]
    
    D --> E[]
    D --> F[]
    D --> G[]
    D --> H[]
```

### 2. 

****
1. 
2. 
3. 
4. PyTorch/TensorFlow
5. Kaggle

### 3. 

****
- - 
- - Ian Goodfellow
- - 
- Pattern Recognition and Machine Learning- Christopher Bishop

****
- Andrew NgCoursera
- 
- Fast.ai
- Stanford CS229/CS230

****
- arXiv.org
- Papers with Code
- Google Scholar

## 

AI

****
1. AI
2. AI
3. -
4. 
5. 

****
- 
- 
- 
- 

<DocCardList />

