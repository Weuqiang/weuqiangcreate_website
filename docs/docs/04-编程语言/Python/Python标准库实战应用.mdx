---
sidebar_position: 2
title: Python
tags: [Python, , , ]
---

# Python

## 

Python

## 

### 1. 

```python
import os
import pathlib
from glob import glob
import shutil

class FileProcessor:
    """"""
    
    def __init__(self, base_dir):
        self.base_dir = pathlib.Path(base_dir)
    
    def find_files(self, pattern='*.txt'):
        """"""
        # 1 glob
        files = glob(str(self.base_dir / '**' / pattern), recursive=True)
        
        # 2 pathlib
        files = list(self.base_dir.rglob(pattern))
        
        return files
    
    def batch_rename(self, pattern, replacement):
        """"""
        for file in self.base_dir.rglob('*'):
            if file.is_file():
                new_name = file.name.replace(pattern, replacement)
                if new_name != file.name:
                    file.rename(file.parent / new_name)
                    print(f': {file.name} -> {new_name}')
    
    def organize_by_extension(self):
        """"""
        for file in self.base_dir.rglob('*'):
            if file.is_file():
                ext = file.suffix[1:] or 'no_extension'
                target_dir = self.base_dir / ext
                target_dir.mkdir(exist_ok=True)
                
                target_file = target_dir / file.name
                shutil.move(str(file), str(target_file))
                print(f': {file} -> {target_file}')
    
    def get_file_info(self, filepath):
        """"""
        stat = os.stat(filepath)
        return {
            'size': stat.st_size,
            'created': stat.st_ctime,
            'modified': stat.st_mtime,
            'accessed': stat.st_atime,
        }
    
    def clean_old_files(self, days=30):
        """"""
        import time
        current_time = time.time()
        cutoff_time = current_time - (days * 86400)
        
        for file in self.base_dir.rglob('*'):
            if file.is_file():
                if os.path.getmtime(file) < cutoff_time:
                    file.unlink()
                    print(f': {file}')

# 
processor = FileProcessor('/path/to/files')
processor.organize_by_extension()
processor.clean_old_files(days=30)
```

### 2. 

```python
import tempfile
import shutil
from contextlib import contextmanager

class TempFileManager:
    """"""
    
    @contextmanager
    def temp_file(self, suffix='', prefix='tmp', text=True):
        """"""
        fd, path = tempfile.mkstemp(suffix=suffix, prefix=prefix, text=text)
        try:
            yield path
        finally:
            os.close(fd)
            os.unlink(path)
    
    @contextmanager
    def temp_directory(self):
        """"""
        temp_dir = tempfile.mkdtemp()
        try:
            yield temp_dir
        finally:
            shutil.rmtree(temp_dir)
    
    def process_large_file(self, input_file, output_file):
        """"""
        with self.temp_file() as temp_path:
            # 
            with open(input_file, 'r') as f_in, open(temp_path, 'w') as f_temp:
                for line in f_in:
                    processed_line = line.upper()
                    f_temp.write(processed_line)
            
            # 
            shutil.move(temp_path, output_file)

# 
manager = TempFileManager()

with manager.temp_file(suffix='.txt') as temp_path:
    with open(temp_path, 'w') as f:
        f.write('')
    # 

with manager.temp_directory() as temp_dir:
    # 
    temp_file = os.path.join(temp_dir, 'data.txt')
    with open(temp_file, 'w') as f:
        f.write('')
    # 
```

## 

### 1. CSV

```python
import csv
from collections import defaultdict, Counter
from typing import List, Dict

class CSVProcessor:
    """CSV"""
    
    def read_csv(self, filepath, encoding='utf-8'):
        """CSV"""
        with open(filepath, 'r', encoding=encoding) as f:
            reader = csv.DictReader(f)
            return list(reader)
    
    def write_csv(self, filepath, data, fieldnames=None, encoding='utf-8'):
        """CSV"""
        if not data:
            return
        
        if fieldnames is None:
            fieldnames = data[0].keys()
        
        with open(filepath, 'w', encoding=encoding, newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
    
    def filter_data(self, data, condition):
        """"""
        return [row for row in data if condition(row)]
    
    def group_by(self, data, key):
        """"""
        groups = defaultdict(list)
        for row in data:
            groups[row[key]].append(row)
        return dict(groups)
    
    def aggregate(self, data, group_key, agg_key, agg_func='sum'):
        """"""
        groups = self.group_by(data, group_key)
        result = {}
        
        for key, rows in groups.items():
            values = [float(row[agg_key]) for row in rows]
            if agg_func == 'sum':
                result[key] = sum(values)
            elif agg_func == 'avg':
                result[key] = sum(values) / len(values)
            elif agg_func == 'count':
                result[key] = len(values)
        
        return result
    
    def merge_csv(self, file1, file2, output_file, join_key):
        """CSV"""
        data1 = self.read_csv(file1)
        data2 = self.read_csv(file2)
        
        # 
        index2 = {row[join_key]: row for row in data2}
        
        # 
        merged = []
        for row1 in data1:
            key = row1[join_key]
            if key in index2:
                merged_row = {**row1, **index2[key]}
                merged.append(merged_row)
        
        self.write_csv(output_file, merged)
        return merged

# 
processor = CSVProcessor()

# 
sales = processor.read_csv('sales.csv')

# 
high_price = processor.filter_data(sales, lambda x: float(x['price']) > 100)

# 
by_category = processor.group_by(sales, 'category')

# 
category_sales = processor.aggregate(sales, 'category', 'amount', 'sum')
```

### 2. JSON

```python
import json
from typing import Any, Dict, List

class JSONProcessor:
    """JSON"""
    
    def read_json(self, filepath, encoding='utf-8'):
        """JSON"""
        with open(filepath, 'r', encoding=encoding) as f:
            return json.load(f)
    
    def write_json(self, filepath, data, indent=2, encoding='utf-8'):
        """JSON"""
        with open(filepath, 'w', encoding=encoding) as f:
            json.dump(data, f, indent=indent, ensure_ascii=False)
    
    def pretty_print(self, data):
        """JSON"""
        print(json.dumps(data, indent=2, ensure_ascii=False))
    
    def flatten_json(self, data, parent_key='', sep='_'):
        """JSON"""
        items = []
        for k, v in data.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else k
            if isinstance(v, dict):
                items.extend(self.flatten_json(v, new_key, sep=sep).items())
            else:
                items.append((new_key, v))
        return dict(items)
    
    def unflatten_json(self, data, sep='_'):
        """JSON"""
        result = {}
        for key, value in data.items():
            parts = key.split(sep)
            d = result
            for part in parts[:-1]:
                if part not in d:
                    d[part] = {}
                d = d[part]
            d[parts[-1]] = value
        return result
    
    def merge_json(self, *jsons):
        """JSON"""
        result = {}
        for data in jsons:
            self._deep_merge(result, data)
        return result
    
    def _deep_merge(self, base, update):
        """"""
        for key, value in update.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                self._deep_merge(base[key], value)
            else:
                base[key] = value

# 
processor = JSONProcessor()

# 
config = processor.read_json('config.json')

# 
flat_config = processor.flatten_json(config)
print(flat_config)
# {'database_host': 'localhost', 'database_port': 3306}

# 
default_config = {'timeout': 30, 'retry': 3}
user_config = {'timeout': 60}
final_config = processor.merge_json(default_config, user_config)
```

## 

### 1. HTTP

```python
import urllib.request
import urllib.parse
import json
from typing import Dict, Optional

class HTTPClient:
    """HTTP"""
    
    def __init__(self, base_url='', timeout=30):
        self.base_url = base_url
        self.timeout = timeout
        self.headers = {
            'User-Agent': 'Python-HTTPClient/1.0'
        }
    
    def get(self, url, params=None, headers=None):
        """GET"""
        if params:
            url = f"{url}?{urllib.parse.urlencode(params)}"
        
        full_url = urllib.parse.urljoin(self.base_url, url)
        req_headers = {**self.headers, **(headers or {})}
        
        request = urllib.request.Request(full_url, headers=req_headers)
        
        try:
            with urllib.request.urlopen(request, timeout=self.timeout) as response:
                return {
                    'status': response.status,
                    'headers': dict(response.headers),
                    'body': response.read().decode('utf-8')
                }
        except urllib.error.HTTPError as e:
            return {
                'status': e.code,
                'error': str(e)
            }
    
    def post(self, url, data=None, json_data=None, headers=None):
        """POST"""
        full_url = urllib.parse.urljoin(self.base_url, url)
        req_headers = {**self.headers, **(headers or {})}
        
        if json_data:
            body = json.dumps(json_data).encode('utf-8')
            req_headers['Content-Type'] = 'application/json'
        elif data:
            body = urllib.parse.urlencode(data).encode('utf-8')
            req_headers['Content-Type'] = 'application/x-www-form-urlencoded'
        else:
            body = None
        
        request = urllib.request.Request(
            full_url,
            data=body,
            headers=req_headers,
            method='POST'
        )
        
        try:
            with urllib.request.urlopen(request, timeout=self.timeout) as response:
                return {
                    'status': response.status,
                    'headers': dict(response.headers),
                    'body': response.read().decode('utf-8')
                }
        except urllib.error.HTTPError as e:
            return {
                'status': e.code,
                'error': str(e)
            }
    
    def download_file(self, url, filepath):
        """"""
        urllib.request.urlretrieve(url, filepath)

# 
client = HTTPClient(base_url='https://api.example.com')

# GET
response = client.get('/users', params={'page': 1, 'limit': 10})
print(response['body'])

# POST
response = client.post('/users', json_data={'name': 'John', 'age': 30})

# 
client.download_file('https://example.com/file.pdf', 'downloaded.pdf')
```

### 2. HTTP

```python
from http.server import HTTPServer, BaseHTTPRequestHandler
import json
from urllib.parse import urlparse, parse_qs

class APIHandler(BaseHTTPRequestHandler):
    """API"""
    
    def do_GET(self):
        """GET"""
        parsed_path = urlparse(self.path)
        path = parsed_path.path
        params = parse_qs(parsed_path.query)
        
        if path == '/api/users':
            self.send_json_response({
                'users': [
                    {'id': 1, 'name': 'Alice'},
                    {'id': 2, 'name': 'Bob'}
                ]
            })
        elif path == '/api/health':
            self.send_json_response({'status': 'ok'})
        else:
            self.send_error(404, 'Not Found')
    
    def do_POST(self):
        """POST"""
        content_length = int(self.headers['Content-Length'])
        body = self.rfile.read(content_length)
        
        try:
            data = json.loads(body.decode('utf-8'))
            
            # 
            response = {
                'success': True,
                'data': data
            }
            self.send_json_response(response)
        except json.JSONDecodeError:
            self.send_error(400, 'Invalid JSON')
    
    def send_json_response(self, data, status=200):
        """JSON"""
        self.send_response(status)
        self.send_header('Content-Type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(data).encode('utf-8'))

# 
def run_server(port=8000):
    server = HTTPServer(('localhost', port), APIHandler)
    print(f' http://localhost:{port}')
    server.serve_forever()

# run_server()
```

## 

### 1. 

```python
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

class ThreadPool:
    """"""
    
    def __init__(self, max_workers=5):
        self.max_workers = max_workers
    
    def map_tasks(self, func, items):
        """"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            results = list(executor.map(func, items))
        return results
    
    def submit_tasks(self, func, items):
        """"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(func, item) for item in items]
            results = []
            for future in as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    print(f': {e}')
        return results
    
    def producer_consumer(self, producer_func, consumer_func, num_consumers=3):
        """-"""
        task_queue = queue.Queue()
        
        def producer():
            for item in producer_func():
                task_queue.put(item)
            # 
            for _ in range(num_consumers):
                task_queue.put(None)
        
        def consumer():
            while True:
                item = task_queue.get()
                if item is None:
                    break
                consumer_func(item)
                task_queue.task_done()
        
        # 
        producer_thread = threading.Thread(target=producer)
        producer_thread.start()
        
        # 
        consumer_threads = []
        for _ in range(num_consumers):
            t = threading.Thread(target=consumer)
            t.start()
            consumer_threads.append(t)
        
        # 
        producer_thread.join()
        for t in consumer_threads:
            t.join()

# 
pool = ThreadPool(max_workers=10)

# 
def download_file(url):
    print(f': {url}')
    time.sleep(1)  # 
    return f'Downloaded: {url}'

urls = [f'http://example.com/file{i}.txt' for i in range(20)]
results = pool.map_tasks(download_file, urls)

# -
def produce_tasks():
    for i in range(100):
        yield f'task-{i}'

def consume_task(task):
    print(f': {task}')
    time.sleep(0.1)

pool.producer_consumer(produce_tasks, consume_task, num_consumers=5)
```

### 2. 

```python
from multiprocessing import Pool, Process, Queue, Manager
import os

class ProcessPool:
    """"""
    
    def __init__(self, processes=None):
        self.processes = processes or os.cpu_count()
    
    def map_tasks(self, func, items):
        """"""
        with Pool(processes=self.processes) as pool:
            results = pool.map(func, items)
        return results
    
    def starmap_tasks(self, func, items):
        """"""
        with Pool(processes=self.processes) as pool:
            results = pool.starmap(func, items)
        return results
    
    def async_tasks(self, func, items):
        """"""
        with Pool(processes=self.processes) as pool:
            async_results = [pool.apply_async(func, (item,)) for item in items]
            results = [ar.get() for ar in async_results]
        return results

# CPU
def cpu_intensive_task(n):
    """"""
    result = 0
    for i in range(n):
        result += i ** 2
    return result

# 
pool = ProcessPool(processes=4)

# 
numbers = [1000000, 2000000, 3000000, 4000000]
results = pool.map_tasks(cpu_intensive_task, numbers)
print(results)

# 
def multiply(x, y):
    return x * y

tasks = [(1, 2), (3, 4), (5, 6)]
results = pool.starmap_tasks(multiply, tasks)
print(results)  # [2, 12, 30]
```

## 

### 1. 

```python
import logging
import logging.handlers
import json
from datetime import datetime

class StructuredLogger:
    """"""
    
    def __init__(self, name, log_file='app.log', level=logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)
        
        # 
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        console_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(console_formatter)
        
        # 
        file_handler = logging.handlers.RotatingFileHandler(
            log_file,
            maxBytes=10*1024*1024,  # 10MB
            backupCount=5
        )
        file_handler.setLevel(level)
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(file_formatter)
        
        self.logger.addHandler(console_handler)
        self.logger.addHandler(file_handler)
    
    def log(self, level, message, **kwargs):
        """"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'message': message,
            **kwargs
        }
        log_message = json.dumps(log_data, ensure_ascii=False)
        
        if level == 'debug':
            self.logger.debug(log_message)
        elif level == 'info':
            self.logger.info(log_message)
        elif level == 'warning':
            self.logger.warning(log_message)
        elif level == 'error':
            self.logger.error(log_message)
        elif level == 'critical':
            self.logger.critical(log_message)
    
    def info(self, message, **kwargs):
        self.log('info', message, **kwargs)
    
    def error(self, message, **kwargs):
        self.log('error', message, **kwargs)
    
    def warning(self, message, **kwargs):
        self.log('warning', message, **kwargs)

# 
logger = StructuredLogger('myapp')

logger.info('', user_id=12345, ip='192.168.1.1')
logger.error('', error='Connection timeout', retry_count=3)
```

## 

### 1. 

```python
import configparser
import json
import os
from typing import Any, Dict

class ConfigManager:
    """"""
    
    def __init__(self, config_file='config.ini'):
        self.config_file = config_file
        self.config = configparser.ConfigParser()
        self.load()
    
    def load(self):
        """"""
        if os.path.exists(self.config_file):
            self.config.read(self.config_file, encoding='utf-8')
    
    def save(self):
        """"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            self.config.write(f)
    
    def get(self, section, key, default=None):
        """"""
        try:
            return self.config.get(section, key)
        except (configparser.NoSectionError, configparser.NoOptionError):
            return default
    
    def set(self, section, key, value):
        """"""
        if not self.config.has_section(section):
            self.config.add_section(section)
        self.config.set(section, key, str(value))
        self.save()
    
    def get_section(self, section):
        """section"""
        if self.config.has_section(section):
            return dict(self.config.items(section))
        return {}
    
    def to_dict(self):
        """"""
        result = {}
        for section in self.config.sections():
            result[section] = dict(self.config.items(section))
        return result

# 
config = ConfigManager('app.ini')

# 
config.set('database', 'host', 'localhost')
config.set('database', 'port', '3306')
config.set('database', 'user', 'root')

# 
db_host = config.get('database', 'host')
db_port = config.get('database', 'port', '3306')

# section
db_config = config.get_section('database')
print(db_config)
```

## 

Python

1. ****
2. ****
3. ****
4. ****

## 

- [Python](https://docs.python.org/zh-cn/3/)
- [Python](https://pymotw.com/3/)
- [Effective Python](https://effectivepython.com/)

