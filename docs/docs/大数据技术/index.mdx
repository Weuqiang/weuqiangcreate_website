---
sidebar_position: 3
title: 
---

import DocCardList from '@theme/DocCardList';

#  - 

""****

## 

### 5V

1. **Volume**TBPB
2. **Velocity**
3. **Variety**
4. **Value**
5. **Veracity**

### 

```python
# 
data = load_all_data()  # 
result = process(data)  # 

# 
data = load_distributed()  # 
result = parallel_process(data)  # 1
```

## 

```

                            
  BI          

           ↑

                            
  SparkFlinkHivePresto         

           ↑

                            
  HDFSHBaseKafkaRedis          

           ↑

                            
  FlumeSqoopLogstashKafka      

           ↑

                                
  API             

```

## 

### 2-3

****

1. **Linux**
   - 
   - Shell
   - 

2. **Hadoop**
   - HDFS
   - MapReduce
   - YARN

3. ****WordCount

### 4-6

****

1. **Spark**
   - RDDDataFrameDataset
   - Spark SQL
   - Spark Streaming

2. **Hive**
   - SQL on Hadoop
   - 
   - ETL

3. **HBase**
   - NoSQL
   - 
   - 

4. ****

### 3-4

****

1. **Kafka**
   - 
   - 
   - 

2. **Flink**
   - 
   - 
   - 

3. ****

### 3-4

****

1. ****
   - 
   - 
   - 

2. ****
   - Hive
   - Presto
   - ClickHouse

3. ****

### 

****

1. ****T+1
2. ****
3. ****
4. ****+AI

## 

### 1. Hadoop - 

**HDFS**

```bash
# HDFS
hdfs dfs -put local_file.txt /user/data/

# 
hdfs dfs -ls /user/data/

# 
hdfs dfs -cat /user/data/local_file.txt

# 
hdfs dfs -get /user/data/local_file.txt ./
```

**MapReduce**

```python
# WordCount
from mrjob.job import MRJob

class WordCount(MRJob):
    
    def mapper(self, _, line):
        """Map"""
        for word in line.split():
            yield word.lower(), 1
    
    def reducer(self, word, counts):
        """Reduce"""
        yield word, sum(counts)

if __name__ == '__main__':
    WordCount.run()
```

### 2. Spark - 

**SparkMapReduce**

- ****
- **DAG**
- **API**MapReduce

**Spark**

```python
from pyspark.sql import SparkSession

# Spark
spark = SparkSession.builder \
    .appName("MyApp") \
    .getOrCreate()

# 
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# 
result = df.filter(df.age > 18) \
    .groupBy("city") \
    .count() \
    .orderBy("count", ascending=False)

# 
result.show()

# 
result.write.csv("output/")
```

### 3. Kafka - 

****

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# 
producer.send('user_events', {
    'user_id': 123,
    'event': 'click',
    'timestamp': '2024-01-01 10:00:00'
})

producer.flush()
```

****

```python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'user_events',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

for message in consumer:
    event = message.value
    print(f"User {event['user_id']} {event['event']} at {event['timestamp']}")
```

### 4. Hive - SQL on Hadoop

```sql
-- 
CREATE TABLE users (
    user_id INT,
    name STRING,
    age INT,
    city STRING
)
PARTITIONED BY (date STRING)
STORED AS PARQUET;

-- 
LOAD DATA INPATH '/data/users.csv' 
INTO TABLE users PARTITION (date='2024-01-01');

-- 
SELECT city, COUNT(*) as user_count
FROM users
WHERE age > 18
GROUP BY city
ORDER BY user_count DESC
LIMIT 10;

-- 
CREATE VIEW active_users AS
SELECT *
FROM users
WHERE last_login > '2024-01-01';
```

## 

### 1

****

****
- Flume
- HDFS
- Spark
- Hive
- Superset

****

1. ****
```bash
# Flume
agent.sources = r1
agent.sinks = k1
agent.channels = c1

agent.sources.r1.type = exec
agent.sources.r1.command = tail -F /var/log/nginx/access.log

agent.sinks.k1.type = hdfs
agent.sinks.k1.hdfs.path = /user/logs/%Y%m%d

agent.channels.c1.type = memory
```

2. ****
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("LogAnalysis").getOrCreate()

# 
logs = spark.read.text("/user/logs/20240101")

# 
parsed = logs.select(
    regexp_extract('value', r'(\d+\.\d+\.\d+\.\d+)', 1).alias('ip'),
    regexp_extract('value', r'"(GET|POST) (.*?) HTTP', 2).alias('url'),
    regexp_extract('value', r'HTTP/\d\.\d" (\d+)', 1).alias('status')
)

# 
url_stats = parsed.groupBy('url').agg(
    count('*').alias('pv'),
    countDistinct('ip').alias('uv')
).orderBy('pv', ascending=False)

# 
url_stats.write.mode('overwrite').saveAsTable('url_stats')
```

3. ****
```sql
-- PV/UV
SELECT 
    date,
    SUM(pv) as total_pv,
    SUM(uv) as total_uv
FROM url_stats
GROUP BY date;

-- TOP10
SELECT url, pv, uv
FROM url_stats
ORDER BY pv DESC
LIMIT 10;
```

### 2

****

****
- Kafka
- Flink
- Redis
- Spark MLlib

****
```
 → Kafka → Flink → Redis → API
                      ↓
                   
```

****

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer
from pyflink.common.serialization import SimpleStringSchema

env = StreamExecutionEnvironment.get_execution_environment()

# Kafka
kafka_consumer = FlinkKafkaConsumer(
    topics='user_events',
    deserialization_schema=SimpleStringSchema(),
    properties={'bootstrap.servers': 'localhost:9092'}
)

stream = env.add_source(kafka_consumer)

# 
def calculate_interest(events):
    # 
    category_counts = {}
    for event in events:
        category = event['category']
        category_counts[category] = category_counts.get(category, 0) + 1
    
    # Top3
    return sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:3]

# 
def generate_recommendations(user_id, interests):
    # Redis
    recommendations = []
    for category, _ in interests:
        items = redis_client.zrevrange(f'hot_items:{category}', 0, 9)
        recommendations.extend(items)
    
    # Redis
    redis_client.setex(f'rec:{user_id}', 3600, json.dumps(recommendations))

stream.key_by(lambda x: x['user_id']) \
    .window(TumblingEventTimeWindows.of(Time.minutes(5))) \
    .apply(calculate_interest) \
    .map(lambda x: generate_recommendations(x[0], x[1]))

env.execute("RealTimeRecommendation")
```

## 

### 
- **Coursera**Big Data Specialization
- **Udacity**Data Engineer Nanodegree
- **B**

### 
- Hadoop
- Spark
- Kafka
- 

### 
- ****Docker Compose
- ****DataWorksAWS EMR
- ****SparkFlink

## 

### 

```

 Linux
 Python/Java/Scala
 SQL
 


 Hadoop
 Spark
 Kafka
 Flink
 


 
 
 
 
```

### 

1. ****
   - ETL
   - 
   - 

2. ****
   - 
   - 
   - 

3. ****
   - 
   - 
   - 

## 



<DocCardList />

****

