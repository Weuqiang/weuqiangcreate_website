---
sidebar_position: 10
title: 
---

# Large Language Models

:::info 
GPTBERTLLaMA
:::

## 

**LLM** Transformer

### LLM

****
- GPT-3: 175B, GPT-4: 1.7T+
- TB
- GPU/TPU

**Emergent Abilities**
- In-Context Learning
- Chain-of-Thought
- Instruction Following

****
- Zero-Shot
- Few-Shot
- 

## LLM

### 2018-2019

**BERT2018**
- 
- MLM
- 

**GPT-22019**
- 
- 
- 1.5B

### LLM2020-2022

**GPT-32020**
- 175B
- Few-Shot Learning
- 

**T5BARTmT5**
- -
- 

### 2022-

**ChatGPT2022.11**
- GPT-3.5
- RLHF
- 

**GPT-42023.03**
- 
- 
- 

**LLM**
- LLaMAMeta
- Mistral
- Qwen
- ChatGLM

## LLM

### 1. Transformer

```python
import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    """"""
    
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Q, K, V
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        
        # 
        self.W_o = nn.Linear(d_model, d_model)
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """"""
        # Q, K, V: (batch, num_heads, seq_len, d_k)
        
        # 
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # mask
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Softmax
        attention_weights = torch.softmax(scores, dim=-1)
        
        # 
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.shape
        
        # 
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        # 
        output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # 
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)
        
        # 
        output = self.W_o(output)
        
        return output, attention_weights


class FeedForward(nn.Module):
    """"""
    
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.GELU()
    
    def forward(self, x):
        return self.linear2(self.dropout(self.activation(self.linear1(x))))


class TransformerBlock(nn.Module):
    """Transformer"""
    
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # 
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        
        # 
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout2 = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        #  + 
        attn_output, _ = self.attention(x, mask)
        x = self.norm1(x + self.dropout1(attn_output))
        
        #  + 
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout2(ffn_output))
        
        return x


class GPTModel(nn.Module):
    """GPT"""
    
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout=0.1):
        super().__init__()
        
        self.d_model = d_model
        
        # Token
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        
        # 
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        # Transformer
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        
        # 
        self.ln_f = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        batch_size, seq_len = x.shape
        
        # Token
        token_emb = self.token_embedding(x)
        
        # 
        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)
        pos_emb = self.position_embedding(positions)
        
        # 
        x = self.dropout(token_emb + pos_emb)
        
        # mask
        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).unsqueeze(0).unsqueeze(0)
        
        # Transformer
        for block in self.blocks:
            x = block(x, mask)
        
        # 
        x = self.ln_f(x)
        logits = self.head(x)
        
        return logits

# 
model = GPTModel(
    vocab_size=50257,
    d_model=768,
    num_heads=12,
    num_layers=12,
    d_ff=3072,
    max_seq_len=1024
)

print(f": {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
```

### 2. 

**GPT**

```python
def autoregressive_loss(model, input_ids, labels):
    """
    
    token
    """
    # 
    logits = model(input_ids)
    
    # 
    # logits: (batch, seq_len, vocab_size)
    # labels: (batch, seq_len)
    loss = nn.CrossEntropyLoss()(
        logits.view(-1, logits.size(-1)),
        labels.view(-1)
    )
    
    return loss

# 
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)

for batch in dataloader:
    input_ids = batch['input_ids']
    labels = batch['labels']
    
    # 
    loss = autoregressive_loss(model, input_ids, labels)
    
    # 
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

**BERT**

```python
def masked_language_model_loss(model, input_ids, labels, mask_token_id):
    """
    
    masktoken
    """
    # mask 15%token
    mask_prob = 0.15
    mask = torch.rand(input_ids.shape) < mask_prob
    
    # token
    labels = input_ids.clone()
    labels[~mask] = -100  # mask
    
    # [MASK]
    input_ids[mask] = mask_token_id
    
    # 
    logits = model(input_ids)
    
    # 
    loss = nn.CrossEntropyLoss()(
        logits.view(-1, logits.size(-1)),
        labels.view(-1)
    )
    
    return loss
```

### 3. 

**Full Fine-Tuning**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments

# 
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# 
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# 
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=2e-5,
    warmup_steps=500,
    logging_steps=100,
)

# 
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
)

trainer.train()
```

**LoRALow-Rank Adaptation**

```python
from peft import LoraConfig, get_peft_model

# LoRA
lora_config = LoraConfig(
    r=8,  # 
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # LoRA
    lora_dropout=0.1,
    bias="none",
)

# LoRA
model = get_peft_model(model, lora_config)

# 
model.print_trainable_parameters()
# : trainable params: 294,912 || all params: 124,439,808 || trainable%: 0.24%
```

**P-Tuning / Prompt Tuning**

```python
class PromptTuning(nn.Module):
    """Prompt Tuning"""
    
    def __init__(self, model, num_virtual_tokens, embedding_dim):
        super().__init__()
        self.model = model
        self.num_virtual_tokens = num_virtual_tokens
        
        # soft prompt
        self.soft_prompt = nn.Parameter(
            torch.randn(num_virtual_tokens, embedding_dim)
        )
    
    def forward(self, input_ids):
        # 
        inputs_embeds = self.model.get_input_embeddings()(input_ids)
        
        # soft prompt
        batch_size = inputs_embeds.shape[0]
        soft_prompt_batch = self.soft_prompt.unsqueeze(0).expand(batch_size, -1, -1)
        
        # 
        inputs_embeds = torch.cat([soft_prompt_batch, inputs_embeds], dim=1)
        
        # 
        outputs = self.model(inputs_embeds=inputs_embeds)
        
        return outputs
```

### 4. RLHF

```python
class RewardModel(nn.Module):
    """"""
    
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        self.reward_head = nn.Linear(base_model.config.hidden_size, 1)
    
    def forward(self, input_ids):
        # 
        outputs = self.base_model(input_ids, output_hidden_states=True)
        last_hidden_state = outputs.hidden_states[-1]
        
        # token
        last_token_hidden = last_hidden_state[:, -1, :]
        
        # 
        reward = self.reward_head(last_token_hidden)
        
        return reward


def ppo_loss(policy_model, ref_model, reward_model, states, actions, old_log_probs):
    """
    PPO
    """
    # log
    logits = policy_model(states)
    log_probs = torch.log_softmax(logits, dim=-1)
    action_log_probs = log_probs.gather(-1, actions.unsqueeze(-1)).squeeze(-1)
    
    # 
    ratio = torch.exp(action_log_probs - old_log_probs)
    
    # 
    rewards = reward_model(states)
    
    # KL
    ref_logits = ref_model(states)
    ref_log_probs = torch.log_softmax(ref_logits, dim=-1)
    kl_penalty = torch.sum(
        torch.exp(log_probs) * (log_probs - ref_log_probs),
        dim=-1
    )
    
    # PPO
    advantages = rewards - 0.1 * kl_penalty
    
    # Clipped surrogate objective
    epsilon = 0.2
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages
    
    loss = -torch.min(surr1, surr2).mean()
    
    return loss
```

## LLM

### GPT

**GPT-3.5 / GPT-4**
- ****: Decoder-only Transformer
- ****: GPT-3.5: 175B, GPT-4: 1.7T+
- ****: 
  - 
  - 
  - GPT-4
- ****: ChatGPT, API

### LLM

**LLaMAMeta**

```python
from transformers import LlamaForCausalLM, LlamaTokenizer

# LLaMA
model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer = LlamaTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# 
prompt = "Once upon a time"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
text = tokenizer.decode(outputs[0])
print(text)
```

****:
- 
- 7B, 13B, 70B
- 

**Mistral**
- Mistral-7B: LLaMA-13B
- Mixtral-8x7B: MoE
- 32K

**Qwen**
- 
- 
- 

### 

|  |  |  |  |  |  |
|------|--------|-----------|------|--------|------|
| GPT-4 | 1.7T+ | 128K |  |  |  |
| Claude 3 | ? | 200K |  |  |  |
| Gemini Pro | ? | 32K |  |  | Google |
| LLaMA-2-70B | 70B | 4K |  |  |  |
| Mistral-7B | 7B | 32K |  |  |  |
| Qwen-72B | 72B | 32K |  |  |  |

## LLM

### 1. Transformers

```python
from transformers import pipeline

# 
generator = pipeline("text-generation", model="gpt2")
result = generator("The future of AI is", max_length=50, num_return_sequences=3)

for i, text in enumerate(result):
    print(f"\n{i+1}: {text['generated_text']}")

# 
qa_pipeline = pipeline("question-answering")
context = "Paris is the capital of France. It is known for the Eiffel Tower."
question = "What is Paris known for?"
answer = qa_pipeline(question=question, context=context)
print(f": {answer['answer']}")

# 
classifier = pipeline("sentiment-analysis")
result = classifier("I love this product!")
print(result)
```

### 2. LangChain

```python
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# LLM
llm = OpenAI(temperature=0.7)

# Prompt
template = """
{role}
{question}
"""

prompt = PromptTemplate(
    input_variables=["role", "question"],
    template=template
)

# Chain
chain = LLMChain(llm=llm, prompt=prompt)

# 
result = chain.run(role="Python", question="")
print(result)
```

### 3. RAG

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# 
documents = [
    "Python",
    "",
    ""
]

# 
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts(documents, embeddings)

# QA
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 
query = ""
answer = qa_chain.run(query)
print(answer)
```

### 4. Function Calling

```python
import json
from openai import OpenAI

client = OpenAI()

# 
functions = [
    {
        "name": "get_weather",
        "description": "",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {
                    "type": "string",
                    "description": ""
                }
            },
            "required": ["city"]
        }
    }
]

# 
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": ""}],
    functions=functions,
    function_call="auto"
)

# 
if response.choices[0].message.function_call:
    function_name = response.choices[0].message.function_call.name
    arguments = json.loads(response.choices[0].message.function_call.arguments)
    
    print(f": {function_name}")
    print(f": {arguments}")
```

## Prompt Engineering

### 

**1. **
```
 : 
 : 500
```

**2. **
```
Python

3
```

**3. Few-Shot**
```


: 
: The weather is nice today

: 
: I love programming

: 
:
```

### 

**Chain-of-Thought**
```
: 231730

:
1. : 23
2. 17: 23 - 17 = 6
3. 30: 6 + 30 = 36

: 36
```

**Self-Consistency**
```python
# 
answers = []
for _ in range(5):
    response = llm.generate(prompt)
    answers.append(response)

# 
from collections import Counter
most_common = Counter(answers).most_common(1)[0][0]
```

**ReAct+**
```
: 2024

: 2024
: [2024]
: 2024
: 
: 2024
```

## 

### 

**Perplexity**
```python
def calculate_perplexity(model, tokenizer, text):
    """"""
    encodings = tokenizer(text, return_tensors='pt')
    
    with torch.no_grad():
        outputs = model(**encodings, labels=encodings['input_ids'])
        loss = outputs.loss
    
    perplexity = torch.exp(loss)
    return perplexity.item()
```

**BLEU**
```python
from nltk.translate.bleu_score import sentence_bleu

reference = [['this', 'is', 'a', 'test']]
candidate = ['this', 'is', 'test']

score = sentence_bleu(reference, candidate)
print(f'BLEU score: {score}')
```

**ROUGE**
```python
from rouge import Rouge

rouge = Rouge()

hypothesis = "the cat is on the mat"
reference = "the cat sat on the mat"

scores = rouge.get_scores(hypothesis, reference)
print(scores)
```

### 

****:
- Accuracy
- Fluency
- Relevance
- Consistency
- Safety

## 

### 1. 

- ****: 
- ****: 
- ****: 
- ****: 

### 2. Prompt

- prompt
- 
- max_tokens
- stop sequences

### 3. 

- 
- 
- 
- prompt

### 4. 

- 
- 
- 
- prompt

## 

****
- 
- 

****
- 4K100K+
- 

****
- +++
- 

****
- 
- 

****
- 
- 

## 

****:
1. LLMTransformer
2. +
3. RLHF
4. Prompt Engineering
5. 

****:
- Transformer
- 
- Prompt Engineering
- 

<DocCardList />

