"use strict";(globalThis.webpackChunkweuqiangcreate_website=globalThis.webpackChunkweuqiangcreate_website||[]).push([[11152],{31674(e,n,s){s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"\u4eba\u5de5\u667a\u80fd/\u591a\u6a21\u6001AI","title":"\u591a\u6a21\u6001AI","description":"\u672c\u7ae0\u8282\u4ecb\u7ecd\u591a\u6a21\u6001AI\u7684\u539f\u7406\u3001\u67b6\u6784\u548c\u5e94\u7528\uff0c\u6db5\u76d6\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3001CLIP\u3001GPT-4V\u7b49\u524d\u6cbf\u6280\u672f\u3002","source":"@site/docs/docs/\u4eba\u5de5\u667a\u80fd/\u591a\u6a21\u6001AI.mdx","sourceDirName":"\u4eba\u5de5\u667a\u80fd","slug":"/\u4eba\u5de5\u667a\u80fd/\u591a\u6a21\u6001AI","permalink":"/weuqiangcreate_website/docs/\u4eba\u5de5\u667a\u80fd/\u591a\u6a21\u6001AI","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11,"title":"\u591a\u6a21\u6001AI"},"sidebar":"tutorialSidebar","previous":{"title":"\u5927\u8bed\u8a00\u6a21\u578b","permalink":"/weuqiangcreate_website/docs/\u4eba\u5de5\u667a\u80fd/\u5927\u8bed\u8a00\u6a21\u578b"},"next":{"title":"AI Agent\u667a\u80fd\u4f53","permalink":"/weuqiangcreate_website/docs/\u4eba\u5de5\u667a\u80fd/AI-Agent"}}');var t=s(74848),r=s(28453);const a={sidebar_position:11,title:"\u591a\u6a21\u6001AI"},l="\u591a\u6a21\u6001AI\uff08Multimodal AI\uff09",o={},d=[{value:"\u4ec0\u4e48\u662f\u591a\u6a21\u6001AI",id:"\u4ec0\u4e48\u662f\u591a\u6a21\u6001ai",level:2},{value:"\u4e3a\u4ec0\u4e48\u9700\u8981\u591a\u6a21\u6001",id:"\u4e3a\u4ec0\u4e48\u9700\u8981\u591a\u6a21\u6001",level:3},{value:"\u591a\u6a21\u6001AI\u53d1\u5c55\u5386\u7a0b",id:"\u591a\u6a21\u6001ai\u53d1\u5c55\u5386\u7a0b",level:2},{value:"\u65e9\u671f\u63a2\u7d22\uff082015-2019\uff09",id:"\u65e9\u671f\u63a2\u7d222015-2019",level:3},{value:"CLIP\u65f6\u4ee3\uff082021\uff09",id:"clip\u65f6\u4ee32021",level:3},{value:"\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff082022-\u81f3\u4eca\uff09",id:"\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b2022-\u81f3\u4eca",level:3},{value:"\u6838\u5fc3\u6280\u672f",id:"\u6838\u5fc3\u6280\u672f",level:2},{value:"1. \u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3",id:"1-\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3",level:3},{value:"2. \u8de8\u6a21\u6001\u6ce8\u610f\u529b",id:"2-\u8de8\u6a21\u6001\u6ce8\u610f\u529b",level:3},{value:"3. \u6a21\u6001\u878d\u5408\u7b56\u7565",id:"3-\u6a21\u6001\u878d\u5408\u7b56\u7565",level:3},{value:"\u4e3b\u6d41\u591a\u6a21\u6001\u6a21\u578b",id:"\u4e3b\u6d41\u591a\u6a21\u6001\u6a21\u578b",level:2},{value:"CLIP\uff08OpenAI\uff09",id:"clipopenai",level:3},{value:"BLIP\uff08Salesforce\uff09",id:"blipsalesforce",level:3},{value:"LLaVA\uff08\u89c6\u89c9\u6307\u4ee4\u5fae\u8c03\uff09",id:"llava\u89c6\u89c9\u6307\u4ee4\u5fae\u8c03",level:3},{value:"GPT-4V\uff08OpenAI\uff09",id:"gpt-4vopenai",level:3},{value:"\u5e94\u7528\u573a\u666f",id:"\u5e94\u7528\u573a\u666f",level:2},{value:"1. \u56fe\u50cf\u7406\u89e3\u4e0e\u63cf\u8ff0",id:"1-\u56fe\u50cf\u7406\u89e3\u4e0e\u63cf\u8ff0",level:3},{value:"2. \u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09",id:"2-\u89c6\u89c9\u95ee\u7b54vqa",level:3},{value:"3. \u56fe\u50cf\u68c0\u7d22",id:"3-\u56fe\u50cf\u68c0\u7d22",level:3},{value:"4. \u6587\u672c\u5230\u56fe\u50cf\u751f\u6210",id:"4-\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210",level:3},{value:"\u6700\u4f73\u5b9e\u8df5",id:"\u6700\u4f73\u5b9e\u8df5",level:2},{value:"1. \u6570\u636e\u51c6\u5907",id:"1-\u6570\u636e\u51c6\u5907",level:3},{value:"2. \u6a21\u578b\u8bad\u7ec3",id:"2-\u6a21\u578b\u8bad\u7ec3",level:3},{value:"3. \u63a8\u7406\u4f18\u5316",id:"3-\u63a8\u7406\u4f18\u5316",level:3},{value:"\u672a\u6765\u8d8b\u52bf",id:"\u672a\u6765\u8d8b\u52bf",level:2},{value:"\u603b\u7ed3",id:"\u603b\u7ed3",level:2}];function c(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{DocCardList:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("DocCardList",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"\u591a\u6a21\u6001aimultimodal-ai",children:"\u591a\u6a21\u6001AI\uff08Multimodal AI\uff09"})}),"\n",(0,t.jsx)(n.admonition,{title:"\u7ae0\u8282\u6982\u8ff0",type:"info",children:(0,t.jsx)(n.p,{children:"\u672c\u7ae0\u8282\u4ecb\u7ecd\u591a\u6a21\u6001AI\u7684\u539f\u7406\u3001\u67b6\u6784\u548c\u5e94\u7528\uff0c\u6db5\u76d6\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3001CLIP\u3001GPT-4V\u7b49\u524d\u6cbf\u6280\u672f\u3002"})}),"\n",(0,t.jsx)(n.h2,{id:"\u4ec0\u4e48\u662f\u591a\u6a21\u6001ai",children:"\u4ec0\u4e48\u662f\u591a\u6a21\u6001AI"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"\u591a\u6a21\u6001AI"})," \u662f\u80fd\u591f\u7406\u89e3\u548c\u5904\u7406\u591a\u79cd\u7c7b\u578b\u6570\u636e\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u7b49\uff09\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002"]}),"\n",(0,t.jsx)(n.h3,{id:"\u4e3a\u4ec0\u4e48\u9700\u8981\u591a\u6a21\u6001",children:"\u4e3a\u4ec0\u4e48\u9700\u8981\u591a\u6a21\u6001"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"\u4eba\u7c7b\u7684\u611f\u77e5\u662f\u591a\u6a21\u6001\u7684"}),"\uff1a"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u6211\u4eec\u901a\u8fc7\u89c6\u89c9\u3001\u542c\u89c9\u3001\u89e6\u89c9\u7b49\u591a\u79cd\u611f\u5b98\u7406\u89e3\u4e16\u754c"}),"\n",(0,t.jsx)(n.li,{children:"\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\u76f8\u4e92\u8865\u5145\u3001\u76f8\u4e92\u9a8c\u8bc1"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"\u5355\u6a21\u6001\u7684\u5c40\u9650\u6027"}),"\uff1a"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u6587\u672c\u65e0\u6cd5\u63cf\u8ff0\u89c6\u89c9\u7ec6\u8282"}),"\n",(0,t.jsx)(n.li,{children:"\u56fe\u50cf\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3"}),"\n",(0,t.jsx)(n.li,{children:"\u97f3\u9891\u7f3a\u5c11\u89c6\u89c9\u4e0a\u4e0b\u6587"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"\u591a\u6a21\u6001\u7684\u4f18\u52bf"}),"\uff1a"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u66f4\u5168\u9762\u7684\u7406\u89e3"}),"\n",(0,t.jsx)(n.li,{children:"\u8de8\u6a21\u6001\u63a8\u7406"}),"\n",(0,t.jsx)(n.li,{children:"\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"\u591a\u6a21\u6001ai\u53d1\u5c55\u5386\u7a0b",children:"\u591a\u6a21\u6001AI\u53d1\u5c55\u5386\u7a0b"}),"\n",(0,t.jsx)(n.h3,{id:"\u65e9\u671f\u63a2\u7d222015-2019",children:"\u65e9\u671f\u63a2\u7d22\uff082015-2019\uff09"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"\u56fe\u50cf\u63cf\u8ff0\uff08Image Captioning\uff09"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# \u65e9\u671f\u7684CNN+RNN\u67b6\u6784\nclass ImageCaptioning(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super().__init__()\n        # CNN\u7f16\u7801\u5668\uff08\u5982ResNet\uff09\n        self.encoder = models.resnet50(pretrained=True)\n        self.encoder.fc = nn.Linear(2048, embed_size)\n        \n        # LSTM\u89e3\u7801\u5668\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n    \n    def forward(self, images, captions):\n        # \u56fe\u50cf\u7279\u5f81\n        features = self.encoder(images).unsqueeze(1)\n        \n        # \u6587\u672c\u5d4c\u5165\n        embeddings = self.embedding(captions)\n        \n        # \u62fc\u63a5\n        inputs = torch.cat([features, embeddings], dim=1)\n        \n        # LSTM\u751f\u6210\n        hiddens, _ = self.lstm(inputs)\n        outputs = self.fc(hiddens)\n        \n        return outputs\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u7ed9\u5b9a\u56fe\u50cf\u548c\u95ee\u9898\uff0c\u751f\u6210\u7b54\u6848"}),"\n",(0,t.jsx)(n.li,{children:"\u9700\u8981\u89c6\u89c9\u7406\u89e3\u548c\u8bed\u8a00\u63a8\u7406"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"clip\u65f6\u4ee32021",children:"CLIP\u65f6\u4ee3\uff082021\uff09"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"CLIP\uff08Contrastive Language-Image Pre-training\uff09"})}),"\n",(0,t.jsx)(n.p,{children:"OpenAI\u57282021\u5e74\u63d0\u51fa\u7684CLIP\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u4e86\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u7edf\u4e00\u8868\u793a\u3002"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CLIP(nn.Module):\n    """CLIP\u6a21\u578b\u7b80\u5316\u5b9e\u73b0"""\n    \n    def __init__(self, image_encoder, text_encoder, embed_dim=512):\n        super().__init__()\n        self.image_encoder = image_encoder\n        self.text_encoder = text_encoder\n        \n        # \u6295\u5f71\u5c42\n        self.image_projection = nn.Linear(image_encoder.output_dim, embed_dim)\n        self.text_projection = nn.Linear(text_encoder.output_dim, embed_dim)\n        \n        # \u6e29\u5ea6\u53c2\u6570\n        self.temperature = nn.Parameter(torch.ones([]) * 0.07)\n    \n    def encode_image(self, images):\n        """\u7f16\u7801\u56fe\u50cf"""\n        features = self.image_encoder(images)\n        embeddings = self.image_projection(features)\n        # L2\u5f52\u4e00\u5316\n        embeddings = F.normalize(embeddings, dim=-1)\n        return embeddings\n    \n    def encode_text(self, texts):\n        """\u7f16\u7801\u6587\u672c"""\n        features = self.text_encoder(texts)\n        embeddings = self.text_projection(features)\n        # L2\u5f52\u4e00\u5316\n        embeddings = F.normalize(embeddings, dim=-1)\n        return embeddings\n    \n    def forward(self, images, texts):\n        # \u7f16\u7801\n        image_embeddings = self.encode_image(images)\n        text_embeddings = self.encode_text(texts)\n        \n        # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\u77e9\u9635\n        logits = (image_embeddings @ text_embeddings.T) / self.temperature\n        \n        return logits\n    \n    def contrastive_loss(self, logits):\n        """\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931"""\n        batch_size = logits.shape[0]\n        labels = torch.arange(batch_size, device=logits.device)\n        \n        # \u56fe\u50cf\u5230\u6587\u672c\u7684\u635f\u5931\n        loss_i2t = F.cross_entropy(logits, labels)\n        \n        # \u6587\u672c\u5230\u56fe\u50cf\u7684\u635f\u5931\n        loss_t2i = F.cross_entropy(logits.T, labels)\n        \n        # \u603b\u635f\u5931\n        loss = (loss_i2t + loss_t2i) / 2\n        \n        return loss\n\n# \u4f7f\u7528CLIP\u8fdb\u884c\u96f6\u6837\u672c\u5206\u7c7b\ndef zero_shot_classification(model, image, class_names):\n    """\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b"""\n    # \u7f16\u7801\u56fe\u50cf\n    image_features = model.encode_image(image)\n    \n    # \u4e3a\u6bcf\u4e2a\u7c7b\u522b\u521b\u5efa\u6587\u672c\u63d0\u793a\n    text_prompts = [f"a photo of a {name}" for name in class_names]\n    text_features = model.encode_text(text_prompts)\n    \n    # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\n    similarities = (image_features @ text_features.T).softmax(dim=-1)\n    \n    # \u8fd4\u56de\u6700\u53ef\u80fd\u7684\u7c7b\u522b\n    pred_idx = similarities.argmax().item()\n    return class_names[pred_idx], similarities[0, pred_idx].item()\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"CLIP\u7684\u5e94\u7528"}),"\uff1a"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b"}),"\n",(0,t.jsx)(n.li,{children:"\u56fe\u50cf\u68c0\u7d22"}),"\n",(0,t.jsx)(n.li,{children:"\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff08DALL-E\uff09"}),"\n",(0,t.jsx)(n.li,{children:"\u56fe\u50cf\u7f16\u8f91"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b2022-\u81f3\u4eca",children:"\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff082022-\u81f3\u4eca\uff09"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Flamingo\uff08DeepMind, 2022\uff09"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Few-shot\u5b66\u4e60\u80fd\u529b"}),"\n",(0,t.jsx)(n.li,{children:"\u4ea4\u9519\u7684\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"GPT-4V\uff08OpenAI, 2023\uff09"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u5f3a\u5927\u7684\u89c6\u89c9\u7406\u89e3"}),"\n",(0,t.jsx)(n.li,{children:"\u591a\u56fe\u50cf\u63a8\u7406"}),"\n",(0,t.jsx)(n.li,{children:"OCR\u548c\u56fe\u8868\u7406\u89e3"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Gemini\uff08Google, 2023\uff09"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u539f\u751f\u591a\u6a21\u6001\u8bbe\u8ba1"}),"\n",(0,t.jsx)(n.li,{children:"\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"\u6838\u5fc3\u6280\u672f",children:"\u6838\u5fc3\u6280\u672f"}),"\n",(0,t.jsx)(n.h3,{id:"1-\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3",children:"1. \u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"\u5bf9\u6bd4\u5b66\u4e60\uff08Contrastive Learning\uff09"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def clip_loss(image_embeddings, text_embeddings, temperature=0.07):\n    """\n    CLIP\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\n    \n    Args:\n        image_embeddings: (batch_size, embed_dim)\n        text_embeddings: (batch_size, embed_dim)\n    """\n    # \u5f52\u4e00\u5316\n    image_embeddings = F.normalize(image_embeddings, dim=-1)\n    text_embeddings = F.normalize(text_embeddings, dim=-1)\n    \n    # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\u77e9\u9635\n    logits = torch.matmul(image_embeddings, text_embeddings.T) / temperature\n    \n    # \u6807\u7b7e\uff08\u5bf9\u89d2\u7ebf\u4e3a\u6b63\u6837\u672c\uff09\n    batch_size = image_embeddings.shape[0]\n    labels = torch.arange(batch_size, device=logits.device)\n    \n    # \u53cc\u5411\u4ea4\u53c9\u71b5\n    loss_i2t = F.cross_entropy(logits, labels)\n    loss_t2i = F.cross_entropy(logits.T, labels)\n    \n    loss = (loss_i2t + loss_t2i) / 2\n    \n    return loss\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"\u63a9\u7801\u5efa\u6a21\uff08Masked Modeling\uff09"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def masked_image_modeling(model, images, mask_ratio=0.75):\n    """\n    \u63a9\u7801\u56fe\u50cf\u5efa\u6a21\uff08\u7c7b\u4f3cMAE\uff09\n    """\n    batch_size, channels, height, width = images.shape\n    \n    # \u5c06\u56fe\u50cf\u5206\u6210patches\n    patch_size = 16\n    num_patches = (height // patch_size) * (width // patch_size)\n    \n    # \u968f\u673amask\n    num_masked = int(mask_ratio * num_patches)\n    mask_indices = torch.randperm(num_patches)[:num_masked]\n    \n    # \u7f16\u7801\n    encoded = model.encode(images, mask_indices)\n    \n    # \u89e3\u7801\u91cd\u5efa\n    reconstructed = model.decode(encoded)\n    \n    # \u8ba1\u7b97\u635f\u5931\uff08\u53ea\u5728mask\u4f4d\u7f6e\uff09\n    loss = F.mse_loss(reconstructed[:, mask_indices], images[:, mask_indices])\n    \n    return loss\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-\u8de8\u6a21\u6001\u6ce8\u610f\u529b",children:"2. \u8de8\u6a21\u6001\u6ce8\u610f\u529b"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class CrossModalAttention(nn.Module):\n    \"\"\"\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\"\"\"\n    \n    def __init__(self, dim, num_heads=8):\n        super().__init__()\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        \n        self.q_proj = nn.Linear(dim, dim)\n        self.k_proj = nn.Linear(dim, dim)\n        self.v_proj = nn.Linear(dim, dim)\n        self.out_proj = nn.Linear(dim, dim)\n    \n    def forward(self, query_modality, key_value_modality):\n        \"\"\"\n        Args:\n            query_modality: (batch, seq_len_q, dim) - \u67e5\u8be2\u6a21\u6001\n            key_value_modality: (batch, seq_len_kv, dim) - \u952e\u503c\u6a21\u6001\n        \"\"\"\n        batch_size = query_modality.shape[0]\n        \n        # \u6295\u5f71\n        Q = self.q_proj(query_modality)\n        K = self.k_proj(key_value_modality)\n        V = self.v_proj(key_value_modality)\n        \n        # \u91cd\u5851\u4e3a\u591a\u5934\n        Q = Q.view(batch_size, -1, self.num_heads, -1).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, -1).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, -1).transpose(1, 2)\n        \n        # \u6ce8\u610f\u529b\n        attn = (Q @ K.transpose(-2, -1)) * self.scale\n        attn = F.softmax(attn, dim=-1)\n        \n        # \u52a0\u6743\u6c42\u548c\n        out = attn @ V\n        out = out.transpose(1, 2).contiguous().view(batch_size, -1, -1)\n        out = self.out_proj(out)\n        \n        return out\n\n\nclass MultimodalTransformer(nn.Module):\n    \"\"\"\u591a\u6a21\u6001Transformer\"\"\"\n    \n    def __init__(self, dim, num_heads, num_layers):\n        super().__init__()\n        \n        self.layers = nn.ModuleList([\n            nn.ModuleDict({\n                'self_attn': nn.MultiheadAttention(dim, num_heads),\n                'cross_attn': CrossModalAttention(dim, num_heads),\n                'ffn': nn.Sequential(\n                    nn.Linear(dim, dim * 4),\n                    nn.GELU(),\n                    nn.Linear(dim * 4, dim)\n                ),\n                'norm1': nn.LayerNorm(dim),\n                'norm2': nn.LayerNorm(dim),\n                'norm3': nn.LayerNorm(dim),\n            })\n            for _ in range(num_layers)\n        ])\n    \n    def forward(self, text_features, image_features):\n        \"\"\"\n        Args:\n            text_features: (batch, text_len, dim)\n            image_features: (batch, image_len, dim)\n        \"\"\"\n        for layer in self.layers:\n            # \u81ea\u6ce8\u610f\u529b\uff08\u6587\u672c\uff09\n            text_features = text_features + layer['self_attn'](\n                layer['norm1'](text_features),\n                layer['norm1'](text_features),\n                layer['norm1'](text_features)\n            )[0]\n            \n            # \u8de8\u6a21\u6001\u6ce8\u610f\u529b\uff08\u6587\u672c\u5173\u6ce8\u56fe\u50cf\uff09\n            text_features = text_features + layer['cross_attn'](\n                layer['norm2'](text_features),\n                layer['norm2'](image_features)\n            )\n            \n            # \u524d\u9988\u7f51\u7edc\n            text_features = text_features + layer['ffn'](layer['norm3'](text_features))\n        \n        return text_features\n"})}),"\n",(0,t.jsx)(n.h3,{id:"3-\u6a21\u6001\u878d\u5408\u7b56\u7565",children:"3. \u6a21\u6001\u878d\u5408\u7b56\u7565"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"\u65e9\u671f\u878d\u5408\uff08Early Fusion\uff09"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def early_fusion(image_features, text_features):\n    """\u5728\u7279\u5f81\u63d0\u53d6\u524d\u878d\u5408"""\n    # \u62fc\u63a5\u539f\u59cb\u8f93\u5165\n    combined = torch.cat([image_features, text_features], dim=1)\n    # \u7edf\u4e00\u7f16\u7801\u5668\u5904\u7406\n    fused_features = encoder(combined)\n    return fused_features\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"\u665a\u671f\u878d\u5408\uff08Late Fusion\uff09"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def late_fusion(image_features, text_features):\n    """\u5728\u7279\u5f81\u63d0\u53d6\u540e\u878d\u5408"""\n    # \u5206\u522b\u7f16\u7801\n    image_encoded = image_encoder(image_features)\n    text_encoded = text_encoder(text_features)\n    \n    # \u7b80\u5355\u62fc\u63a5\u6216\u52a0\u6743\n    fused = torch.cat([image_encoded, text_encoded], dim=-1)\n    # \u6216\u8005\u52a0\u6743\u5e73\u5747\n    # fused = alpha * image_encoded + (1 - alpha) * text_encoded\n    \n    return fused\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"\u6df7\u5408\u878d\u5408\uff08Hybrid Fusion\uff09"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def hybrid_fusion(image_features, text_features):\n    """\u591a\u5c42\u6b21\u878d\u5408"""\n    # \u65e9\u671f\u878d\u5408\n    early_fused = early_fusion(image_features, text_features)\n    \n    # \u4e2d\u95f4\u5c42\u4ea4\u4e92\n    image_mid = image_encoder.mid_layers(image_features)\n    text_mid = text_encoder.mid_layers(text_features)\n    mid_fused = cross_attention(image_mid, text_mid)\n    \n    # \u665a\u671f\u878d\u5408\n    late_fused = late_fusion(image_mid, text_mid)\n    \n    # \u7ec4\u5408\n    final = torch.cat([early_fused, mid_fused, late_fused], dim=-1)\n    return final\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u4e3b\u6d41\u591a\u6a21\u6001\u6a21\u578b",children:"\u4e3b\u6d41\u591a\u6a21\u6001\u6a21\u578b"}),"\n",(0,t.jsx)(n.h3,{id:"clipopenai",children:"CLIP\uff08OpenAI\uff09"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\nimport requests\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\nprocessor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n\n# \u52a0\u8f7d\u56fe\u50cf\nurl = "http://images.cocodataset.org/val2017/000000039769.jpg"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# \u51c6\u5907\u8f93\u5165\ninputs = processor(\n    text=["a photo of a cat", "a photo of a dog"],\n    images=image,\n    return_tensors="pt",\n    padding=True\n)\n\n# \u524d\u5411\u4f20\u64ad\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = logits_per_image.softmax(dim=1)\n\nprint(f"\u6982\u7387: {probs}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"blipsalesforce",children:"BLIP\uff08Salesforce\uff09"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from transformers import BlipProcessor, BlipForConditionalGeneration\n\n# \u52a0\u8f7d\u6a21\u578b\nprocessor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")\nmodel = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")\n\n# \u56fe\u50cf\u63cf\u8ff0\u751f\u6210\ninputs = processor(image, return_tensors="pt")\nout = model.generate(**inputs)\ncaption = processor.decode(out[0], skip_special_tokens=True)\nprint(f"\u63cf\u8ff0: {caption}")\n\n# \u89c6\u89c9\u95ee\u7b54\nquestion = "What is in the image?"\ninputs = processor(image, question, return_tensors="pt")\nout = model.generate(**inputs)\nanswer = processor.decode(out[0], skip_special_tokens=True)\nprint(f"\u7b54\u6848: {answer}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"llava\u89c6\u89c9\u6307\u4ee4\u5fae\u8c03",children:"LLaVA\uff08\u89c6\u89c9\u6307\u4ee4\u5fae\u8c03\uff09"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from llava.model import LlavaLlamaForCausalLM\nfrom llava.conversation import conv_templates\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel = LlavaLlamaForCausalLM.from_pretrained("liuhaotian/llava-v1.5-7b")\n\n# \u51c6\u5907\u5bf9\u8bdd\nconv = conv_templates["llava_v1"].copy()\nconv.append_message(conv.roles[0], "What is shown in this image?")\nconv.append_message(conv.roles[1], None)\n\n# \u751f\u6210\u56de\u7b54\nprompt = conv.get_prompt()\ninputs = tokenizer([prompt], return_tensors="pt")\noutput = model.generate(**inputs, images=image_tensor)\nresponse = tokenizer.decode(output[0])\n\nprint(response)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"gpt-4vopenai",children:"GPT-4V\uff08OpenAI\uff09"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\n# \u591a\u6a21\u6001\u5bf9\u8bdd\nresponse = client.chat.completions.create(\n    model="gpt-4-vision-preview",\n    messages=[\n        {\n            "role": "user",\n            "content": [\n                {"type": "text", "text": "\u8fd9\u5f20\u56fe\u7247\u91cc\u6709\u4ec0\u4e48\uff1f"},\n                {\n                    "type": "image_url",\n                    "image_url": {\n                        "url": "https://example.com/image.jpg",\n                    },\n                },\n            ],\n        }\n    ],\n    max_tokens=300,\n)\n\nprint(response.choices[0].message.content)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u5e94\u7528\u573a\u666f",children:"\u5e94\u7528\u573a\u666f"}),"\n",(0,t.jsx)(n.h3,{id:"1-\u56fe\u50cf\u7406\u89e3\u4e0e\u63cf\u8ff0",children:"1. \u56fe\u50cf\u7406\u89e3\u4e0e\u63cf\u8ff0"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class ImageCaptioningSystem:\n    """\u56fe\u50cf\u63cf\u8ff0\u7cfb\u7edf"""\n    \n    def __init__(self, model_name="Salesforce/blip-image-captioning-large"):\n        self.processor = BlipProcessor.from_pretrained(model_name)\n        self.model = BlipForConditionalGeneration.from_pretrained(model_name)\n    \n    def generate_caption(self, image, max_length=50):\n        """\u751f\u6210\u56fe\u50cf\u63cf\u8ff0"""\n        inputs = self.processor(image, return_tensors="pt")\n        outputs = self.model.generate(**inputs, max_length=max_length)\n        caption = self.processor.decode(outputs[0], skip_special_tokens=True)\n        return caption\n    \n    def generate_detailed_caption(self, image):\n        """\u751f\u6210\u8be6\u7ec6\u63cf\u8ff0"""\n        # \u4f7f\u7528\u6761\u4ef6\u751f\u6210\n        text = "a detailed description of"\n        inputs = self.processor(image, text, return_tensors="pt")\n        outputs = self.model.generate(**inputs, max_length=100)\n        caption = self.processor.decode(outputs[0], skip_special_tokens=True)\n        return caption\n\n# \u4f7f\u7528\nsystem = ImageCaptioningSystem()\ncaption = system.generate_caption(image)\nprint(f"\u63cf\u8ff0: {caption}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-\u89c6\u89c9\u95ee\u7b54vqa",children:"2. \u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class VisualQASystem:\n    """\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf"""\n    \n    def __init__(self):\n        self.model = BlipForQuestionAnswering.from_pretrained(\n            "Salesforce/blip-vqa-base"\n        )\n        self.processor = BlipProcessor.from_pretrained(\n            "Salesforce/blip-vqa-base"\n        )\n    \n    def answer_question(self, image, question):\n        """\u56de\u7b54\u5173\u4e8e\u56fe\u50cf\u7684\u95ee\u9898"""\n        inputs = self.processor(image, question, return_tensors="pt")\n        outputs = self.model.generate(**inputs)\n        answer = self.processor.decode(outputs[0], skip_special_tokens=True)\n        return answer\n    \n    def batch_qa(self, image, questions):\n        """\u6279\u91cf\u95ee\u7b54"""\n        answers = []\n        for question in questions:\n            answer = self.answer_question(image, question)\n            answers.append(answer)\n        return answers\n\n# \u4f7f\u7528\nvqa = VisualQASystem()\nquestions = [\n    "What color is the cat?",\n    "How many cats are there?",\n    "What is the cat doing?"\n]\nanswers = vqa.batch_qa(image, questions)\nfor q, a in zip(questions, answers):\n    print(f"Q: {q}\\nA: {a}\\n")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3-\u56fe\u50cf\u68c0\u7d22",children:"3. \u56fe\u50cf\u68c0\u7d22"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class ImageSearchEngine:\n    """\u56fe\u50cf\u641c\u7d22\u5f15\u64ce"""\n    \n    def __init__(self):\n        self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\n        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n        self.image_database = []\n        self.image_embeddings = None\n    \n    def add_images(self, images):\n        """\u6dfb\u52a0\u56fe\u50cf\u5230\u6570\u636e\u5e93"""\n        self.image_database.extend(images)\n        \n        # \u8ba1\u7b97\u56fe\u50cf\u5d4c\u5165\n        inputs = self.processor(images=images, return_tensors="pt")\n        with torch.no_grad():\n            embeddings = self.model.get_image_features(**inputs)\n        \n        if self.image_embeddings is None:\n            self.image_embeddings = embeddings\n        else:\n            self.image_embeddings = torch.cat([self.image_embeddings, embeddings])\n    \n    def search_by_text(self, query, top_k=5):\n        """\u6587\u672c\u641c\u7d22\u56fe\u50cf"""\n        # \u7f16\u7801\u67e5\u8be2\u6587\u672c\n        inputs = self.processor(text=[query], return_tensors="pt")\n        with torch.no_grad():\n            text_embedding = self.model.get_text_features(**inputs)\n        \n        # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\n        similarities = (text_embedding @ self.image_embeddings.T).squeeze()\n        \n        # \u83b7\u53d6top-k\n        top_indices = similarities.argsort(descending=True)[:top_k]\n        \n        results = [(self.image_database[i], similarities[i].item()) \n                   for i in top_indices]\n        \n        return results\n    \n    def search_by_image(self, query_image, top_k=5):\n        """\u56fe\u50cf\u641c\u7d22\u56fe\u50cf"""\n        # \u7f16\u7801\u67e5\u8be2\u56fe\u50cf\n        inputs = self.processor(images=query_image, return_tensors="pt")\n        with torch.no_grad():\n            query_embedding = self.model.get_image_features(**inputs)\n        \n        # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\n        similarities = (query_embedding @ self.image_embeddings.T).squeeze()\n        \n        # \u83b7\u53d6top-k\uff08\u6392\u9664\u81ea\u5df1\uff09\n        top_indices = similarities.argsort(descending=True)[1:top_k+1]\n        \n        results = [(self.image_database[i], similarities[i].item()) \n                   for i in top_indices]\n        \n        return results\n\n# \u4f7f\u7528\nsearch_engine = ImageSearchEngine()\nsearch_engine.add_images(image_list)\n\n# \u6587\u672c\u641c\u7d22\nresults = search_engine.search_by_text("a cat sitting on a couch")\nfor img, score in results:\n    print(f"\u76f8\u4f3c\u5ea6: {score:.4f}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"4-\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210",children:"4. \u6587\u672c\u5230\u56fe\u50cf\u751f\u6210"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from diffusers import StableDiffusionPipeline\n\nclass TextToImageGenerator:\n    """\u6587\u672c\u751f\u6210\u56fe\u50cf"""\n    \n    def __init__(self, model_id="stabilityai/stable-diffusion-2-1"):\n        self.pipe = StableDiffusionPipeline.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16\n        )\n        self.pipe = self.pipe.to("cuda")\n    \n    def generate(self, prompt, negative_prompt="", num_images=1, \n                 guidance_scale=7.5, num_inference_steps=50):\n        """\u751f\u6210\u56fe\u50cf"""\n        images = self.pipe(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images,\n            guidance_scale=guidance_scale,\n            num_inference_steps=num_inference_steps\n        ).images\n        \n        return images\n    \n    def generate_with_controlnet(self, prompt, control_image):\n        """\u4f7f\u7528ControlNet\u751f\u6210"""\n        # \u9700\u8981\u52a0\u8f7dControlNet\u6a21\u578b\n        pass\n\n# \u4f7f\u7528\ngenerator = TextToImageGenerator()\nimages = generator.generate(\n    prompt="a beautiful sunset over the ocean, highly detailed",\n    negative_prompt="blurry, low quality",\n    num_images=4\n)\n\nfor i, img in enumerate(images):\n    img.save(f"generated_{i}.png")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u6700\u4f73\u5b9e\u8df5",children:"\u6700\u4f73\u5b9e\u8df5"}),"\n",(0,t.jsx)(n.h3,{id:"1-\u6570\u636e\u51c6\u5907",children:"1. \u6570\u636e\u51c6\u5907"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MultimodalDataset(torch.utils.data.Dataset):\n    """\u591a\u6a21\u6001\u6570\u636e\u96c6"""\n    \n    def __init__(self, image_paths, captions, processor):\n        self.image_paths = image_paths\n        self.captions = captions\n        self.processor = processor\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # \u52a0\u8f7d\u56fe\u50cf\n        image = Image.open(self.image_paths[idx]).convert(\'RGB\')\n        caption = self.captions[idx]\n        \n        # \u5904\u7406\n        encoding = self.processor(\n            images=image,\n            text=caption,\n            return_tensors="pt",\n            padding="max_length",\n            truncation=True\n        )\n        \n        # \u79fb\u9664batch\u7ef4\u5ea6\n        encoding = {k: v.squeeze() for k, v in encoding.items()}\n        \n        return encoding\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-\u6a21\u578b\u8bad\u7ec3",children:"2. \u6a21\u578b\u8bad\u7ec3"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def train_multimodal_model(model, train_loader, val_loader, epochs=10):\n    """\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b"""\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n    \n    for epoch in range(epochs):\n        # \u8bad\u7ec3\n        model.train()\n        train_loss = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            \n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        # \u9a8c\u8bc1\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                outputs = model(**batch)\n                val_loss += outputs.loss.item()\n        \n        scheduler.step()\n        \n        print(f"Epoch {epoch+1}/{epochs}")\n        print(f"Train Loss: {train_loss/len(train_loader):.4f}")\n        print(f"Val Loss: {val_loss/len(val_loader):.4f}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3-\u63a8\u7406\u4f18\u5316",children:"3. \u63a8\u7406\u4f18\u5316"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'@torch.no_grad()\ndef optimized_inference(model, image, text):\n    """\u4f18\u5316\u7684\u63a8\u7406"""\n    # \u4f7f\u7528\u534a\u7cbe\u5ea6\n    model = model.half()\n    \n    # \u6279\u5904\u7406\n    if isinstance(image, list):\n        batch_size = len(image)\n    else:\n        batch_size = 1\n        image = [image]\n        text = [text]\n    \n    # \u5904\u7406\u8f93\u5165\n    inputs = processor(images=image, text=text, return_tensors="pt")\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    # \u63a8\u7406\n    outputs = model(**inputs)\n    \n    return outputs\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u672a\u6765\u8d8b\u52bf",children:"\u672a\u6765\u8d8b\u52bf"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"\u7edf\u4e00\u7684\u591a\u6a21\u6001\u67b6\u6784"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u5355\u4e00\u6a21\u578b\u5904\u7406\u6240\u6709\u6a21\u6001"}),"\n",(0,t.jsx)(n.li,{children:"\u7aef\u5230\u7aef\u8bad\u7ec3"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"\u66f4\u591a\u6a21\u6001\u878d\u5408"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u6587\u672c+\u56fe\u50cf+\u97f3\u9891+\u89c6\u9891+3D"}),"\n",(0,t.jsx)(n.li,{children:"\u4f20\u611f\u5668\u6570\u636e\u878d\u5408"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"\u5177\u8eab\u667a\u80fd\uff08Embodied AI\uff09"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u673a\u5668\u4eba\u89c6\u89c9"}),"\n",(0,t.jsx)(n.li,{children:"\u73af\u5883\u7406\u89e3"}),"\n",(0,t.jsx)(n.li,{children:"\u52a8\u4f5c\u89c4\u5212"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"\u5b9e\u65f6\u591a\u6a21\u6001\u4ea4\u4e92"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u4f4e\u5ef6\u8fdf\u63a8\u7406"}),"\n",(0,t.jsx)(n.li,{children:"\u6d41\u5f0f\u5904\u7406"}),"\n",(0,t.jsx)(n.li,{children:"\u8fb9\u7f18\u90e8\u7f72"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"\u603b\u7ed3",children:"\u603b\u7ed3"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"\u5173\u952e\u8981\u70b9"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"\u591a\u6a21\u6001AI\u878d\u5408\u591a\u79cd\u6570\u636e\u7c7b\u578b"}),"\n",(0,t.jsx)(n.li,{children:"CLIP\u5f00\u542f\u4e86\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u65b0\u65f6\u4ee3"}),"\n",(0,t.jsx)(n.li,{children:"\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u662f\u6838\u5fc3\u6280\u672f"}),"\n",(0,t.jsx)(n.li,{children:"\u5e94\u7528\u5e7f\u6cdb\uff1aVQA\u3001\u56fe\u50cf\u68c0\u7d22\u3001\u751f\u6210\u7b49"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"\u5b66\u4e60\u5efa\u8bae"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u7406\u89e3CLIP\u7684\u5bf9\u6bd4\u5b66\u4e60\u539f\u7406"}),"\n",(0,t.jsx)(n.li,{children:"\u5b9e\u8df5\u4e0d\u540c\u7684\u878d\u5408\u7b56\u7565"}),"\n",(0,t.jsx)(n.li,{children:"\u5173\u6ce8\u6700\u65b0\u7684\u591a\u6a21\u6001\u6a21\u578b"}),"\n",(0,t.jsx)(n.li,{children:"\u52a8\u624b\u5b9e\u73b0\u5e94\u7528\u9879\u76ee"}),"\n"]}),"\n",(0,t.jsx)(s,{})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},28453(e,n,s){s.d(n,{R:()=>a,x:()=>l});var i=s(96540);const t={},r=i.createContext(t);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);