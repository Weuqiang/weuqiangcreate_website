---
sidebar_position: 7
title:  - 
---

#  - 

****ETL

## 

### 

**Data Warehouse**

** vs **

|  | OLTP | OLAP |
|------|---------------|-----------------|
|  |  |  |
|  | GB-TB | TB-PB |
|  |  |  |
|  |  |  |
|  |  |  |

### 

```

              Source                  
  API                     

                    ↓ 

              ODS                 
                          

                    ↓ 

              DWD                 
                        

                    ↓ 

              DWS                 
                             

                    ↓ 

              ADS                 
                               

```

## 

### 

```
        
           
        
              ↓
    
   →      ←   
    
              ↑
        
           
        
```

### 

```
        
           
        
              ↓
    
   →      ←   
    
      ↓              ↑              ↓
    
             
    
```

##  - 

### 

****
1. 
2. 
3. 
4. 
5. 

### 

```sql
-- 
-- 
CREATE TABLE users (
    user_id BIGINT,
    username VARCHAR(50),
    gender VARCHAR(10),
    birthday DATE,
    city VARCHAR(50),
    register_time TIMESTAMP
);

-- 
CREATE TABLE products (
    product_id BIGINT,
    product_name VARCHAR(200),
    category_id BIGINT,
    price DECIMAL(10,2),
    stock INT
);

-- 
CREATE TABLE orders (
    order_id BIGINT,
    user_id BIGINT,
    order_time TIMESTAMP,
    total_amount DECIMAL(10,2),
    status VARCHAR(20)
);

-- 
CREATE TABLE order_items (
    order_id BIGINT,
    product_id BIGINT,
    quantity INT,
    price DECIMAL(10,2)
);

-- 
CREATE TABLE user_logs (
    log_id BIGINT,
    user_id BIGINT,
    action VARCHAR(20),
    item_id BIGINT,
    timestamp TIMESTAMP
);
```

### ODS

```sql
-- ODSHive

-- ODS
CREATE EXTERNAL TABLE ods_users (
    user_id BIGINT,
    username STRING,
    gender STRING,
    birthday STRING,
    city STRING,
    register_time STRING
)
PARTITIONED BY (dt STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
LOCATION '/warehouse/ods/users';

-- ODS
CREATE EXTERNAL TABLE ods_orders (
    order_id BIGINT,
    user_id BIGINT,
    order_time STRING,
    total_amount DECIMAL(10,2),
    status STRING
)
PARTITIONED BY (dt STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
LOCATION '/warehouse/ods/orders';

-- 
-- SqoopMySQLHDFS
sqoop import \
  --connect jdbc:mysql://localhost:3306/ecommerce \
  --username root \
  --password password \
  --table orders \
  --hive-import \
  --hive-table ods_orders \
  --hive-partition-key dt \
  --hive-partition-value '2024-01-01' \
  --target-dir /warehouse/ods/orders/dt=2024-01-01
```

### DWD

```sql
-- 

-- 
CREATE TABLE dwd_dim_date (
    date_id STRING,
    date_value DATE,
    year INT,
    quarter INT,
    month INT,
    week INT,
    day INT,
    weekday INT,
    is_weekend INT,
    is_holiday INT
)
STORED AS PARQUET;

-- 
INSERT INTO dwd_dim_date
SELECT 
    date_format(date_value, 'yyyyMMdd') as date_id,
    date_value,
    year(date_value) as year,
    quarter(date_value) as quarter,
    month(date_value) as month,
    weekofyear(date_value) as week,
    day(date_value) as day,
    dayofweek(date_value) as weekday,
    CASE WHEN dayofweek(date_value) IN (1,7) THEN 1 ELSE 0 END as is_weekend,
    0 as is_holiday
FROM (
    SELECT date_add('2020-01-01', pos) as date_value
    FROM (SELECT posexplode(split(space(1825), ' '))) t
) dates;

-- 
CREATE TABLE dwd_dim_user (
    user_id BIGINT,
    username STRING,
    gender STRING,
    age INT,
    city STRING,
    register_date STRING,
    user_level STRING
)
PARTITIONED BY (dt STRING)
STORED AS PARQUET;

-- 
CREATE TABLE dwd_dim_product (
    product_id BIGINT,
    product_name STRING,
    category_id BIGINT,
    category_name STRING,
    price DECIMAL(10,2)
)
PARTITIONED BY (dt STRING)
STORED AS PARQUET;

-- 

-- 
CREATE TABLE dwd_fact_order (
    order_id BIGINT,
    user_id BIGINT,
    product_id BIGINT,
    quantity INT,
    price DECIMAL(10,2),
    amount DECIMAL(10,2),
    order_time STRING,
    date_id STRING
)
PARTITIONED BY (dt STRING)
STORED AS PARQUET;

-- ETLODSDWD
INSERT OVERWRITE TABLE dwd_fact_order PARTITION(dt='2024-01-01')
SELECT 
    o.order_id,
    o.user_id,
    oi.product_id,
    oi.quantity,
    oi.price,
    oi.quantity * oi.price as amount,
    o.order_time,
    date_format(o.order_time, 'yyyyMMdd') as date_id
FROM ods_orders o
JOIN ods_order_items oi ON o.order_id = oi.order_id
WHERE o.dt = '2024-01-01'
  AND o.status = 'completed';
```

### DWS

```sql
-- 

-- 
CREATE TABLE dws_user_daily (
    user_id BIGINT,
    order_count BIGINT,
    order_amount DECIMAL(10,2),
    view_count BIGINT,
    cart_count BIGINT
)
PARTITIONED BY (dt STRING)
STORED AS PARQUET;

-- ETL
INSERT OVERWRITE TABLE dws_user_daily PARTITION(dt='2024-01-01')
SELECT 
    user_id,
    COUNT(DISTINCT order_id) as order_count,
    SUM(amount) as order_amount,
    SUM(CASE WHEN action='view' THEN 1 ELSE 0 END) as view_count,
    SUM(CASE WHEN action='cart' THEN 1 ELSE 0 END) as cart_count
FROM (
    -- 
    SELECT user_id, order_id, amount, NULL as action
    FROM dwd_fact_order
    WHERE dt = '2024-01-01'
    
    UNION ALL
    
    -- 
    SELECT user_id, NULL as order_id, NULL as amount, action
    FROM ods_user_logs
    WHERE dt = '2024-01-01'
) t
GROUP BY user_id;

-- 

-- 
CREATE TABLE dws_product_daily (
    product_id BIGINT,
    order_count BIGINT,
    sales_quantity BIGINT,
    sales_amount DECIMAL(10,2),
    view_count BIGINT
)
PARTITIONED BY (dt STRING)
STORED AS PARQUET;

-- ETL
INSERT OVERWRITE TABLE dws_product_daily PARTITION(dt='2024-01-01')
SELECT 
    product_id,
    COUNT(DISTINCT order_id) as order_count,
    SUM(quantity) as sales_quantity,
    SUM(amount) as sales_amount,
    SUM(CASE WHEN action='view' THEN 1 ELSE 0 END) as view_count
FROM (
    SELECT product_id, order_id, quantity, amount, NULL as action
    FROM dwd_fact_order
    WHERE dt = '2024-01-01'
    
    UNION ALL
    
    SELECT item_id as product_id, NULL, NULL, NULL, action
    FROM ods_user_logs
    WHERE dt = '2024-01-01'
) t
GROUP BY product_id;
```

### ADS

```sql
-- 
CREATE TABLE ads_sales_daily (
    date_id STRING,
    order_count BIGINT,
    user_count BIGINT,
    sales_amount DECIMAL(10,2),
    avg_order_amount DECIMAL(10,2)
)
STORED AS PARQUET;

INSERT INTO ads_sales_daily
SELECT 
    dt as date_id,
    SUM(order_count) as order_count,
    COUNT(DISTINCT user_id) as user_count,
    SUM(order_amount) as sales_amount,
    AVG(order_amount) as avg_order_amount
FROM dws_user_daily
WHERE dt = '2024-01-01'
GROUP BY dt;

-- 
CREATE TABLE ads_product_top (
    date_id STRING,
    product_id BIGINT,
    product_name STRING,
    sales_quantity BIGINT,
    sales_amount DECIMAL(10,2),
    rank INT
)
STORED AS PARQUET;

INSERT INTO ads_product_top
SELECT 
    dt as date_id,
    p.product_id,
    d.product_name,
    p.sales_quantity,
    p.sales_amount,
    ROW_NUMBER() OVER (PARTITION BY dt ORDER BY p.sales_amount DESC) as rank
FROM dws_product_daily p
JOIN dwd_dim_product d ON p.product_id = d.product_id
WHERE p.dt = '2024-01-01'
  AND d.dt = '2024-01-01';

-- RFM
CREATE TABLE ads_user_rfm (
    user_id BIGINT,
    recency INT,
    frequency BIGINT,
    monetary DECIMAL(10,2),
    r_score INT,
    f_score INT,
    m_score INT,
    rfm_score STRING,
    user_level STRING
)
PARTITIONED BY (dt STRING)
STORED AS PARQUET;

INSERT OVERWRITE TABLE ads_user_rfm PARTITION(dt='2024-01-01')
WITH user_metrics AS (
    SELECT 
        user_id,
        DATEDIFF('2024-01-01', MAX(dt)) as recency,
        SUM(order_count) as frequency,
        SUM(order_amount) as monetary
    FROM dws_user_daily
    WHERE dt <= '2024-01-01'
    GROUP BY user_id
),
user_scores AS (
    SELECT 
        user_id,
        recency,
        frequency,
        monetary,
        NTILE(5) OVER (ORDER BY recency) as r_score,
        NTILE(5) OVER (ORDER BY frequency DESC) as f_score,
        NTILE(5) OVER (ORDER BY monetary DESC) as m_score
    FROM user_metrics
)
SELECT 
    user_id,
    recency,
    frequency,
    monetary,
    r_score,
    f_score,
    m_score,
    CONCAT(r_score, f_score, m_score) as rfm_score,
    CASE 
        WHEN r_score >= 4 AND f_score >= 4 AND m_score >= 4 THEN ''
        WHEN r_score >= 4 AND f_score >= 4 THEN ''
        WHEN r_score >= 4 AND m_score >= 4 THEN ''
        WHEN r_score >= 4 THEN ''
        WHEN f_score >= 4 AND m_score >= 4 THEN ''
        ELSE ''
    END as user_level
FROM user_scores;
```

## ETL

### Airflow

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'ecommerce_etl',
    default_args=default_args,
    description='ETL',
    schedule_interval='0 2 * * *',  # 2
    catchup=False
)

# ODS
ods_import = BashOperator(
    task_id='ods_import',
    bash_command='sh /scripts/ods_import.sh {{ ds }}',
    dag=dag
)

# DWD
dwd_process = BashOperator(
    task_id='dwd_process',
    bash_command='hive -f /scripts/dwd_process.sql -hivevar dt={{ ds }}',
    dag=dag
)

# DWS
dws_aggregate = BashOperator(
    task_id='dws_aggregate',
    bash_command='hive -f /scripts/dws_aggregate.sql -hivevar dt={{ ds }}',
    dag=dag
)

# ADS
ads_report = BashOperator(
    task_id='ads_report',
    bash_command='hive -f /scripts/ads_report.sql -hivevar dt={{ ds }}',
    dag=dag
)

# 
data_quality_check = BashOperator(
    task_id='data_quality_check',
    bash_command='python /scripts/data_quality_check.py {{ ds }}',
    dag=dag
)

# 
ods_import >> dwd_process >> dws_aggregate >> ads_report >> data_quality_check
```

## 

### 

```python
import happybase
from datetime import datetime

class DataQualityChecker:
    """"""
    
    def __init__(self):
        self.checks = []
        self.results = []
    
    def check_null_rate(self, table, column, threshold=0.1):
        """"""
        query = f"""
        SELECT 
            COUNT(*) as total,
            SUM(CASE WHEN {column} IS NULL THEN 1 ELSE 0 END) as null_count
        FROM {table}
        WHERE dt = '{{{{ ds }}}}'
        """
        # 
        # null_rate = null_count / total
        # return null_rate < threshold
        pass
    
    def check_duplicate_rate(self, table, key_columns):
        """"""
        key_cols = ', '.join(key_columns)
        query = f"""
        SELECT 
            COUNT(*) as total,
            COUNT(DISTINCT {key_cols}) as distinct_count
        FROM {table}
        WHERE dt = '{{{{ ds }}}}'
        """
        # duplicate_rate = 1 - distinct_count / total
        pass
    
    def check_data_freshness(self, table, time_column, hours=24):
        """"""
        query = f"""
        SELECT MAX({time_column}) as max_time
        FROM {table}
        WHERE dt = '{{{{ ds }}}}'
        """
        # 
        pass
    
    def check_data_volume(self, table, min_count=1000):
        """"""
        query = f"""
        SELECT COUNT(*) as count
        FROM {table}
        WHERE dt = '{{{{ ds }}}}'
        """
        # return count >= min_count
        pass
    
    def run_all_checks(self):
        """"""
        print("...")
        
        # 
        for check in self.checks:
            result = check()
            self.results.append(result)
        
        # 
        self.generate_report()
    
    def generate_report(self):
        """"""
        print("\n")
        print("=" * 50)
        
        passed = sum(1 for r in self.results if r['status'] == 'pass')
        failed = len(self.results) - passed
        
        print(f": {len(self.results)}")
        print(f": {passed}")
        print(f": {failed}")
        
        if failed > 0:
            print("\n:")
            for r in self.results:
                if r['status'] == 'fail':
                    print(f"  - {r['name']}: {r['message']}")
```

## 

### 1. 

```sql
-- 
CREATE TABLE orders
PARTITIONED BY (dt STRING);

-- 
CREATE TABLE logs
PARTITIONED BY (dt STRING, hour STRING);

-- 
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
```

### 2. 

```sql
-- 
CREATE TABLE data_orc
STORED AS ORC
TBLPROPERTIES ("orc.compress"="SNAPPY");

-- Parquet
CREATE TABLE data_parquet
STORED AS PARQUET;
```

### 3. 

```sql
-- CBO
SET hive.cbo.enable=true;

-- 
ANALYZE TABLE orders COMPUTE STATISTICS;
ANALYZE TABLE orders COMPUTE STATISTICS FOR COLUMNS;

-- Map Join
SELECT /*+ MAPJOIN(small_table) */
    *
FROM large_table
JOIN small_table ON large_table.id = small_table.id;
```

## 



1. ****ODS → DWD → DWS → ADS
2. ****
3. **ETL**
4. ****

****

## 

1. 
2. 
3. 
4. 

[HBase](./HBase.mdx)

