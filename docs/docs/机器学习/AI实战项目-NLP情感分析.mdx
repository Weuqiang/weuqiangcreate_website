---
sidebar_position: 21
title: AINLP
tags: [AI, NLP, BERT, Transformer, ]
---

# AINLP

## 

BERT

### 

- ****: PyTorch, Transformers
- **NLP**: NLTK, spaCy
- ****: Pandas, NumPy
- **API**: FastAPI
- ****: Vue 3

### 

-  //
-  
-  
-  
-  API

## 

### 1. 

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer

class SentimentDataset(Dataset):
    """"""
    
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # 
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

def load_data(file_path):
    """"""
    df = pd.read_csv(file_path)
    
    # 
    df = df.dropna()
    df['text'] = df['text'].str.lower()
    df['text'] = df['text'].str.replace(r'http\S+', '', regex=True)
    df['text'] = df['text'].str.replace(r'@\w+', '', regex=True)
    df['text'] = df['text'].str.replace(r'#\w+', '', regex=True)
    
    # 
    label_map = {'negative': 0, 'neutral': 1, 'positive': 2}
    df['label'] = df['sentiment'].map(label_map)
    
    return df

def prepare_dataloaders(df, tokenizer, batch_size=32, test_size=0.2):
    """"""
    
    # 
    train_df, test_df = train_test_split(
        df, test_size=test_size, random_state=42, stratify=df['label']
    )
    
    # 
    train_dataset = SentimentDataset(
        train_df['text'].values,
        train_df['label'].values,
        tokenizer
    )
    
    test_dataset = SentimentDataset(
        test_df['text'].values,
        test_df['label'].values,
        tokenizer
    )
    
    # 
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4
    )
    
    return train_loader, test_loader

# 
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
df = load_data('sentiment_data.csv')
train_loader, test_loader = prepare_dataloaders(df, tokenizer)
```

### 2. 

```python
import nlpaug.augmenter.word as naw
import nlpaug.augmenter.sentence as nas

class TextAugmenter:
    """"""
    
    def __init__(self):
        # 
        self.synonym_aug = naw.SynonymAug(aug_src='wordnet')
        
        # 
        self.back_translation_aug = naw.BackTranslationAug(
            from_model_name='facebook/wmt19-en-de',
            to_model_name='facebook/wmt19-de-en'
        )
        
        # 
        self.contextual_aug = naw.ContextualWordEmbsAug(
            model_path='bert-base-uncased',
            action="substitute"
        )
    
    def augment_synonym(self, text):
        """"""
        return self.synonym_aug.augment(text)
    
    def augment_back_translation(self, text):
        """"""
        return self.back_translation_aug.augment(text)
    
    def augment_contextual(self, text):
        """"""
        return self.contextual_aug.augment(text)
    
    def augment_all(self, texts, num_aug=2):
        """"""
        augmented_texts = []
        
        for text in texts:
            augmented_texts.append(text)  # 
            
            # 
            methods = [
                self.augment_synonym,
                self.augment_contextual
            ]
            
            for _ in range(num_aug):
                method = np.random.choice(methods)
                try:
                    aug_text = method(text)
                    augmented_texts.append(aug_text)
                except:
                    continue
        
        return augmented_texts
```

## 

### 1. BERT

```python
import torch.nn as nn
from transformers import BertModel, BertConfig

class BERTSentimentClassifier(nn.Module):
    """BERT"""
    
    def __init__(self, num_classes=3, dropout=0.3):
        super(BERTSentimentClassifier, self).__init__()
        
        # BERT
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        
        # 
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)
    
    def forward(self, input_ids, attention_mask):
        # BERT
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # [CLS]
        pooled_output = outputs.pooler_output
        
        # 
        output = self.dropout(pooled_output)
        logits = self.classifier(output)
        
        return logits

class BERTWithAttention(nn.Module):
    """BERT"""
    
    def __init__(self, num_classes=3, dropout=0.3):
        super(BERTWithAttention, self).__init__()
        
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        hidden_size = self.bert.config.hidden_size
        
        # 
        self.attention = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1),
            nn.Softmax(dim=1)
        )
        
        # 
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(hidden_size, num_classes)
    
    def forward(self, input_ids, attention_mask):
        # BERT
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # token
        sequence_output = outputs.last_hidden_state
        
        # 
        attention_weights = self.attention(sequence_output)
        
        # 
        weighted_output = torch.sum(attention_weights * sequence_output, dim=1)
        
        # 
        output = self.dropout(weighted_output)
        logits = self.classifier(output)
        
        return logits, attention_weights
```

### 2. 

```python
class MultiTaskBERT(nn.Module):
    """BERT+"""
    
    def __init__(self, num_classes=3, dropout=0.3):
        super(MultiTaskBERT, self).__init__()
        
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        hidden_size = self.bert.config.hidden_size
        
        # 
        self.shared = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # 
        self.classifier = nn.Linear(hidden_size, num_classes)
        
        # 
        self.regressor = nn.Linear(hidden_size, 1)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        pooled_output = outputs.pooler_output
        shared_output = self.shared(pooled_output)
        
        # 
        class_logits = self.classifier(shared_output)
        
        # 
        intensity = self.regressor(shared_output)
        
        return class_logits, intensity
```

## 

### 1. 

```python
from transformers import AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm

class SentimentTrainer:
    """"""
    
    def __init__(self, model, train_loader, test_loader, device='cuda'):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.device = device
        
        # 
        self.optimizer = AdamW(
            model.parameters(),
            lr=2e-5,
            eps=1e-8
        )
        
        # 
        total_steps = len(train_loader) * 10  # 10 epochs
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=0,
            num_training_steps=total_steps
        )
        
        # 
        self.criterion = nn.CrossEntropyLoss()
        
        # 
        self.best_acc = 0
    
    def train_epoch(self, epoch):
        """epoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        progress_bar = tqdm(self.train_loader, desc=f'Epoch {epoch}')
        
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['label'].to(self.device)
            
            # 
            self.optimizer.zero_grad()
            logits = self.model(input_ids, attention_mask)
            loss = self.criterion(logits, labels)
            
            # 
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            self.scheduler.step()
            
            # 
            total_loss += loss.item()
            _, predicted = torch.max(logits, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            # 
            progress_bar.set_postfix({
                'loss': total_loss / (progress_bar.n + 1),
                'acc': 100. * correct / total
            })
        
        return total_loss / len(self.train_loader), 100. * correct / total
    
    def evaluate(self):
        """"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(self.test_loader, desc='Evaluating'):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['label'].to(self.device)
                
                logits = self.model(input_ids, attention_mask)
                loss = self.criterion(logits, labels)
                
                total_loss += loss.item()
                _, predicted = torch.max(logits, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        acc = 100. * correct / total
        
        # 
        if acc > self.best_acc:
            self.best_acc = acc
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'acc': acc,
            }, 'best_sentiment_model.pth')
            print(f'Model saved! Acc: {acc:.2f}%')
        
        return total_loss / len(self.test_loader), acc, all_preds, all_labels
    
    def train(self, num_epochs=10):
        """"""
        for epoch in range(num_epochs):
            print(f'\nEpoch {epoch + 1}/{num_epochs}')
            train_loss, train_acc = self.train_epoch(epoch)
            test_loss, test_acc, preds, labels = self.evaluate()
            
            print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')
            print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')
        
        print(f'\nBest Test Acc: {self.best_acc:.2f}%')
```

### 2. 

```python
class AdversarialTrainer(SentimentTrainer):
    """"""
    
    def __init__(self, model, train_loader, test_loader, device='cuda', epsilon=1e-3):
        super().__init__(model, train_loader, test_loader, device)
        self.epsilon = epsilon
    
    def train_epoch(self, epoch):
        """"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch in tqdm(self.train_loader, desc=f'Epoch {epoch}'):
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['label'].to(self.device)
            
            # 
            self.optimizer.zero_grad()
            logits = self.model(input_ids, attention_mask)
            loss = self.criterion(logits, labels)
            loss.backward()
            
            # 
            embeddings = self.model.bert.embeddings.word_embeddings
            embedding_gradients = embeddings.weight.grad
            
            if embedding_gradients is not None:
                # 
                delta = self.epsilon * embedding_gradients / (
                    torch.norm(embedding_gradients) + 1e-8
                )
                
                # 
                embeddings.weight.data += delta
                
                # 
                adv_logits = self.model(input_ids, attention_mask)
                adv_loss = self.criterion(adv_logits, labels)
                adv_loss.backward()
                
                # 
                embeddings.weight.data -= delta
            
            # 
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            self.scheduler.step()
            
            # 
            total_loss += loss.item()
            _, predicted = torch.max(logits, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        return total_loss / len(self.train_loader), 100. * correct / total
```

## 

### 1. FastAPI

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import torch

app = FastAPI(title="API")

# 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BERTSentimentClassifier(num_classes=3)
checkpoint = torch.load('best_sentiment_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.to(device)
model.eval()

# 
label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}

class TextInput(BaseModel):
    text: str

class BatchTextInput(BaseModel):
    texts: List[str]

class SentimentOutput(BaseModel):
    text: str
    sentiment: str
    confidence: float
    probabilities: dict

@app.post("/predict", response_model=SentimentOutput)
async def predict_sentiment(input_data: TextInput):
    """"""
    try:
        # 
        encoding = tokenizer.encode_plus(
            input_data.text,
            add_special_tokens=True,
            max_length=128,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].to(device)
        attention_mask = encoding['attention_mask'].to(device)
        
        # 
        with torch.no_grad():
            logits = model(input_ids, attention_mask)
            probabilities = torch.softmax(logits, dim=1)
            confidence, predicted = torch.max(probabilities, 1)
        
        # 
        return SentimentOutput(
            text=input_data.text,
            sentiment=label_map[predicted.item()],
            confidence=confidence.item(),
            probabilities={
                label_map[i]: prob.item()
                for i, prob in enumerate(probabilities[0])
            }
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predict_batch", response_model=List[SentimentOutput])
async def predict_batch(input_data: BatchTextInput):
    """"""
    results = []
    
    for text in input_data.texts:
        result = await predict_sentiment(TextInput(text=text))
        results.append(result)
    
    return results

@app.get("/health")
async def health_check():
    """"""
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2. Vue 3

```vue
<template>
  <div class="sentiment-analyzer">
    <h1></h1>
    
    <div class="input-section">
      <textarea
        v-model="text"
        placeholder="..."
        rows="5"
      ></textarea>
      
      <button @click="analyze" :disabled="!text || loading">
        {{ loading ? '...' : '' }}
      </button>
    </div>
    
    <div v-if="result" class="result-section">
      <h2></h2>
      
      <div class="sentiment-badge" :class="result.sentiment">
        {{ sentimentLabel[result.sentiment] }}
      </div>
      
      <div class="confidence">
        : {{ (result.confidence * 100).toFixed(2) }}%
      </div>
      
      <div class="probabilities">
        <h3>:</h3>
        <div
          v-for="(prob, label) in result.probabilities"
          :key="label"
          class="prob-bar"
        >
          <span class="label">{{ sentimentLabel[label] }}</span>
          <div class="bar">
            <div
              class="fill"
              :style="{ width: (prob * 100) + '%' }"
              :class="label"
            ></div>
          </div>
          <span class="value">{{ (prob * 100).toFixed(2) }}%</span>
        </div>
      </div>
    </div>
  </div>
</template>

<script setup>
import { ref } from 'vue';
import axios from 'axios';

const text = ref('');
const result = ref(null);
const loading = ref(false);

const sentimentLabel = {
  positive: '',
  neutral: '',
  negative: ''
};

const analyze = async () => {
  loading.value = true;
  
  try {
    const response = await axios.post('http://localhost:8000/predict', {
      text: text.value
    });
    result.value = response.data;
  } catch (error) {
    console.error('Error:', error);
    alert('');
  } finally {
    loading.value = false;
  }
};
</script>

<style scoped>
.sentiment-analyzer {
  max-width: 800px;
  margin: 0 auto;
  padding: 20px;
}

textarea {
  width: 100%;
  padding: 10px;
  border: 1px solid #ddd;
  border-radius: 4px;
  font-size: 16px;
}

button {
  margin-top: 10px;
  padding: 10px 20px;
  background: #4CAF50;
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
}

button:disabled {
  background: #ccc;
  cursor: not-allowed;
}

.sentiment-badge {
  display: inline-block;
  padding: 10px 20px;
  border-radius: 20px;
  font-weight: bold;
  margin: 10px 0;
}

.sentiment-badge.positive {
  background: #4CAF50;
  color: white;
}

.sentiment-badge.neutral {
  background: #FFC107;
  color: white;
}

.sentiment-badge.negative {
  background: #F44336;
  color: white;
}

.prob-bar {
  display: flex;
  align-items: center;
  margin: 10px 0;
}

.bar {
  flex: 1;
  height: 20px;
  background: #eee;
  border-radius: 10px;
  margin: 0 10px;
  overflow: hidden;
}

.fill {
  height: 100%;
  transition: width 0.3s;
}

.fill.positive {
  background: #4CAF50;
}

.fill.neutral {
  background: #FFC107;
}

.fill.negative {
  background: #F44336;
}
</style>
```

## 



1. **NLP**: 
2. **BERT**: 
3. ****: 
4. ****: FastAPI
5. ****: 

## 

- [Transformers](https://huggingface.co/docs/transformers/)
- [BERT](https://arxiv.org/abs/1810.04805)
- [FastAPI](https://fastapi.tiangolo.com/)

