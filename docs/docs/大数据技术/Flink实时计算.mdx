---
sidebar_position: 4
title: Flink - 
---

# Flink - 

Flink**Flink**

## Flink

### Flink vs Spark Streaming

```python
# Spark Streaming
# 1
stream.window(1, 'seconds').process()

# Flink
# 
stream.process()  # 
```

**Flink**
- ****
- ****Exactly-Once
- ****
- ****
- ****API

## 

### Flink

**1Docker**

```yaml
# docker-compose.yml
version: '3'
services:
  jobmanager:
    image: flink:1.17
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager

  taskmanager:
    image: flink:1.17
    depends_on:
      - jobmanager
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 2
```

```bash
# 
docker-compose up -d

# Web UI
# http://localhost:8081
```

**2PyFlink**

```bash
pip install apache-flink
```

### Flink

```python
from pyflink.datastream import StreamExecutionEnvironment

# 
env = StreamExecutionEnvironment.get_execution_environment()

# 
data = env.from_collection([1, 2, 3, 4, 5])

# 
result = data.map(lambda x: x * 2)

# 
result.print()

# 
env.execute("First Flink Job")
```

## DataStream API

### 

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common.typeinfo import Types

env = StreamExecutionEnvironment.get_execution_environment()

# 1. map
data = env.from_collection([1, 2, 3, 4, 5])
result = data.map(lambda x: x * 2, output_type=Types.INT())

# 2. filter
result = data.filter(lambda x: x % 2 == 0)

# 3. flat_map
text = env.from_collection(["hello world", "flink is great"])
words = text.flat_map(
    lambda line: line.split(),
    output_type=Types.STRING()
)

# 4. key_bykey
pairs = env.from_collection([("a", 1), ("b", 2), ("a", 3)])
keyed = pairs.key_by(lambda x: x[0])

# 5. reduce
result = keyed.reduce(lambda x, y: (x[0], x[1] + y[1]))

# 6. window
from pyflink.datastream.window import TumblingProcessingTimeWindows
from pyflink.common.time import Time

windowed = keyed.window(TumblingProcessingTimeWindows.of(Time.seconds(5)))
result = windowed.reduce(lambda x, y: (x[0], x[1] + y[1]))

result.print()
env.execute("DataStream Operations")
```

### WordCount

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common.typeinfo import Types

def word_count():
    """WordCount"""
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # socket
    # nc -lk 9999
    text = env.socket_text_stream("localhost", 9999)
    
    # 
    counts = text.flat_map(lambda line: line.split()) \
        .map(lambda word: (word, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \
        .key_by(lambda x: x[0]) \
        .reduce(lambda x, y: (x[0], x[1] + y[1]))
    
    # 
    counts.print()
    
    # 
    env.execute("Streaming WordCount")

if __name__ == '__main__':
    word_count()
```

## 

### 

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.window import TumblingProcessingTimeWindows, \
    SlidingProcessingTimeWindows, SessionWindows
from pyflink.common.time import Time
from pyflink.common.typeinfo import Types

env = StreamExecutionEnvironment.get_execution_environment()

# 
data = env.from_collection([
    ("user1", 100),
    ("user2", 200),
    ("user1", 150),
])

keyed = data.key_by(lambda x: x[0])

# 1. 
tumbling = keyed.window(TumblingProcessingTimeWindows.of(Time.seconds(10)))

# 2. 
sliding = keyed.window(SlidingProcessingTimeWindows.of(
    Time.seconds(10),  # 
    Time.seconds(5)    # 
))

# 3. 
session = keyed.window(SessionWindows.with_gap(Time.seconds(30)))

# 
result = tumbling.reduce(lambda x, y: (x[0], x[1] + y[1]))
result.print()

env.execute("Window Operations")
```

### 

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.window import TumblingProcessingTimeWindows
from pyflink.common.time import Time
from pyflink.common.typeinfo import Types
import json

class SalesAnalyzer:
    """"""
    
    @staticmethod
    def run():
        env = StreamExecutionEnvironment.get_execution_environment()
        
        # Kafka
        from pyflink.datastream.connectors import FlinkKafkaConsumer
        from pyflink.common.serialization import SimpleStringSchema
        
        kafka_consumer = FlinkKafkaConsumer(
            topics='orders',
            deserialization_schema=SimpleStringSchema(),
            properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'flink-group'}
        )
        
        orders = env.add_source(kafka_consumer)
        
        # JSON
        parsed = orders.map(
            lambda x: json.loads(x),
            output_type=Types.MAP(Types.STRING(), Types.STRING())
        )
        
        # 
        amounts = parsed.map(
            lambda x: (x['category'], float(x['amount'])),
            output_type=Types.TUPLE([Types.STRING(), Types.FLOAT()])
        )
        
        # 5
        result = amounts.key_by(lambda x: x[0]) \
            .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) \
            .reduce(lambda x, y: (x[0], x[1] + y[1]))
        
        # 
        result.print()
        
        env.execute("Real-time Sales Analysis")

if __name__ == '__main__':
    SalesAnalyzer.run()
```

## 

### 

****

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import MapFunction, RuntimeContext
from pyflink.common.typeinfo import Types

class StatefulMapper(MapFunction):
    """Map"""
    
    def open(self, runtime_context: RuntimeContext):
        """"""
        # 
        from pyflink.datastream.state import ValueStateDescriptor
        
        descriptor = ValueStateDescriptor(
            "count",  # 
            Types.INT()  # 
        )
        
        # 
        self.count_state = runtime_context.get_state(descriptor)
    
    def map(self, value):
        """"""
        # 
        count = self.count_state.value()
        if count is None:
            count = 0
        
        # 
        count += 1
        self.count_state.update(count)
        
        return f"{value}: count={count}"

# 
env = StreamExecutionEnvironment.get_execution_environment()
data = env.from_collection(["a", "b", "a", "c", "a"])

result = data.key_by(lambda x: x).map(
    StatefulMapper(),
    output_type=Types.STRING()
)

result.print()
env.execute("Stateful Processing")
```

### 

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import KeyedProcessFunction
from pyflink.common.typeinfo import Types
import json

class UserBehaviorTracker(KeyedProcessFunction):
    """"""
    
    def open(self, runtime_context):
        from pyflink.datastream.state import ValueStateDescriptor, ListStateDescriptor
        
        # 
        self.last_active = runtime_context.get_state(
            ValueStateDescriptor("last_active", Types.LONG())
        )
        
        # 
        self.behavior_history = runtime_context.get_list_state(
            ListStateDescriptor("history", Types.STRING())
        )
    
    def process_element(self, value, ctx):
        """"""
        user_id = value[0]
        action = value[1]
        timestamp = ctx.timestamp()
        
        # 
        last_time = self.last_active.value()
        self.last_active.update(timestamp)
        
        # 
        self.behavior_history.add(action)
        
        # 
        history = list(self.behavior_history.get())
        activity_score = len(history)
        
        # 
        if last_time and (timestamp - last_time) < 1000:  # 1
            yield f" : {user_id}"
        
        yield f"{user_id}: {action}, ={activity_score}"

# 
env = StreamExecutionEnvironment.get_execution_environment()

# 
behaviors = env.from_collection([
    ("user1", "login"),
    ("user1", "view"),
    ("user2", "login"),
    ("user1", "purchase"),
    ("user2", "view"),
])

result = behaviors.key_by(lambda x: x[0]).process(
    UserBehaviorTracker(),
    output_type=Types.STRING()
)

result.print()
env.execute("User Behavior Tracking")
```

## 

### 

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import KeyedProcessFunction
from pyflink.common.typeinfo import Types
from collections import defaultdict
import json

class RealtimeRecommender(KeyedProcessFunction):
    """"""
    
    def open(self, runtime_context):
        from pyflink.datastream.state import MapStateDescriptor
        
        # 
        self.view_history = runtime_context.get_map_state(
            MapStateDescriptor("views", Types.STRING(), Types.INT())
        )
        
        # 
        self.interest_tags = runtime_context.get_map_state(
            MapStateDescriptor("tags", Types.STRING(), Types.FLOAT())
        )
    
    def process_element(self, value, ctx):
        """"""
        user_id = value['user_id']
        item_id = value['item_id']
        action = value['action']
        category = value['category']
        
        # 
        view_count = self.view_history.get(item_id)
        if view_count is None:
            view_count = 0
        self.view_history.put(item_id, view_count + 1)
        
        # 
        interest_score = self.interest_tags.get(category)
        if interest_score is None:
            interest_score = 0.0
        
        # 
        if action == 'view':
            interest_score += 1.0
        elif action == 'click':
            interest_score += 2.0
        elif action == 'cart':
            interest_score += 5.0
        elif action == 'purchase':
            interest_score += 10.0
        
        self.interest_tags.put(category, interest_score)
        
        # 
        recommendations = self.generate_recommendations()
        
        yield {
            'user_id': user_id,
            'recommendations': recommendations,
            'interests': dict(self.interest_tags.items())
        }
    
    def generate_recommendations(self):
        """"""
        # Top3
        interests = list(self.interest_tags.items())
        interests.sort(key=lambda x: x[1], reverse=True)
        
        top_categories = [cat for cat, score in interests[:3]]
        
        # 
        recommendations = []
        for category in top_categories:
            # 
            recommendations.append(f"{category}")
        
        return recommendations

# 
def run_recommender():
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # Kafka
    from pyflink.datastream.connectors import FlinkKafkaConsumer
    from pyflink.common.serialization import SimpleStringSchema
    
    kafka_consumer = FlinkKafkaConsumer(
        topics='user_behavior',
        deserialization_schema=SimpleStringSchema(),
        properties={'bootstrap.servers': 'localhost:9092'}
    )
    
    behaviors = env.add_source(kafka_consumer)
    
    # JSON
    parsed = behaviors.map(
        lambda x: json.loads(x),
        output_type=Types.MAP(Types.STRING(), Types.STRING())
    )
    
    # 
    recommendations = parsed.key_by(lambda x: x['user_id']).process(
        RealtimeRecommender(),
        output_type=Types.MAP(Types.STRING(), Types.STRING())
    )
    
    # Kafka
    from pyflink.datastream.connectors import FlinkKafkaProducer
    
    kafka_producer = FlinkKafkaProducer(
        topic='recommendations',
        serialization_schema=SimpleStringSchema(),
        producer_config={'bootstrap.servers': 'localhost:9092'}
    )
    
    recommendations.map(
        lambda x: json.dumps(x),
        output_type=Types.STRING()
    ).add_sink(kafka_producer)
    
    env.execute("Realtime Recommender")

if __name__ == '__main__':
    run_recommender()
```

### 

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import KeyedProcessFunction
from pyflink.common.typeinfo import Types
import json

class AnomalyDetector(KeyedProcessFunction):
    """"""
    
    def open(self, runtime_context):
        from pyflink.datastream.state import ValueStateDescriptor, ListStateDescriptor
        
        # N
        self.recent_values = runtime_context.get_list_state(
            ListStateDescriptor("recent", Types.FLOAT())
        )
        
        # 
        self.stats = runtime_context.get_value_state(
            ValueStateDescriptor("stats", Types.MAP(Types.STRING(), Types.FLOAT()))
        )
    
    def process_element(self, value, ctx):
        """"""
        metric_name = value['metric']
        metric_value = float(value['value'])
        timestamp = value['timestamp']
        
        # 
        history = list(self.recent_values.get())
        history.append(metric_value)
        
        # 100
        if len(history) > 100:
            history = history[-100:]
        
        # 
        self.recent_values.clear()
        for v in history:
            self.recent_values.add(v)
        
        # 
        if len(history) >= 10:
            import numpy as np
            mean = np.mean(history)
            std = np.std(history)
            
            # 3-sigma
            if abs(metric_value - mean) > 3 * std:
                yield {
                    'type': 'anomaly',
                    'metric': metric_name,
                    'value': metric_value,
                    'mean': mean,
                    'std': std,
                    'timestamp': timestamp,
                    'message': f': {metric_value:.2f} (={mean:.2f}, ={std:.2f})'
                }
            else:
                yield {
                    'type': 'normal',
                    'metric': metric_name,
                    'value': metric_value
                }

# 
def run_anomaly_detector():
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # 
    metrics = env.from_collection([
        {'metric': 'cpu', 'value': 50, 'timestamp': 1000},
        {'metric': 'cpu', 'value': 52, 'timestamp': 2000},
        {'metric': 'cpu', 'value': 95, 'timestamp': 3000},  # 
        {'metric': 'cpu', 'value': 51, 'timestamp': 4000},
    ])
    
    # 
    results = metrics.key_by(lambda x: x['metric']).process(
        AnomalyDetector(),
        output_type=Types.MAP(Types.STRING(), Types.STRING())
    )
    
    # 
    anomalies = results.filter(lambda x: x['type'] == 'anomaly')
    
    # 
    anomalies.print()
    
    env.execute("Anomaly Detection")

if __name__ == '__main__':
    run_anomaly_detector()
```

## 

Flink

1. ****
2. ****
3. ****
4. ****API

**Flink**

## 

1. Top-N
2. 
3. CEP
4. Flink

Hive

