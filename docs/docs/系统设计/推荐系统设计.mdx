---
sidebar_position: 7
title: æ¨èç³»ç»Ÿè®¾è®¡
tags: [ç³»ç»Ÿè®¾è®¡, æ¨èç³»ç»Ÿ, ååŒè¿‡æ»¤, æœºå™¨å­¦ä¹ ]
---

# æ¨èç³»ç»Ÿè®¾è®¡

æ¨èç³»ç»Ÿæ˜¯ç°ä»£äº’è”ç½‘äº§å“çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œæœ¬æ–‡å°†è¯¦ç»†è®²è§£å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜æ•ˆçš„ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿã€‚

## éœ€æ±‚åˆ†æ

### åŠŸèƒ½éœ€æ±‚

```javascript
const requirements = {
  // æ¨èåœºæ™¯
  scenarios: {
    feed: true,           // ä¿¡æ¯æµæ¨è
    related: true,        // ç›¸å…³æ¨è
    personalized: true,   // ä¸ªæ€§åŒ–æ¨è
    hotTrending: true     // çƒ­é—¨è¶‹åŠ¿
  },
  
  // æ¨èç®—æ³•
  algorithms: {
    collaborative: true,  // ååŒè¿‡æ»¤
    contentBased: true,   // åŸºäºå†…å®¹
    hybrid: true,         // æ··åˆæ¨è
    deepLearning: true    // æ·±åº¦å­¦ä¹ 
  },
  
  // ä¸šåŠ¡éœ€æ±‚
  business: {
    realtime: true,       // å®æ—¶æ¨è
    coldStart: true,      // å†·å¯åŠ¨
    diversity: true,      // å¤šæ ·æ€§
    explainability: true  // å¯è§£é‡Šæ€§
  }
};
```

### éåŠŸèƒ½éœ€æ±‚

```python
# å®¹é‡ä¼°ç®—
ç”¨æˆ·æ•° = 100_000_000          # 1äº¿ç”¨æˆ·
ç‰©å“æ•° = 10_000_000           # 1000ä¸‡ç‰©å“
æ¯æ—¥æ¨èè¯·æ±‚ = 500_000_000    # 5äº¿è¯·æ±‚

# QPSè®¡ç®—
æ¨èQPS = æ¯æ—¥æ¨èè¯·æ±‚ / 86400
å³°å€¼QPS = æ¨èQPS * 3

print(f"å¹³å‡QPS: {æ¨èQPS:,.0f}")
print(f"å³°å€¼QPS: {å³°å€¼QPS:,.0f}")

# å“åº”æ—¶é—´
P99å»¶è¿Ÿ = 100                 # 100ms
P95å»¶è¿Ÿ = 50                  # 50ms
```

## æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           æ•°æ®æ”¶é›†å±‚                     â”‚
â”‚  ç”¨æˆ·è¡Œä¸º â”‚ ç‰©å“ç‰¹å¾ â”‚ ä¸Šä¸‹æ–‡ä¿¡æ¯        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ç¦»çº¿è®¡ç®—å±‚                     â”‚
â”‚  ç‰¹å¾å·¥ç¨‹ â”‚ æ¨¡å‹è®­ç»ƒ â”‚ ç›¸ä¼¼åº¦è®¡ç®—        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           åœ¨çº¿æœåŠ¡å±‚                     â”‚
â”‚  å¬å› â”‚ æ’åº â”‚ é‡æ’ â”‚ è¿‡æ»¤              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŠ€æœ¯é€‰å‹

```javascript
const techStack = {
  // ç¦»çº¿è®¡ç®—
  offline: {
    spark: 'å¤§è§„æ¨¡æ•°æ®å¤„ç†',
    tensorflow: 'æ¨¡å‹è®­ç»ƒ',
    pytorch: 'æ·±åº¦å­¦ä¹ '
  },
  
  // åœ¨çº¿æœåŠ¡
  online: {
    redis: 'ç‰¹å¾ç¼“å­˜',
    faiss: 'å‘é‡æ£€ç´¢',
    tensorflow_serving: 'æ¨¡å‹æœåŠ¡'
  },
  
  // å­˜å‚¨
  storage: {
    hbase: 'ç”¨æˆ·ç‰¹å¾',
    mysql: 'ç‰©å“ä¿¡æ¯',
    hdfs: 'æ—¥å¿—æ•°æ®'
  }
};
```

## ååŒè¿‡æ»¤

### åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤

```python
import numpy as np
from scipy.spatial.distance import cosine

class UserBasedCF:
    def __init__(self):
        self.user_item_matrix = None
        self.user_similarity = None
    
    def fit(self, ratings):
        """
        è®­ç»ƒæ¨¡å‹
        ratings: DataFrame with columns [user_id, item_id, rating]
        """
        # æ„å»ºç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µ
        self.user_item_matrix = ratings.pivot(
            index='user_id',
            columns='item_id',
            values='rating'
        ).fillna(0)
        
        # è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦
        self.user_similarity = self.calculate_similarity()
    
    def calculate_similarity(self):
        """è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µ"""
        n_users = len(self.user_item_matrix)
        similarity = np.zeros((n_users, n_users))
        
        for i in range(n_users):
            for j in range(i + 1, n_users):
                # ä½™å¼¦ç›¸ä¼¼åº¦
                sim = 1 - cosine(
                    self.user_item_matrix.iloc[i],
                    self.user_item_matrix.iloc[j]
                )
                similarity[i][j] = sim
                similarity[j][i] = sim
        
        return similarity
    
    def recommend(self, user_id, k=10, n=10):
        """
        ä¸ºç”¨æˆ·æ¨èç‰©å“
        k: ç›¸ä¼¼ç”¨æˆ·æ•°é‡
        n: æ¨èç‰©å“æ•°é‡
        """
        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„kä¸ªç”¨æˆ·
        user_idx = self.user_item_matrix.index.get_loc(user_id)
        similar_users = np.argsort(self.user_similarity[user_idx])[-k-1:-1]
        
        # è·å–ç”¨æˆ·å·²è¯„åˆ†çš„ç‰©å“
        rated_items = set(
            self.user_item_matrix.columns[
                self.user_item_matrix.iloc[user_idx] > 0
            ]
        )
        
        # è®¡ç®—å€™é€‰ç‰©å“çš„é¢„æµ‹è¯„åˆ†
        item_scores = {}
        
        for item in self.user_item_matrix.columns:
            if item in rated_items:
                continue
            
            # åŠ æƒå¹³å‡ç›¸ä¼¼ç”¨æˆ·çš„è¯„åˆ†
            weighted_sum = 0
            similarity_sum = 0
            
            for similar_user_idx in similar_users:
                rating = self.user_item_matrix.iloc[similar_user_idx][item]
                if rating > 0:
                    sim = self.user_similarity[user_idx][similar_user_idx]
                    weighted_sum += sim * rating
                    similarity_sum += sim
            
            if similarity_sum > 0:
                item_scores[item] = weighted_sum / similarity_sum
        
        # è¿”å›è¯„åˆ†æœ€é«˜çš„nä¸ªç‰©å“
        recommendations = sorted(
            item_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:n]
        
        return recommendations
```

### åŸºäºç‰©å“çš„ååŒè¿‡æ»¤

```python
class ItemBasedCF:
    def __init__(self):
        self.user_item_matrix = None
        self.item_similarity = None
    
    def fit(self, ratings):
        """è®­ç»ƒæ¨¡å‹"""
        # æ„å»ºç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µ
        self.user_item_matrix = ratings.pivot(
            index='user_id',
            columns='item_id',
            values='rating'
        ).fillna(0)
        
        # è®¡ç®—ç‰©å“ç›¸ä¼¼åº¦
        self.item_similarity = self.calculate_similarity()
    
    def calculate_similarity(self):
        """è®¡ç®—ç‰©å“ç›¸ä¼¼åº¦çŸ©é˜µ"""
        n_items = len(self.user_item_matrix.columns)
        similarity = np.zeros((n_items, n_items))
        
        for i in range(n_items):
            for j in range(i + 1, n_items):
                # ä½™å¼¦ç›¸ä¼¼åº¦
                sim = 1 - cosine(
                    self.user_item_matrix.iloc[:, i],
                    self.user_item_matrix.iloc[:, j]
                )
                similarity[i][j] = sim
                similarity[j][i] = sim
        
        return similarity
    
    def recommend(self, user_id, n=10):
        """ä¸ºç”¨æˆ·æ¨èç‰©å“"""
        user_idx = self.user_item_matrix.index.get_loc(user_id)
        user_ratings = self.user_item_matrix.iloc[user_idx]
        
        # è·å–ç”¨æˆ·å·²è¯„åˆ†çš„ç‰©å“
        rated_items = user_ratings[user_ratings > 0]
        
        # è®¡ç®—å€™é€‰ç‰©å“çš„é¢„æµ‹è¯„åˆ†
        item_scores = {}
        
        for item_idx, item in enumerate(self.user_item_matrix.columns):
            if item in rated_items.index:
                continue
            
            # åŸºäºç›¸ä¼¼ç‰©å“è®¡ç®—è¯„åˆ†
            weighted_sum = 0
            similarity_sum = 0
            
            for rated_item in rated_items.index:
                rated_item_idx = self.user_item_matrix.columns.get_loc(rated_item)
                sim = self.item_similarity[item_idx][rated_item_idx]
                rating = rated_items[rated_item]
                
                weighted_sum += sim * rating
                similarity_sum += abs(sim)
            
            if similarity_sum > 0:
                item_scores[item] = weighted_sum / similarity_sum
        
        # è¿”å›è¯„åˆ†æœ€é«˜çš„nä¸ªç‰©å“
        recommendations = sorted(
            item_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:n]
        
        return recommendations
```

## çŸ©é˜µåˆ†è§£

### SVDåˆ†è§£

```python
from scipy.sparse.linalg import svds

class MatrixFactorization:
    def __init__(self, n_factors=50):
        self.n_factors = n_factors
        self.user_factors = None
        self.item_factors = None
    
    def fit(self, ratings):
        """ä½¿ç”¨SVDåˆ†è§£"""
        # æ„å»ºè¯„åˆ†çŸ©é˜µ
        user_item_matrix = ratings.pivot(
            index='user_id',
            columns='item_id',
            values='rating'
        ).fillna(0)
        
        # SVDåˆ†è§£
        U, sigma, Vt = svds(user_item_matrix.values, k=self.n_factors)
        
        # ä¿å­˜å› å­çŸ©é˜µ
        self.user_factors = U
        self.item_factors = Vt.T
        self.sigma = np.diag(sigma)
        
        self.user_ids = user_item_matrix.index
        self.item_ids = user_item_matrix.columns
    
    def predict(self, user_id, item_id):
        """é¢„æµ‹è¯„åˆ†"""
        user_idx = self.user_ids.get_loc(user_id)
        item_idx = self.item_ids.get_loc(item_id)
        
        prediction = np.dot(
            np.dot(self.user_factors[user_idx], self.sigma),
            self.item_factors[item_idx]
        )
        
        return prediction
    
    def recommend(self, user_id, n=10):
        """æ¨èç‰©å“"""
        user_idx = self.user_ids.get_loc(user_id)
        
        # è®¡ç®—æ‰€æœ‰ç‰©å“çš„é¢„æµ‹è¯„åˆ†
        user_vector = np.dot(self.user_factors[user_idx], self.sigma)
        predictions = np.dot(user_vector, self.item_factors.T)
        
        # è¿”å›è¯„åˆ†æœ€é«˜çš„nä¸ªç‰©å“
        top_items = np.argsort(predictions)[-n:][::-1]
        
        return [(self.item_ids[i], predictions[i]) for i in top_items]
```

## æ·±åº¦å­¦ä¹ æ¨è

### Neural Collaborative Filtering

```python
import tensorflow as tf

class NCF(tf.keras.Model):
    def __init__(self, n_users, n_items, embedding_dim=64, layers=[128, 64, 32]):
        super(NCF, self).__init__()
        
        # ç”¨æˆ·å’Œç‰©å“çš„åµŒå…¥å±‚
        self.user_embedding = tf.keras.layers.Embedding(
            n_users, embedding_dim
        )
        self.item_embedding = tf.keras.layers.Embedding(
            n_items, embedding_dim
        )
        
        # MLPå±‚
        self.mlp_layers = []
        for units in layers:
            self.mlp_layers.append(
                tf.keras.layers.Dense(units, activation='relu')
            )
        
        # è¾“å‡ºå±‚
        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')
    
    def call(self, inputs):
        user_id, item_id = inputs
        
        # è·å–åµŒå…¥å‘é‡
        user_vec = self.user_embedding(user_id)
        item_vec = self.item_embedding(item_id)
        
        # æ‹¼æ¥
        concat = tf.concat([user_vec, item_vec], axis=-1)
        
        # MLP
        x = concat
        for layer in self.mlp_layers:
            x = layer(x)
        
        # è¾“å‡º
        output = self.output_layer(x)
        
        return output

# è®­ç»ƒæ¨¡å‹
def train_ncf(train_data, n_users, n_items):
    model = NCF(n_users, n_items)
    
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    model.fit(
        [train_data['user_id'], train_data['item_id']],
        train_data['label'],
        epochs=10,
        batch_size=256
    )
    
    return model
```

## å¬å›ç­–ç•¥

### å¤šè·¯å¬å›

```javascript
class MultiRecallStrategy {
  constructor() {
    this.strategies = [
      new CollaborativeRecall(),
      new ContentBasedRecall(),
      new HotItemRecall(),
      new UserHistoryRecall()
    ];
  }
  
  async recall(userId, limit = 1000) {
    // å¹¶è¡Œæ‰§è¡Œå¤šè·¯å¬å›
    const recallPromises = this.strategies.map(strategy =>
      strategy.recall(userId, limit / this.strategies.length)
    );
    
    const results = await Promise.all(recallPromises);
    
    // åˆå¹¶å»é‡
    const itemSet = new Set();
    const items = [];
    
    for (const result of results) {
      for (const item of result) {
        if (!itemSet.has(item.id)) {
          itemSet.add(item.id);
          items.push(item);
        }
      }
    }
    
    return items;
  }
}

// ååŒè¿‡æ»¤å¬å›
class CollaborativeRecall {
  async recall(userId, limit) {
    // åŸºäºç”¨æˆ·ç›¸ä¼¼åº¦å¬å›
    const similarUsers = await this.getSimilarUsers(userId, 100);
    
    const items = [];
    for (const similarUser of similarUsers) {
      const userItems = await this.getUserItems(similarUser.id);
      items.push(...userItems);
    }
    
    // å»é‡å¹¶æŒ‰ç›¸ä¼¼åº¦æ’åº
    return this.deduplicate(items).slice(0, limit);
  }
}

// åŸºäºå†…å®¹å¬å›
class ContentBasedRecall {
  async recall(userId, limit) {
    // è·å–ç”¨æˆ·å…´è¶£æ ‡ç­¾
    const userTags = await this.getUserTags(userId);
    
    // å¬å›ç›¸å…³ç‰©å“
    const items = [];
    for (const tag of userTags) {
      const tagItems = await this.getItemsByTag(tag.name, tag.weight);
      items.push(...tagItems);
    }
    
    return items.slice(0, limit);
  }
}
```

## æ’åºæ¨¡å‹

### LRæ’åº

```python
from sklearn.linear_model import LogisticRegression

class LRRanker:
    def __init__(self):
        self.model = LogisticRegression()
    
    def train(self, features, labels):
        """è®­ç»ƒæ’åºæ¨¡å‹"""
        self.model.fit(features, labels)
    
    def predict(self, features):
        """é¢„æµ‹ç‚¹å‡»ç‡"""
        return self.model.predict_proba(features)[:, 1]
    
    def rank(self, items, user_features):
        """å¯¹ç‰©å“æ’åº"""
        # æ„å»ºç‰¹å¾
        features = []
        for item in items:
            feature = self.build_features(user_features, item)
            features.append(feature)
        
        # é¢„æµ‹å¾—åˆ†
        scores = self.predict(features)
        
        # æ’åº
        ranked_items = sorted(
            zip(items, scores),
            key=lambda x: x[1],
            reverse=True
        )
        
        return ranked_items
    
    def build_features(self, user_features, item):
        """æ„å»ºç‰¹å¾å‘é‡"""
        features = []
        
        # ç”¨æˆ·ç‰¹å¾
        features.extend([
            user_features['age'],
            user_features['gender'],
            user_features['active_days']
        ])
        
        # ç‰©å“ç‰¹å¾
        features.extend([
            item['popularity'],
            item['quality_score'],
            item['freshness']
        ])
        
        # äº¤å‰ç‰¹å¾
        features.append(
            user_features['age'] * item['popularity']
        )
        
        return features
```

### GBDT+LR

```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import OneHotEncoder

class GBDTLRRanker:
    def __init__(self):
        self.gbdt = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=5
        )
        self.lr = LogisticRegression()
        self.encoder = OneHotEncoder()
    
    def train(self, X_train, y_train):
        """è®­ç»ƒæ¨¡å‹"""
        # è®­ç»ƒGBDT
        self.gbdt.fit(X_train, y_train)
        
        # GBDTå¶å­èŠ‚ç‚¹ä½œä¸ºç‰¹å¾
        leaf_indices = self.gbdt.apply(X_train)
        leaf_features = self.encoder.fit_transform(leaf_indices)
        
        # è®­ç»ƒLR
        self.lr.fit(leaf_features, y_train)
    
    def predict(self, X):
        """é¢„æµ‹"""
        leaf_indices = self.gbdt.apply(X)
        leaf_features = self.encoder.transform(leaf_indices)
        return self.lr.predict_proba(leaf_features)[:, 1]
```

## å†·å¯åŠ¨

### æ–°ç”¨æˆ·å†·å¯åŠ¨

```javascript
class ColdStartHandler {
  async handleNewUser(userId) {
    // 1. çƒ­é—¨æ¨è
    const hotItems = await this.getHotItems(20);
    
    // 2. å¤šæ ·æ€§æ¨è
    const diverseItems = await this.getDiverseItems(20);
    
    // 3. å¼•å¯¼ç”¨æˆ·é€‰æ‹©å…´è¶£
    const interestGuide = await this.getInterestGuide();
    
    return {
      recommendations: [...hotItems, ...diverseItems],
      interestGuide
    };
  }
  
  async updateUserProfile(userId, selectedInterests) {
    // æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„å…´è¶£æ›´æ–°ç”»åƒ
    await redis.hset(`user:${userId}:profile`, {
      interests: JSON.stringify(selectedInterests),
      coldStart: false
    });
    
    // åŸºäºå…´è¶£æ¨è
    return await this.getInterestBasedRecommendations(
      userId,
      selectedInterests
    );
  }
}
```

### æ–°ç‰©å“å†·å¯åŠ¨

```javascript
class NewItemHandler {
  async handleNewItem(itemId) {
    // 1. æå–ç‰©å“ç‰¹å¾
    const features = await this.extractFeatures(itemId);
    
    // 2. æ‰¾åˆ°ç›¸ä¼¼ç‰©å“
    const similarItems = await this.findSimilarItems(features);
    
    // 3. æ¨èç»™ç›¸ä¼¼ç‰©å“çš„ç”¨æˆ·
    const targetUsers = await this.getTargetUsers(similarItems);
    
    // 4. å°æµé‡æµ‹è¯•
    await this.startABTest(itemId, targetUsers);
  }
  
  async extractFeatures(itemId) {
    const item = await db.items.findOne({ id: itemId });
    
    return {
      category: item.category,
      tags: item.tags,
      price: item.price,
      brand: item.brand
    };
  }
}
```

## å®æ—¶æ¨è

### æµå¼è®¡ç®—

```javascript
class RealtimeRecommender {
  constructor() {
    this.userBehaviorStream = new KafkaConsumer('user-behavior');
    this.featureStore = new Redis();
  }
  
  async start() {
    // æ¶ˆè´¹ç”¨æˆ·è¡Œä¸ºæµ
    this.userBehaviorStream.on('message', async (message) => {
      const behavior = JSON.parse(message.value);
      await this.processBehavior(behavior);
    });
  }
  
  async processBehavior(behavior) {
    const { userId, itemId, action, timestamp } = behavior;
    
    // 1. æ›´æ–°ç”¨æˆ·ç‰¹å¾
    await this.updateUserFeatures(userId, behavior);
    
    // 2. æ›´æ–°ç‰©å“ç‰¹å¾
    await this.updateItemFeatures(itemId, behavior);
    
    // 3. è§¦å‘å®æ—¶æ¨è
    if (action === 'click' || action === 'purchase') {
      await this.triggerRecommendation(userId);
    }
  }
  
  async updateUserFeatures(userId, behavior) {
    // æ›´æ–°ç”¨æˆ·å®æ—¶ç‰¹å¾
    await this.featureStore.hincrby(
      `user:${userId}:realtime`,
      `${behavior.action}_count`,
      1
    );
    
    // æ›´æ–°æœ€è¿‘æµè§ˆ
    await this.featureStore.lpush(
      `user:${userId}:recent_items`,
      behavior.itemId
    );
    await this.featureStore.ltrim(
      `user:${userId}:recent_items`,
      0,
      99
    );
  }
}
```

## è¯„ä¼°æŒ‡æ ‡

```python
class RecommenderMetrics:
    @staticmethod
    def precision_at_k(recommended, relevant, k):
        """å‡†ç¡®ç‡@K"""
        recommended_k = recommended[:k]
        hits = len(set(recommended_k) & set(relevant))
        return hits / k
    
    @staticmethod
    def recall_at_k(recommended, relevant, k):
        """å¬å›ç‡@K"""
        recommended_k = recommended[:k]
        hits = len(set(recommended_k) & set(relevant))
        return hits / len(relevant) if relevant else 0
    
    @staticmethod
    def ndcg_at_k(recommended, relevant, k):
        """NDCG@K"""
        dcg = 0
        for i, item in enumerate(recommended[:k]):
            if item in relevant:
                dcg += 1 / np.log2(i + 2)
        
        idcg = sum(1 / np.log2(i + 2) for i in range(min(k, len(relevant))))
        
        return dcg / idcg if idcg > 0 else 0
    
    @staticmethod
    def diversity(recommended):
        """å¤šæ ·æ€§"""
        categories = [item['category'] for item in recommended]
        return len(set(categories)) / len(categories)
```

## æ€»ç»“

æ¨èç³»ç»Ÿè®¾è®¡çš„æ ¸å¿ƒï¼š
- ğŸ¤ **ååŒè¿‡æ»¤**ï¼šç”¨æˆ·CF + ç‰©å“CF + çŸ©é˜µåˆ†è§£
- ğŸ§  **æ·±åº¦å­¦ä¹ **ï¼šNCF + Wide&Deep + DeepFM
- ğŸ¯ **å¬å›æ’åº**ï¼šå¤šè·¯å¬å› + ç²¾æ’æ¨¡å‹
- â„ï¸ **å†·å¯åŠ¨**ï¼šçƒ­é—¨æ¨è + å…´è¶£å¼•å¯¼
- âš¡ **å®æ—¶æ¨è**ï¼šæµå¼è®¡ç®— + ç‰¹å¾æ›´æ–°
- ğŸ“Š **æ•ˆæœè¯„ä¼°**ï¼šå‡†ç¡®ç‡ + å¬å›ç‡ + NDCG

è®°ä½ï¼š**æ¨èç³»ç»Ÿçš„æœ¬è´¨æ˜¯ç²¾å‡†çš„ç”¨æˆ·å…´è¶£å»ºæ¨¡å’Œç‰©å“åŒ¹é…ï¼**

