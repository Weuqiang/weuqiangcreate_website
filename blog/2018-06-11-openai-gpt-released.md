---
slug: openai-gpt-released
title: OpenAI 发布 GPT，语言模型新时代
authors: [weuqiang]
tags: [OpenAI, GPT, 语言模型, 时事]
date: 2018-06-11
---

OpenAI 发布了 GPT（Generative Pre-trained Transformer），证明了预训练语言模型的潜力。

<!-- truncate -->

## 什么是 GPT

GPT 是个语言模型：

- 基于 Transformer
- 用大量文本预训练
- 能生成连贯的文本

## 核心思想

**预训练 + 微调**

1. 先用大量文本预训练
2. 再用少量数据微调
3. 适配特定任务

这比从头训练效果好得多。

## 技术细节

- **参数量**：1.17 亿
- **训练数据**：BooksCorpus（7000 本书）
- **架构**：12 层 Transformer

在当时算很大了。

## 效果

GPT 在多个任务上表现不错：

- 文本生成
- 问答
- 文本分类

但还不够强，有时候会瞎编。

## 意义

GPT 证明了：

- 预训练很有效
- 模型越大越好
- Transformer 很强

这为后续的 GPT-2、GPT-3 奠定了基础。

## 对比 BERT

同年，Google 发布了 BERT：

- **GPT**：单向，适合生成
- **BERT**：双向，适合理解

两者各有优势。

## 后续发展

**GPT-2**（2019）

参数量 15 亿，效果大幅提升。

**GPT-3**（2020）

参数量 1750 亿，能做各种任务。

**ChatGPT**（2022）

基于 GPT-3.5，引爆了 AI 热潮。

## 总结

GPT 是语言模型的里程碑：

- 证明了预训练的有效性
- 开启了大模型时代
- 为 ChatGPT 铺平了道路

2018 年，语言模型的新时代开始了。

