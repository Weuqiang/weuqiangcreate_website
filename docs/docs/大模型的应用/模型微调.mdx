---
sidebar_position: 6
title: 
---

Fine-Tuning 
:::info
AB
:::

LLaMA-Factory  LLaMA  LoRA  WebUI 

[https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)



```bash
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install --no-deps -e .
```

 WebUI

```bash
llamafactory-cli webui
```



## 

LoRA 

 **14B **

### LoRA 

****

- ****~24GB  NVIDIA RTX 3090  A100LoRA 
- **GPU ** 2 

****

- ****100k steps  1 
- ****300k steps2-3 

****

- 
- LoRA  MB

### Freeze

****

- **** 48GB  A6000  80GB A100
- **GPU **1-4  GPU

****

- ****1-2 
- ****3-5 

****

- 
- 

### Full Fine-tuning

****

- **** 80GB  GPU NVIDIA A100  H10014B  4  GPU 
- **GPU **4-8  A100GPU

****

- ****2-3 
- ****5  1 

****

- 
-  LoRA 

****

- 
-  GB 



## 

  

  

 3 



### QAT: 
QAT, **Quantization-Aware Training**

---

### PTQ: 
PTQ, **Post-Training Quantization****calibration dataset**

---

### GPTQ: 
GPTQ**Group-wise Precision Tuning Quantization**  
“”  
**GPTQ **
-  fp16  4-bit
-  75% 
-   

****
```yaml
model_name_or_path: TechxGenus/Meta-Llama-3-8B-Instruct-GPTQ
```

---

### AWQ: 
AWQ**Activation-Aware Layer Quantization**  
**AWQ **
- 
-   

****
```yaml
model_name_or_path: TechxGenus/Meta-Llama-3-8B-Instruct-AWQ
```

---

### AQLM: 
AQLM**Additive Quantization of Language Models** PTQ   
****
-  2-bit 
-  3-bit  4-bit 
- 2-bit   

 AQLM 

---

### OFTQ: 
OFTQ**On-the-fly Quantization**  
****
- 
-   

****
```yaml
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
quantization_bit: 4
quantization_method: bitsandbytes  # : [bitsandbytes (4/8), hqq (2/3/4/5/6/8), eetq (8)]
```

---

### bitsandbytes: 
 GPTQ**bitsandbytes**   
****
-  1B 
-  8-bit 
-  50%   

---

### HQQ: 
HQQ**Half-Quadratic Quantization**  
****
- 
- 
-   

HQQ 

---

### EETQ: 
EETQ**Easy and Efficient Quantization for Transformers** PTQ   
****
- 
-   

EETQ 

## 

20256,



200MB80200MB`Qwen2.5 7b`4181G`Qwen2.5 7b`20901