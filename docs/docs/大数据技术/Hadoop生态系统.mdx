---
sidebar_position: 1
title: Hadoop - 
---

# Hadoop - 

Hadoop**Hadoop**

## Hadoop

### Hadoop

****1TBIP

****
```python
#  - 
ip_counts = {}
with open('1TB_log.txt') as f:  # 
    for line in f:
        ip = extract_ip(line)
        ip_counts[ip] = ip_counts.get(ip, 0) + 1
```

**Hadoop**
```python
#  - 100
# 10GB
# 106
```

### Hadoop

1. **HDFS**
2. **MapReduce**
3. **YARN**

```

                                
    MapReduceSparkHive           

              ↓

           YARN                       
                     

              ↓

           HDFS                       
                         

```

## HDFS - 

### HDFS

****
- 
- GBTB
- 
- 

### HDFS

```

  NameNode      ← 

       ↓

 DataNode 1    DataNode 2    DataNode 3     ← 
 Block 1,3,5   Block 2,4,6   Block 1,2,3  

```

****

```python
# 300MB
# HDFS

1. 128MB
    → Block1(128MB) + Block2(128MB) + Block3(44MB)

2. 3
   Block1 → DataNode1, DataNode2, DataNode3
   Block2 → DataNode2, DataNode3, DataNode1
   Block3 → DataNode3, DataNode1, DataNode2

3. NameNode
   file.txt → [Block1@DN1,DN2,DN3, Block2@DN2,DN3,DN1, ...]
```

### HDFS

**1. Docker**

```bash
# docker-compose.yml
version: '3'
services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    environment:
      - CLUSTER_NAME=test
    volumes:
      - hadoop_namenode:/hadoop/dfs/name

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - hadoop_datanode:/hadoop/dfs/data

volumes:
  hadoop_namenode:
  hadoop_datanode:
```

```bash
# 
docker-compose up -d

# Web
# : http://localhost:9870
```

**2. HDFS**

```bash
# 
docker exec -it namenode bash

# 
hdfs dfs -mkdir -p /user/data

# 
hdfs dfs -put local_file.txt /user/data/

# 
hdfs dfs -ls /user/data/

# 
hdfs dfs -cat /user/data/local_file.txt

# 
hdfs dfs -get /user/data/local_file.txt ./

# 
hdfs dfs -rm /user/data/local_file.txt

# 
hdfs dfs -du -h /user/data/

# 
hdfs dfsadmin -report
```

**3. PythonHDFS**

```python
from hdfs import InsecureClient

# HDFS
client = InsecureClient('http://localhost:9870', user='root')

# 
client.upload('/user/data/test.txt', 'local_test.txt')

# 
client.download('/user/data/test.txt', 'downloaded.txt')

# 
with client.read('/user/data/test.txt', encoding='utf-8') as reader:
    content = reader.read()
    print(content)

# 
with client.write('/user/data/output.txt', encoding='utf-8') as writer:
    writer.write('Hello HDFS!')

# 
files = client.list('/user/data/')
print(files)

# 
client.delete('/user/data/test.txt')

# 
status = client.status('/user/data/output.txt')
print(f": {status['length']} bytes")
```

### HDFS

**1. **

```python
# HDFS
# DataNode
# DataNode
# DataNode

# 
# - 
# - 
# - 
```

**2. **

```bash
# HDFS
# checksum

# 
hdfs fsck /user/data/file.txt -files -blocks -locations
```

**3. **

```bash
# 
hdfs dfsadmin -allowSnapshot /user/data

# 
hdfs dfs -createSnapshot /user/data snapshot1

# 
hdfs dfs -ls /user/data/.snapshot/

# 
hdfs dfs -cp /user/data/.snapshot/snapshot1/file.txt /user/data/

# 
hdfs dfs -deleteSnapshot /user/data snapshot1
```

## MapReduce - 

### MapReduce

****

```python
# 1TB

# Map
Machine1: "hello world" → [("hello", 1), ("world", 1)]
Machine2: "hello hadoop" → [("hello", 1), ("hadoop", 1)]
Machine3: "world hadoop" → [("world", 1), ("hadoop", 1)]

# Shufflekey
Machine1: ("hello", [1, 1])
Machine2: ("world", [1, 1])
Machine3: ("hadoop", [1, 1])

# Reduce
Machine1: ("hello", 2)
Machine2: ("world", 2)
Machine3: ("hadoop", 2)
```

### WordCount

**1. Pythonmrjob**

```python
# wordcount.py
from mrjob.job import MRJob
import re

class WordCount(MRJob):
    
    def mapper(self, _, line):
        """Map"""
        # 
        words = re.findall(r'\w+', line.lower())
        for word in words:
            yield word, 1
    
    def reducer(self, word, counts):
        """Reduce"""
        yield word, sum(counts)

if __name__ == '__main__':
    WordCount.run()
```

```bash
# 
python wordcount.py input.txt

# Hadoop
python wordcount.py -r hadoop hdfs:///user/data/input.txt
```

**2. **

```python
# log_analysis.py
from mrjob.job import MRJob
from mrjob.step import MRStep
import re

class LogAnalysis(MRJob):
    
    def steps(self):
        """MapReduce"""
        return [
            MRStep(mapper=self.mapper_parse_log,
                   reducer=self.reducer_count_by_ip),
            MRStep(reducer=self.reducer_find_top)
        ]
    
    def mapper_parse_log(self, _, line):
        """IP"""
        # : 192.168.1.1 - - [01/Jan/2024:10:00:00] "GET /index.html"
        match = re.match(r'(\d+\.\d+\.\d+\.\d+)', line)
        if match:
            ip = match.group(1)
            yield ip, 1
    
    def reducer_count_by_ip(self, ip, counts):
        """IP"""
        total = sum(counts)
        yield None, (total, ip)  # (, IP)
    
    def reducer_find_top(self, _, ip_counts):
        """TOP10 IP"""
        top_ips = sorted(ip_counts, reverse=True)[:10]
        for count, ip in top_ips:
            yield ip, count

if __name__ == '__main__':
    LogAnalysis.run()
```

**3. **

```python
# user_behavior.py
from mrjob.job import MRJob
from mrjob.step import MRStep
import json

class UserBehavior(MRJob):
    
    def steps(self):
        return [
            MRStep(mapper=self.mapper_parse_event,
                   reducer=self.reducer_aggregate_user),
            MRStep(mapper=self.mapper_user_stats,
                   reducer=self.reducer_summary)
        ]
    
    def mapper_parse_event(self, _, line):
        """"""
        try:
            event = json.loads(line)
            user_id = event['user_id']
            action = event['action']
            timestamp = event['timestamp']
            
            yield user_id, {
                'action': action,
                'timestamp': timestamp
            }
        except:
            pass
    
    def reducer_aggregate_user(self, user_id, events):
        """"""
        events_list = list(events)
        
        # 
        action_counts = {}
        for event in events_list:
            action = event['action']
            action_counts[action] = action_counts.get(action, 0) + 1
        
        # 
        total_actions = sum(action_counts.values())
        
        yield 'user_stats', {
            'user_id': user_id,
            'total_actions': total_actions,
            'action_counts': action_counts
        }
    
    def mapper_user_stats(self, key, stats):
        """"""
        total = stats['total_actions']
        
        if total >= 100:
            category = 'highly_active'
        elif total >= 10:
            category = 'active'
        else:
            category = 'inactive'
        
        yield category, 1
    
    def reducer_summary(self, category, counts):
        """"""
        yield category, sum(counts)

if __name__ == '__main__':
    UserBehavior.run()
```

### MapReduce

**1. Combiner**

```python
class WordCountOptimized(MRJob):
    
    def mapper(self, _, line):
        for word in line.split():
            yield word, 1
    
    def combiner(self, word, counts):
        """Map"""
        yield word, sum(counts)
    
    def reducer(self, word, counts):
        yield word, sum(counts)
```

**2. **

```python
class CustomPartitioner(MRJob):
    
    def mapper(self, _, line):
        # 
        word = line.strip().lower()
        if word:
            first_letter = word[0]
            yield (first_letter, word), 1
    
    def reducer(self, key, counts):
        first_letter, word = key
        yield word, sum(counts)
```

## YARN - 

### YARN

```

  ResourceManager   ← 

         ↓

  NM1     NM2     NM3     ← NodeManager

    ↓        ↓        ↓

ContainerContainerContainer  ← 

```

### YARN

```bash
# 
yarn application -list

# 
yarn application -status application_1234567890_0001

# 
yarn application -kill application_1234567890_0001

# 
yarn logs -applicationId application_1234567890_0001

# 
yarn queue -status default

# 
yarn node -list
```

## 

### 

****
1. PVUV
2. 
3. 

****
```json
{
  "timestamp": "2024-01-01 10:00:00",
  "user_id": "user123",
  "action": "view",
  "item_id": "item456",
  "category": "electronics"
}
```

****

```python
# ecommerce_analysis.py
from mrjob.job import MRJob
from mrjob.step import MRStep
import json
from datetime import datetime

class EcommerceAnalysis(MRJob):
    
    def steps(self):
        return [
            MRStep(mapper=self.mapper_parse,
                   reducer=self.reducer_daily_stats),
            MRStep(mapper=self.mapper_category,
                   reducer=self.reducer_top_items)
        ]
    
    def mapper_parse(self, _, line):
        """"""
        try:
            log = json.loads(line)
            
            # 
            date = log['timestamp'][:10]
            user_id = log['user_id']
            item_id = log['item_id']
            category = log['category']
            action = log['action']
            
            # 
            # 1. + -> UV
            yield (date, 'uv', user_id), 1
            
            # 2. +PV -> PV
            yield (date, 'pv'), 1
            
            # 3. 
            if action == 'view':
                yield (date, 'item', item_id), 1
            
            # 4. 
            yield (date, 'category', category), 1
            
        except Exception as e:
            pass
    
    def reducer_daily_stats(self, key, values):
        """"""
        if len(key) == 3:
            date, metric_type, item = key
            if metric_type == 'uv':
                # UV
                yield (date, 'uv'), 1
            elif metric_type == 'item':
                # 
                yield (date, 'item', item), sum(values)
            elif metric_type == 'category':
                # 
                yield (date, 'category', item), sum(values)
        else:
            date, metric_type = key
            if metric_type == 'pv':
                # PV
                yield (date, 'pv'), sum(values)
    
    def mapper_category(self, key, value):
        """"""
        if len(key) == 3:
            date, metric_type, item = key
            if metric_type == 'item':
                # 
                yield (date, 'top_items'), (value, item)
            elif metric_type == 'category':
                # 
                yield (date, 'category_stats'), (item, value)
        else:
            # PV/UV
            yield key, value
    
    def reducer_top_items(self, key, values):
        """"""
        date, metric_type = key
        
        if metric_type == 'top_items':
            # TOP10
            items = sorted(values, reverse=True)[:10]
            for count, item_id in items:
                yield f"{date}_top_item", f"{item_id}: {count}"
        
        elif metric_type == 'category_stats':
            # 
            for category, count in values:
                yield f"{date}_category", f"{category}: {count}"
        
        else:
            # PV/UV
            total = sum(values)
            yield f"{date}_{metric_type}", total

if __name__ == '__main__':
    EcommerceAnalysis.run()
```

****

```bash
# 
python generate_test_data.py > test_logs.json

# 
python ecommerce_analysis.py test_logs.json

# HDFS
hdfs dfs -put test_logs.json /user/data/logs/

# Hadoop
python ecommerce_analysis.py -r hadoop hdfs:///user/data/logs/test_logs.json \
    --output-dir hdfs:///user/data/output/
```

## 

Hadoop

1. **HDFS**
2. **MapReduce**
3. **YARN**

****
- SparkMapReduce100
- HiveSQL
- HBase

**Hadoop**

## 

1. 
2. Twitter
3. PageRank
4. 

[Spark](./Spark.mdx)

