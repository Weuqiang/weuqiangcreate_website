---
sidebar_position: 1
title:  - 
---

#  - AI

""****

##  - 

### 

****

```python
import numpy as np

# 
v = np.array([1, 2, 3])
print(v)  # [1 2 3]
```

********

```python
import matplotlib.pyplot as plt

# 
plt.figure(figsize=(6, 6))
plt.arrow(0, 0, 3, 2, head_width=0.3, head_length=0.3, fc='blue', ec='blue')
plt.xlim(-1, 5)
plt.ylim(-1, 5)
plt.grid(True)
plt.axhline(y=0, color='k', linewidth=0.5)
plt.axvline(x=0, color='k', linewidth=0.5)
plt.title(' [3, 2]')
plt.show()
```

### AI

**1. **

```python
# 
person = np.array([
    175,  # (cm)
    70,   # (kg)
    25,   # 
    1     # (1=, 0=)
])

# 
image_features = np.array([
    0.8,  # 
    0.3,  # 
    0.1,  # 
    0.9,  # 
    0.2   # 
])
```

**2. **

```python
# 
word_king = np.array([0.5, 0.8, 0.1, 0.3])
word_queen = np.array([0.5, 0.8, 0.9, 0.3])
word_man = np.array([0.5, 0.1, 0.1, 0.2])
word_woman = np.array([0.5, 0.1, 0.9, 0.2])

# 
# king - man + woman ≈ queen
result = word_king - word_man + word_woman
print("king - man + woman =", result)
print("queen =", word_queen)
print(":", np.dot(result, word_queen))
```

**3. **

```python
# 28x28784
image = np.random.rand(28, 28)
input_vector = image.flatten()
print(":", input_vector.shape)  # (784,)

# 10
output_vector = np.array([0.1, 0.05, 0.6, 0.05, 0.1, 0.02, 0.03, 0.02, 0.02, 0.01])
print(":", np.argmax(output_vector))  # 2
```

### 

**1. **

```python
v1 = np.array([1, 2])
v2 = np.array([3, 1])
v3 = v1 + v2
print(v3)  # [4 3]

# 
plt.figure(figsize=(6, 6))
plt.arrow(0, 0, v1[0], v1[1], head_width=0.2, fc='blue', ec='blue', label='v1')
plt.arrow(0, 0, v2[0], v2[1], head_width=0.2, fc='red', ec='red', label='v2')
plt.arrow(0, 0, v3[0], v3[1], head_width=0.2, fc='green', ec='green', label='v1+v2')
plt.xlim(-1, 5)
plt.ylim(-1, 4)
plt.grid(True)
plt.legend()
plt.title('')
plt.show()
```

**AI**

```python
#  +  = 
image_feature = np.array([0.8, 0.3, 0.1])
text_feature = np.array([0.2, 0.5, 0.7])
multimodal_feature = image_feature + text_feature
print(":", multimodal_feature)
```

**2. **

```python
v = np.array([2, 3])
v_scaled = 2 * v
print(v_scaled)  # [4 6]

# 
plt.figure(figsize=(6, 6))
plt.arrow(0, 0, v[0], v[1], head_width=0.3, fc='blue', ec='blue', label='v')
plt.arrow(0, 0, v_scaled[0], v_scaled[1], head_width=0.3, fc='red', ec='red', label='2v')
plt.xlim(-1, 7)
plt.ylim(-1, 7)
plt.grid(True)
plt.legend()
plt.title('')
plt.show()
```

**AI**

```python
# 
gradient = np.array([0.5, -0.3, 0.8])
learning_rate = 0.01

# 
update = -learning_rate * gradient
print(":", update)
```

**3. **

```python
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

# 
dot_product = np.dot(v1, v2)
print(dot_product)  # 1*4 + 2*5 + 3*6 = 32

# 
manual = v1[0]*v2[0] + v1[1]*v2[1] + v1[2]*v2[2]
print(manual)  # 32
```

****

```python
# 
v1 = np.array([1, 0])
v2 = np.array([1, 0])  # 
v3 = np.array([0, 1])  # 
v4 = np.array([-1, 0]) # 

print(":", np.dot(v1, v2))  # 1
print(":", np.dot(v1, v3))      # 0
print(":", np.dot(v1, v4))  # -1
```

**AI**

```python
# 
sentence1 = np.array([0.8, 0.3, 0.5, 0.2])  # 1
sentence2 = np.array([0.7, 0.4, 0.6, 0.1])  # 2

# 
def cosine_similarity(v1, v2):
    dot = np.dot(v1, v2)
    norm1 = np.linalg.norm(v1)
    norm2 = np.linalg.norm(v2)
    return dot / (norm1 * norm2)

similarity = cosine_similarity(sentence1, sentence2)
print(f": {similarity:.3f}")
```

### 

```python
class SimpleRecommender:
    """"""
    
    def __init__(self):
        # [, , , ]
        self.users = {
            'Alice': np.array([25, 0, 5000, 16]),
            'Bob': np.array([30, 1, 8000, 18]),
            'Charlie': np.array([28, 1, 6000, 16]),
            'Diana': np.array([26, 0, 5500, 16])
        }
        
        # [, , , ]
        self.items = {
            'iPhone': np.array([1000, 0.9, 0.8, 0.7]),
            'Book': np.array([30, 0.3, 0.2, 0.9]),
            'Shoes': np.array([100, 0.2, 0.9, 0.8]),
            'Laptop': np.array([1500, 0.95, 0.5, 0.85])
        }
    
    def normalize(self, v):
        """"""
        return v / np.linalg.norm(v)
    
    def recommend(self, user_name, top_k=2):
        """"""
        user_vec = self.normalize(self.users[user_name])
        
        scores = {}
        for item_name, item_vec in self.items.items():
            item_vec_norm = self.normalize(item_vec)
            # 
            score = np.dot(user_vec, item_vec_norm)
            scores[item_name] = score
        
        # 
        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        
        print(f"\n {user_name} :")
        for item, score in sorted_items[:top_k]:
            print(f"  {item}: {score:.3f}")
        
        return sorted_items[:top_k]

# 
recommender = SimpleRecommender()
recommender.recommend('Alice')
recommender.recommend('Bob')
```

##  - 

### 

****

```python
# 
A = np.array([
    [1, 2, 3],
    [4, 5, 6]
])
print(A)
# [[1 2 3]
#  [4 5 6]]

print(":", A.shape)  # (2, 3) - 23
```

### AI

**1. **

```python
# 3228x28
batch_images = np.random.rand(32, 28, 28)
print(":", batch_images.shape)

# 10010
batch_features = np.random.rand(100, 10)
print(":", batch_features.shape)
```

**2. **

```python
# 1020
# 10x20
W = np.random.randn(10, 20) * 0.01
print(":", W.shape)

# 
print(":", W[:, 0])
```

**3. **

```python
# 
image = np.random.rand(28, 28)
plt.imshow(image, cmap='gray')
plt.title('28x28 ')
plt.colorbar()
plt.show()

# RGB3
color_image = np.random.rand(28, 28, 3)
plt.imshow(color_image)
plt.title('28x28 ')
plt.show()
```

###  - 

****

```python
# 
def slow_forward(X, W):
    """X: (batch, input_dim), W: (input_dim, output_dim)"""
    batch_size = X.shape[0]
    output_dim = W.shape[1]
    output = np.zeros((batch_size, output_dim))
    
    for i in range(batch_size):
        for j in range(output_dim):
            for k in range(X.shape[1]):
                output[i, j] += X[i, k] * W[k, j]
    
    return output

# 100
def fast_forward(X, W):
    return np.dot(X, W)

# 
import time

X = np.random.rand(100, 784)
W = np.random.rand(784, 128)

start = time.time()
result1 = slow_forward(X, W)
time1 = time.time() - start

start = time.time()
result2 = fast_forward(X, W)
time2 = time.time() - start

print(f": {time1:.4f}")
print(f": {time2:.4f}")
print(f": {time1/time2:.1f}")
print(f": {np.allclose(result1, result2)}")
```

****

```python
# A: (m, n) × B: (n, p) = C: (m, p)
A = np.array([
    [1, 2],
    [3, 4]
])  # 2x2

B = np.array([
    [5, 6, 7],
    [8, 9, 10]
])  # 2x3

C = np.dot(A, B)  # 2x3
print(C)
# [[21 24 27]
#  [47 54 61]]

# 
# C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0]
#        = 1*5 + 2*8 = 21 
```

****

```python
# 178428x28
X = np.random.rand(1, 784)

# 784 -> 128
W1 = np.random.randn(784, 128) * 0.01
b1 = np.zeros((1, 128))
Z1 = np.dot(X, W1) + b1
A1 = np.maximum(0, Z1)  # ReLU

# 128 -> 64
W2 = np.random.randn(128, 64) * 0.01
b2 = np.zeros((1, 64))
Z2 = np.dot(A1, W2) + b2
A2 = np.maximum(0, Z2)

# 64 -> 10
W3 = np.random.randn(64, 10) * 0.01
b3 = np.zeros((1, 10))
Z3 = np.dot(A2, W3) + b3

# Softmax
exp_scores = np.exp(Z3)
probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

print(":", X.shape)
print(":", probs.shape)
print(":", np.argmax(probs))
print(":", probs[0])
```

###  - 

```python
A = np.array([
    [1, 2, 3],
    [4, 5, 6]
])
print(" (2x3):")
print(A)

print("\n (3x2):")
print(A.T)

# 
B = np.array([[7, 8], [9, 10], [11, 12]])
print("\n(AB)^T = B^T A^T")
print(":", np.dot(A, B).T)
print(":", np.dot(B.T, A.T))
```

**AI**

```python
# 5100
sentences = np.random.rand(5, 100)

# 
# similarity[i,j] = dot(sentences[i], sentences[j])
similarity = np.dot(sentences, sentences.T)

print(":", similarity.shape)  # (5, 5)
print("\n:")
print(similarity)

# 
plt.imshow(similarity, cmap='hot')
plt.colorbar()
plt.title('')
plt.xlabel('ID')
plt.ylabel('ID')
plt.show()
```

### 

```python
class NeuralNetwork:
    """"""
    
    def __init__(self, input_size, hidden_size, output_size):
        # 
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))
    
    def relu(self, Z):
        """ReLU"""
        return np.maximum(0, Z)
    
    def relu_derivative(self, Z):
        """ReLU"""
        return (Z > 0).astype(float)
    
    def softmax(self, Z):
        """Softmax"""
        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))
        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)
    
    def forward(self, X):
        """"""
        # 
        self.Z1 = np.dot(X, self.W1) + self.b1
        self.A1 = self.relu(self.Z1)
        
        # 
        self.Z2 = np.dot(self.A1, self.W2) + self.b2
        self.A2 = self.softmax(self.Z2)
        
        return self.A2
    
    def backward(self, X, y, learning_rate=0.01):
        """"""
        m = X.shape[0]
        
        # 
        dZ2 = self.A2.copy()
        dZ2[range(m), y] -= 1
        dZ2 /= m
        
        # 
        dW2 = np.dot(self.A1.T, dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        
        # 
        dA1 = np.dot(dZ2, self.W2.T)
        dZ1 = dA1 * self.relu_derivative(self.Z1)
        
        # 
        dW1 = np.dot(X.T, dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)
        
        # 
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
    
    def train(self, X, y, epochs=1000, learning_rate=0.01):
        """"""
        losses = []
        
        for epoch in range(epochs):
            # 
            probs = self.forward(X)
            
            # 
            correct_logprobs = -np.log(probs[range(len(y)), y])
            loss = np.sum(correct_logprobs) / len(y)
            losses.append(loss)
            
            # 
            self.backward(X, y, learning_rate)
            
            if epoch % 100 == 0:
                acc = np.mean(np.argmax(probs, axis=1) == y)
                print(f"Epoch {epoch}, Loss: {loss:.4f}, Acc: {acc:.4f}")
        
        return losses
    
    def predict(self, X):
        """"""
        probs = self.forward(X)
        return np.argmax(probs, axis=1)

# 
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 
iris = load_iris()
X, y = iris.data, iris.target

# 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 
model = NeuralNetwork(input_size=4, hidden_size=10, output_size=3)
losses = model.train(X_train, y_train, epochs=1000, learning_rate=0.1)

# 
y_pred = model.predict(X_test)
accuracy = np.mean(y_pred == y_test)
print(f"\n: {accuracy:.4f}")

# 
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('')
plt.grid(True)
plt.show()
```

## 

### 

```python
# 
A = np.array([
    [4, 2],
    [1, 3]
])

# 
eigenvalues, eigenvectors = np.linalg.eig(A)

print(":", eigenvalues)
print(":")
print(eigenvectors)

# A·v = λ·v
v1 = eigenvectors[:, 0]
lambda1 = eigenvalues[0]

print("\nA·v =", np.dot(A, v1))
print("λ·v =", lambda1 * v1)
```

**AIPCA**

```python
class PCA:
    """"""
    
    def __init__(self, n_components):
        self.n_components = n_components
    
    def fit(self, X):
        """"""
        # 
        self.mean = np.mean(X, axis=0)
        X_centered = X - self.mean
        
        # 
        cov = np.dot(X_centered.T, X_centered) / len(X)
        
        # 
        eigenvalues, eigenvectors = np.linalg.eig(cov)
        
        # 
        idx = eigenvalues.argsort()[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]
        
        # n
        self.components = eigenvectors[:, :self.n_components]
        self.explained_variance = eigenvalues[:self.n_components]
        
        return self
    
    def transform(self, X):
        """"""
        X_centered = X - self.mean
        return np.dot(X_centered, self.components)
    
    def inverse_transform(self, X_reduced):
        """"""
        return np.dot(X_reduced, self.components.T) + self.mean

# 
X = np.random.rand(100, 50)  # 10050

pca = PCA(n_components=10)
pca.fit(X)

X_reduced = pca.transform(X)
print(":", X.shape)
print(":", X_reduced.shape)

X_restored = pca.inverse_transform(X_reduced)
print(":", X_restored.shape)

# 
variance_ratio = np.sum(pca.explained_variance) / np.sum(np.var(X, axis=0))
print(f": {variance_ratio:.2%}")
```

### (SVD)

```python
# SVD: A = U·Σ·V^T
A = np.random.rand(5, 3)

U, S, VT = np.linalg.svd(A, full_matrices=False)

print("A:", A.shape)
print("U:", U.shape)
print("S:", S.shape)
print("VT:", VT.shape)

# 
A_reconstructed = np.dot(U * S, VT)
print("\n:", np.linalg.norm(A - A_reconstructed))
```

**AI**

```python
def compress_image(image, k):
    """SVD"""
    # SVD
    compressed = np.zeros_like(image)
    
    for i in range(3):  # RGB
        U, S, VT = np.linalg.svd(image[:, :, i], full_matrices=False)
        
        # k
        compressed[:, :, i] = np.dot(U[:, :k] * S[:k], VT[:k, :])
    
    return np.clip(compressed, 0, 1)

# 
from skimage import data
image = data.astronaut() / 255.0

# 
compressed_10 = compress_image(image, 10)
compressed_50 = compress_image(image, 50)

# 
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
axes[0].imshow(image)
axes[0].set_title('')
axes[1].imshow(compressed_10)
axes[1].set_title('k=10')
axes[2].imshow(compressed_50)
axes[2].set_title('k=50')
plt.show()

# 
original_size = image.size
compressed_size = (image.shape[0] * 10 + 10 + 10 * image.shape[1]) * 3
print(f": {compressed_size / original_size:.2%}")
```

## 

AI

1. ****
2. ****
3. ****
4. ****PCA
5. **SVD**

****

## 

1. 
2. SVD
3. Batch Normalization
4. 

[](./.mdx)

