---
sidebar_position: 0
title: LSTM
---

:::info
LSTM1997  Hochreiter  Schmidhuber  RNN 

**LSTM **
- ****
- ****
- ****

Long Short-Term Memory 2025  5

[Long Short-Term Memory (1997)](https://www.bioinf.jku.at/publications/older/2604.pdf)
:::

## LSTM 

LSTMLong Short-Term MemoryRNN RNN 

- ****
- ****

**LSTM **
- 
- 
- 
- 
- 

**LSTM **
-   Transformer 


### RNN 

 RNN 

:::warning
**RNN **

T

- ****0
- ****

****


```
∂L/∂h₀ = ∂L/∂hₜ × ∏(∂hₜ/∂hₜ₋₁)
```

 < 10  
 > 1∞

****
:::


## LSTM 

LSTM ********

### 

LSTM 

1. **Forget Gate**
2. **Input Gate**
3. **Output Gate**
4. **Cell State**""

### 

LSTM 

```python
# x_th_{t-1}c_{t-1}

# 1. 
f_t = σ(W_f · [h_{t-1}, x_t] + b_f)

# 2. 
i_t = σ(W_i · [h_{t-1}, x_t] + b_i)
c̃_t = tanh(W_c · [h_{t-1}, x_t] + b_c)  # 

# 3. 
c_t = f_t ⊙ c_{t-1} + i_t ⊙ c̃_t

# 4. 
o_t = σ(W_o · [h_{t-1}, x_t] + b_o)
h_t = o_t ⊙ tanh(c_t)

# 
# σsigmoid 0-1
# tanh-11
# ⊙
# Wb
```

:::tip
****

- **** \(f_t\)
  - 1
  - 0
  
- **** \(i_t\)
  - 1
  - 0
  
- **** \(o_t\)
  - 

****
- 
- 
- 
:::

### 

```python showLineNumbers
import torch
import torch.nn as nn

# LSTM 
class SimplifiedLSTM:
    """
    LSTM 
    """
    def step(self, x_t, h_prev, c_prev):
        """
        x_t: 
        h_prev: 
        c_prev: 
        """
        # 
        combined = torch.cat([h_prev, x_t], dim=1)
        
        # 1. 
        f_t = torch.sigmoid(self.W_f @ combined + self.b_f)
        # 
        
        # 2. 
        i_t = torch.sigmoid(self.W_i @ combined + self.b_i)
        c_tilde = torch.tanh(self.W_c @ combined + self.b_c)
        # 
        
        # 3. 
        c_t = f_t * c_prev + i_t * c_tilde
        #  ×  +  × 
        
        # 4. 
        o_t = torch.sigmoid(self.W_o @ combined + self.b_o)
        h_t = o_t * torch.tanh(c_t)
        # 
        
        return h_t, c_t

#  PyTorch  LSTM
lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=1, batch_first=True)

# (batch_size, seq_len, input_size)
x = torch.randn(2, 5, 10)  # 2510

# 
output, (h_n, c_n) = lstm(x)

print(f": {output.shape}")      # (2, 5, 20) - 
print(f": {h_n.shape}")     # (1, 2, 20) - h
print(f": {c_n.shape}")     # (1, 2, 20) - c
```

### 

```python showLineNumbers
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):
        super(LSTMModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM 
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # 
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # x: (batch_size, seq_len, input_size)
        
        # LSTM 
        # output: (batch_size, seq_len, hidden_size)
        # (h_n, c_n): 
        output, (h_n, c_n) = self.lstm(x)
        
        # 
        last_output = output[:, -1, :]  # (batch_size, hidden_size)
        
        # 
        out = self.fc(last_output)  # (batch_size, output_size)
        
        return out

# 
model = LSTMModel(
    input_size=10,
    hidden_size=64,
    num_layers=2,
    output_size=1,
    dropout=0.5
)

print(model)
# LSTMModel(
#   (lstm): LSTM(10, 64, num_layers=2, batch_first=True, dropout=0.5)
#   (fc): Linear(in_features=64, out_features=1, bias=True)
# )

# 
x = torch.randn(32, 20, 10)  # 322010
output = model(x)
print(f": {output.shape}")  # (32, 1)
```


:::tip
**PyTorch LSTM **

|  |  |  |
|------|------|--------|
| `input_size` |  |  |
| `hidden_size` |  |  |
| `num_layers` | LSTM  | 1 |
| `bias` |  | True |
| `batch_first` |  | False |
| `dropout` | Dropout  | 0 |
| `bidirectional` |  | False |

**batch_first **
- `True` `(batch, seq, feature)`
- `False` `(seq, batch, feature)`PyTorch
:::


### Seq2Seq

 LSTM 

```python showLineNumbers
import torch
import torch.nn as nn

class Encoder(nn.Module):
    """"""
    def __init__(self, input_size, embedding_dim, hidden_size, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(input_size, embedding_dim)
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_size,
            num_layers,
            batch_first=True
        )
    
    def forward(self, x):
        # x: (batch_size, seq_len)
        embedded = self.embedding(x)  # (batch, seq, emb)
        outputs, (h_n, c_n) = self.lstm(embedded)
        return h_n, c_n  # 

class Decoder(nn.Module):
    """"""
    def __init__(self, output_size, embedding_dim, hidden_size, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(output_size, embedding_dim)
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_size,
            num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x, h_0, c_0):
        # x: (batch_size, seq_len)
        # h_0, c_0: 
        embedded = self.embedding(x)
        outputs, (h_n, c_n) = self.lstm(embedded, (h_0, c_0))
        predictions = self.fc(outputs)  # (batch, seq, vocab)
        return predictions, (h_n, c_n)

class Seq2Seq(nn.Module):
    """"""
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
    
    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        # src: (batch_size, src_len) 
        # trg: (batch_size, trg_len) 
        
        batch_size = trg.shape[0]
        trg_len = trg.shape[1]
        trg_vocab_size = self.decoder.fc.out_features
        
        # 
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size)
        
        # 
        h, c = self.encoder(src)
        
        # <SOS> token
        input = trg[:, 0].unsqueeze(1)
        
        # 
        for t in range(1, trg_len):
            output, (h, c) = self.decoder(input, h, c)
            outputs[:, t, :] = output.squeeze(1)
            
            # Teacher forcing
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = output.argmax(2)
            input = trg[:, t].unsqueeze(1) if teacher_force else top1
        
        return outputs

#  Seq2Seq 
encoder = Encoder(
    input_size=10000,      # 
    embedding_dim=256,
    hidden_size=512,
    num_layers=2
)

decoder = Decoder(
    output_size=8000,      # 
    embedding_dim=256,
    hidden_size=512,
    num_layers=2
)

model = Seq2Seq(encoder, decoder)

# 
src = torch.randint(0, 10000, (32, 20))  # 
trg = torch.randint(0, 8000, (32, 15))   # 

# 
outputs = model(src, trg, teacher_forcing_ratio=0.5)
print(f": {outputs.shape}")  # (32, 15, 8000)
```

:::tip
**Seq2Seq **

1. **Teacher Forcing**
   - 
   - 
   ```python
   # 
   input = trg[:, t]  # 
   
   # 
   input = prediction  # 
   ```

2. **-**
   - 
   - 
   
3. ****
   - 
   - 
:::

### 



```python showLineNumbers
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# 
class SentimentDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_len=100):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.max_len = max_len
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        
        # 
        indices = [self.vocab.get(word, self.vocab['<UNK>']) for word in text.split()]
        
        # 
        if len(indices) < self.max_len:
            indices += [self.vocab['<PAD>']] * (self.max_len - len(indices))
        else:
            indices = indices[:self.max_len]
        
        return torch.LongTensor(indices), torch.LongTensor([label])

# 
class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_size,
            num_layers,
            batch_first=True,
            dropout=0.5
        )
        self.fc = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, (h_n, c_n) = self.lstm(embedded)
        
        # 
        last_hidden = lstm_out[:, -1, :]
        out = self.fc(last_hidden)
        return out

# 
texts = [
    "I love this movie",
    "This is terrible",
    "Great performance",
    "Waste of time"
]
labels = [1, 0, 1, 0]  # 1=, 0=

# 
vocab = {
    '<PAD>': 0,
    '<UNK>': 1,
    'I': 2,
    'love': 3,
    'this': 4,
    'movie': 5,
    'is': 6,
    'terrible': 7,
    'Great': 8,
    'performance': 9,
    'Waste': 10,
    'of': 11,
    'time': 12
}

# 
dataset = SentimentDataset(texts, labels, vocab, max_len=20)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# 
model = SentimentLSTM(
    vocab_size=len(vocab),
    embedding_dim=50,
    hidden_size=128,
    num_layers=2,
    num_classes=2
)

# 
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    
    for batch_texts, batch_labels in dataloader:
        # 
        outputs = model(batch_texts)
        loss = criterion(outputs, batch_labels.squeeze())
        
        # 
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    avg_loss = total_loss / len(dataloader)
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')

# 
model.eval()
test_text = "I love this"
test_indices = [vocab.get(word, vocab['<UNK>']) for word in test_text.split()]
test_indices += [vocab['<PAD>']] * (20 - len(test_indices))
test_tensor = torch.LongTensor([test_indices])

with torch.no_grad():
    output = model(test_tensor)
    prediction = torch.argmax(output, dim=1)
    print(f": {'' if prediction.item() == 1 else ''}")
```

### Bidirectional LSTMLSTM

 LSTM 

```python showLineNumbers
import torch
import torch.nn as nn

class BiLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        
        #  LSTM
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True  # 
        )
        
        # LSTM hidden_size * 2
        self.fc = nn.Linear(hidden_size * 2, output_size)
    
    def forward(self, x):
        # x: (batch_size, seq_len, input_size)
        
        # LSTM
        # output: (batch_size, seq_len, hidden_size * 2)
        # h_n: (num_layers * 2, batch_size, hidden_size)
        output, (h_n, c_n) = self.lstm(x)
        
        # 
        last_output = output[:, -1, :]  # (batch_size, hidden_size * 2)
        
        # 
        out = self.fc(last_output)
        return out

# 
model = BiLSTM(input_size=10, hidden_size=64, num_layers=2, output_size=2)
x = torch.randn(32, 20, 10)
output = model(x)
print(f": {output.shape}")  # (32, 2)
```

:::tip
** LSTM **

```python
#  LSTM
"   []" →  "  "

#  LSTM
"   []" →  "  " + ""
"   []  " →  "  " + " "

# 
-  
-  
-  
-  
-  
```
:::

## 

### 



```python showLineNumbers
import torch
import torch.nn as nn
import torch.optim as optim

model = LSTMModel(...)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# 
for epoch in range(num_epochs):
    for batch_x, batch_y in dataloader:
        # 
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        
        # 
        optimizer.zero_grad()
        loss.backward()
        
        # 
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
        # 
        # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
        
        optimizer.step()
```

:::tip
****

1. ****
   ```python
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
   #  > 5.0
   ```

2. ****
   ```python
   torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
   #  [-0.5, 0.5]
   ```

****
- LSTM/GRU
- 
- loss  NaN max_norm

****
- max_norm: 1.0 - 10.05.0
:::

### 

```python showLineNumbers
import torch
import torch.nn as nn

class RobustLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):
        super().__init__()
        
        self.embedding = nn.Embedding(input_size, 128)
        
        # 1. LSTM dropout
        self.lstm = nn.LSTM(
            128,
            hidden_size,
            num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # 2.  dropout
        self.dropout = nn.Dropout(dropout)
        
        # 3. 
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        embedded = self.embedding(x)
        
        # LSTM
        lstm_out, _ = self.lstm(embedded)
        
        # Dropout
        dropped = self.dropout(lstm_out[:, -1, :])
        
        # 
        out = self.fc(dropped)
        return out

# 

# 4. L2
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

# 5. Early Stopping
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# 
early_stopping = EarlyStopping(patience=10)

for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, train_loader)
    val_loss = validate(model, val_loader)
    
    early_stopping(val_loss)
    if early_stopping.early_stop:
        print(f"Early stopping at epoch {epoch}")
        break

# 6. 
# 
# 
```

:::tip
****

1. ****
2. **Dropout**0.3-0.5
3. ****1e-4  1e-6
4. ****
5. ****
6. ****

```python
# 
model = nn.LSTM(..., dropout=0.5)  # LSTM dropout
optimizer = Adam(..., weight_decay=1e-5)  # L2
scheduler = ReduceLROnPlateau(...)  # 
early_stopping = EarlyStopping(patience=10)  # 
```
:::

### 

```python showLineNumbers
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR

model = LSTMModel(...)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 1loss
scheduler = ReduceLROnPlateau(
    optimizer,
    mode='min',          # 
    factor=0.5,          # 
    patience=5,          # epoch
    verbose=True
)

for epoch in range(num_epochs):
    train_loss = train(model, train_loader)
    val_loss = validate(model, val_loader)
    
    # 
    scheduler.step(val_loss)

# 2
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=50,     # 
    eta_min=1e-6  # 
)

for epoch in range(num_epochs):
    train(model, train_loader)
    scheduler.step()  # epoch

# 3
scheduler = optim.lr_scheduler.StepLR(
    optimizer,
    step_size=30,  # 30epoch
    gamma=0.1      # 0.1
)
```

### 

```python showLineNumbers
import torch
import torch.nn as nn

# 1. 
def check_shapes(model, input_shape):
    x = torch.randn(*input_shape)
    print(f": {x.shape}")
    
    output = model(x)
    print(f": {output.shape}")
    
    # 
    for name, module in model.named_modules():
        if isinstance(module, nn.LSTM):
            print(f"{name} - LSTM")

# 2. 
def check_gradients(model):
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            print(f"{name}:  = {grad_norm:.4f}")
            
            if grad_norm == 0:
                print(f"  {name} 0")
            elif grad_norm > 100:
                print(f"  {name} ")

# 3. 
def check_weights(model):
    for name, param in model.named_parameters():
        print(f"{name}:")
        print(f"  : {param.data.mean():.4f}")
        print(f"  : {param.data.std():.4f}")
        print(f"  : {param.data.min():.4f}")
        print(f"  : {param.data.max():.4f}")

# 
model = LSTMModel(...)
check_shapes(model, (32, 50, 10))

# 
loss.backward()
check_gradients(model)
check_weights(model)
```

### 

```python showLineNumbers
import torch
import torch.nn as nn

# 1.  GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
x = x.to(device)

# 2. 
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for epoch in range(num_epochs):
    for batch_x, batch_y in dataloader:
        batch_x = batch_x.to(device)
        batch_y = batch_y.to(device)
        
        optimizer.zero_grad()
        
        # 
        with autocast():
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
        
        # 
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

# 3. DataLoader 
dataloader = DataLoader(
    dataset,
    batch_size=64,
    shuffle=True,
    num_workers=4,      # 
    pin_memory=True     # GPU
)

# 4. PyTorch 2.0+
model = torch.compile(model)  # 
```

:::tip
****

1. ****
   - GPU
   - CPU32-128

2. ****
   -  O(n²)
   - 

3. ****
   - 64128256512
   - 
   - 

4. ****
   - 1-2
   - 3-4

```python
# 
import time

configs = [
    {'hidden': 64, 'layers': 1},
    {'hidden': 128, 'layers': 2},
    {'hidden': 256, 'layers': 3}
]

for config in configs:
    model = LSTMModel(hidden_size=config['hidden'], 
                      num_layers=config['layers'])
    start = time.time()
    train(model)
    duration = time.time() - start
    print(f" {config}: {duration:.2f}")
```
:::
