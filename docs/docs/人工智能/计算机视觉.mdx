---
sidebar_position: 7
title: 
---

# Computer Vision

:::info 

:::

## 

**CV** ""

### 

```mermaid
graph TD
    A[] --> B[]
    A --> C[]
    A --> D[]
    A --> E[]
    A --> F[]
    A --> G[]
```

## CNN

### CNN

```python
import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    """CNN"""
    def __init__(self, num_classes=10):
        super().__init__()
        
        # 
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        
        # 
        self.pool = nn.MaxPool2d(2, 2)
        
        # 
        self.fc1 = nn.Linear(128 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, num_classes)
        
        # Dropout
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        # 1: 32x32 -> 16x16
        x = self.pool(self.relu(self.conv1(x)))
        
        # 2: 16x16 -> 8x8
        x = self.pool(self.relu(self.conv2(x)))
        
        # 3: 8x8 -> 4x4
        x = self.pool(self.relu(self.conv3(x)))
        
        # 
        x = x.view(-1, 128 * 4 * 4)
        
        # 
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.fc2(x)
        
        return x

# 
model = SimpleCNN(num_classes=10)
x = torch.randn(1, 3, 32, 32)  # batch_size=1, channels=3, height=32, width=32
output = model(x)
print(f": {output.shape}")  # [1, 10]
```

### CNN

**ResNet**

```python
class ResidualBlock(nn.Module):
    """"""
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # 
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)  # 
        out = torch.relu(out)
        return out
```

## 

### 

```python
from torchvision import models, transforms
from PIL import Image

class ImageClassifier:
    """"""
    def __init__(self, model_name='resnet50'):
        # 
        self.model = models.resnet50(pretrained=True)
        self.model.eval()
        
        # 
        self.transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
        
        # 
        self.labels = self._load_labels()
    
    def predict(self, image_path, top_k=5):
        """"""
        # 
        image = Image.open(image_path).convert('RGB')
        input_tensor = self.transform(image).unsqueeze(0)
        
        # 
        with torch.no_grad():
            output = self.model(input_tensor)
            probabilities = torch.softmax(output[0], dim=0)
        
        # top-k
        top_probs, top_indices = torch.topk(probabilities, top_k)
        
        results = []
        for prob, idx in zip(top_probs, top_indices):
            results.append({
                'label': self.labels[idx],
                'probability': prob.item()
            })
        
        return results
    
    def _load_labels(self):
        """ImageNet"""
        # 
        return [f"class_{i}" for i in range(1000)]

# 
classifier = ImageClassifier()
results = classifier.predict('cat.jpg')
for r in results:
    print(f"{r['label']}: {r['probability']:.2%}")
```

## 

### YOLO

```python
from ultralytics import YOLO

class ObjectDetector:
    """"""
    def __init__(self, model_path='yolov8n.pt'):
        self.model = YOLO(model_path)
    
    def detect(self, image_path, conf_threshold=0.5):
        """"""
        results = self.model(image_path, conf=conf_threshold)
        
        detections = []
        for result in results:
            boxes = result.boxes
            for box in boxes:
                detections.append({
                    'class': result.names[int(box.cls)],
                    'confidence': float(box.conf),
                    'bbox': box.xyxy[0].tolist()  # [x1, y1, x2, y2]
                })
        
        return detections
    
    def detect_video(self, video_path, output_path):
        """"""
        results = self.model(video_path, stream=True)
        
        for result in results:
            # 
            annotated_frame = result.plot()
            # 
            pass

# 
detector = ObjectDetector()
detections = detector.detect('street.jpg')
for det in detections:
    print(f"{det['class']}: {det['confidence']:.2f} at {det['bbox']}")
```

## 

### 

```python
import torch
import torchvision.models.segmentation as segmentation

class SemanticSegmentation:
    """"""
    def __init__(self):
        self.model = segmentation.deeplabv3_resnet50(pretrained=True)
        self.model.eval()
    
    def segment(self, image):
        """"""
        with torch.no_grad():
            output = self.model(image)['out']
            # 
            segmentation_map = output.argmax(1)
        
        return segmentation_map
    
    def visualize(self, image, segmentation_map):
        """"""
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.imshow(image.permute(1, 2, 0))
        plt.title('Original')
        
        plt.subplot(1, 2, 2)
        plt.imshow(segmentation_map.squeeze())
        plt.title('Segmentation')
        plt.show()
```

## 

```python
import face_recognition

class FaceRecognizer:
    """"""
    def __init__(self):
        self.known_faces = {}
    
    def add_face(self, name, image_path):
        """"""
        image = face_recognition.load_image_file(image_path)
        encoding = face_recognition.face_encodings(image)[0]
        self.known_faces[name] = encoding
    
    def recognize(self, image_path):
        """"""
        # 
        image = face_recognition.load_image_file(image_path)
        
        # 
        face_locations = face_recognition.face_locations(image)
        
        # 
        face_encodings = face_recognition.face_encodings(image, face_locations)
        
        results = []
        for face_encoding, face_location in zip(face_encodings, face_locations):
            # 
            matches = face_recognition.compare_faces(
                list(self.known_faces.values()),
                face_encoding
            )
            
            name = "Unknown"
            if True in matches:
                match_index = matches.index(True)
                name = list(self.known_faces.keys())[match_index]
            
            results.append({
                'name': name,
                'location': face_location
            })
        
        return results

# 
recognizer = FaceRecognizer()
recognizer.add_face("Alice", "alice.jpg")
recognizer.add_face("Bob", "bob.jpg")

results = recognizer.recognize("group.jpg")
for r in results:
    print(f"Found {r['name']} at {r['location']}")
```

## 

### 1

```python
import streamlit as st
from PIL import Image

def main():
    st.title("")
    
    # 
    uploaded_file = st.file_uploader("", type=['jpg', 'png'])
    
    if uploaded_file:
        # 
        image = Image.open(uploaded_file)
        st.image(image, caption='')
        
        # 
        if st.button(''):
            classifier = ImageClassifier()
            results = classifier.predict(uploaded_file)
            
            st.write("")
            for r in results:
                st.write(f"- {r['label']}: {r['probability']:.2%}")

if __name__ == '__main__':
    main()
```

### 2

```python
import cv2

def real_time_detection():
    """"""
    detector = ObjectDetector()
    cap = cv2.VideoCapture(0)  # 
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # 
        detections = detector.detect(frame)
        
        # 
        for det in detections:
            x1, y1, x2, y2 = map(int, det['bbox'])
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, f"{det['class']} {det['confidence']:.2f}",
                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # 
        cv2.imshow('Detection', frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
```

## 

### 

```python
from torchvision import transforms

# 
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])

# 
test_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])
```

### 

```python
def create_transfer_model(num_classes):
    """"""
    # 
    model = models.resnet50(pretrained=True)
    
    # 
    for param in model.parameters():
        param.requires_grad = False
    
    # 
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, num_classes)
    
    return model
```

## 

****:
1. CNN
2. 
3. 
4. 

****:
- CNN
- 
- 
- 

<DocCardList />

